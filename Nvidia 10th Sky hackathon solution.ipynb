{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c9f8ce6-f90f-4e27-abb3-836231e98bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 2903 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch  \n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings  \n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "nvidia_api_key = \"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\"\n",
    "# \"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\"\n",
    "# \"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\"\n",
    "# \"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\"\n",
    "\"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\"\n",
    "\n",
    "FAISS_path = \"/home/nvidia/NvHacker/retrive_module/embed/\"\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
    "\n",
    "# Prompt library\n",
    "EnglishTemplate_0 =\"\"\"\n",
    "You are an expert in the field of machine learning. \\\n",
    "You excel at responding to inquiries in English, particularly those related to machine learning concepts and applications. \\\n",
    "Your proficiency stems from your ability to deconstruct intricate questions into manageable components, \\\n",
    "address each part with precision, and subsequently recombine your insights into a coherent, English-language response that thoroughly addresses the overarching inquiry.\n",
    "\n",
    "Now, consider the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "EnglishTemplate = \"\"\"\n",
    "You are an expert in the field of machine learning. \\\n",
    "You excel at responding to inquiries in English, particularly those related to machine learning concepts and applications. \\\n",
    "Your proficiency stems from your ability to deconstruct intricate questions into manageable components, \\\n",
    "address each part with precision, and subsequently recombine your insights into a coherent, English-language response that thoroughly addresses the overarching inquiry.\n",
    "Related knowledge can also be found here:\n",
    "{context}\n",
    "Now, consider the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ChineseTemplate_0 =  \"\"\"\n",
    "你是一个精通机器学习领域的助手，对机器学习的知识和算法有着深入的理解。\\\n",
    "你擅长将复杂的机器学习问题拆解成清晰易懂的子问题，\\\n",
    "并针对每个子问题给出专业的中文解答，最后将这些解答整合成全面而准确的回答。\\\n",
    "\n",
    "现在，请针对以下问题给出你的分析和解答：\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "ChineseTemplate = \"\"\"\n",
    "你是一个精通机器学习领域的助手，对机器学习的知识和算法有着深入的理解。\\\n",
    "你擅长将复杂的机器学习问题拆解成清晰易懂的子问题，\\\n",
    "并针对每个子问题给出专业的中文解答，最后将这些解答整合成全面而准确的回答。\\\n",
    "以下是相关知识：\n",
    "{context}\n",
    "现在，请针对以下问题给出你的分析和解答：\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "\n",
    "prompt_templates = [EnglishTemplate_0, ChineseTemplate_0]  \n",
    "prompt_embeddings = embedder.embed_documents(prompt_templates)  \n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embedder.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    # Chosen prompt \n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example openai chain\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "vecstores =  [FAISS.load_local(folder_path=FAISS_path, embeddings=embedder)]  # ,allow_dangerous_deserialization=True\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## 初始化一个空的 FAISS 索引并将其他索引合并到其中\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "if 'docstore' not in globals():\n",
    "    docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "latent_dim  = len(embedder.embed_query(\"test\"))\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "llm_nemo = ChatNVIDIA(model=\"ai-nemotron-4-340b-instruct\")\n",
    "\n",
    "chain_nv_template = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm_nemo\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x: x[:512])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RERANKER\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results, k=5):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # 初始化一个字典来保存每个唯一文档的融合分数\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # 将文档转换为字符串格式以用作键（假设文档可以序列化为 JSON）\n",
    "            doc_str = dumps(doc)\n",
    "            # 如果文档尚未在 fused_scores 字典中，添加初始分数0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # 更新rank\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "            \n",
    "    # print(\"fused_scores\")\n",
    "    # print(doc.page_content)\n",
    "    # 排序\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "retriever = docstore.as_retriever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77edb8-21d9-4924-b797-7840bebf427c",
   "metadata": {},
   "source": [
    "### **First generation on question(skip)** \n",
    "- fisrt stage with prompt selection\n",
    "- without RAG\n",
    "- str are clipped to 512 size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fb110-abfa-4b1b-b67e-a861c4f096b4",
   "metadata": {},
   "source": [
    "API_KEY not enough amout, so just run the final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6ea7fa-d2ff-4c83-bcda-c1d23a83b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks or questions into smaller, more manageable sub-tasks or sub-questions. This approach allows LLM agents to handle intricate problems more effectively by addressing each component individually and then combining the results to form a comprehensive solution. Here's a step-by-step explanation of task decomposition for LLM agents:\n",
      "\n",
      "1. **Identify the main task or question**: Begin by understanding the ove\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "result = chain_nv_template.invoke(question)\n",
    "print(\"result:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934579f2-7a2d-4bf4-b0db-d3f5a882968e",
   "metadata": {},
   "source": [
    "###  **Retrievial** \n",
    "- second stage with reranker\n",
    "- long recorder\n",
    "- doc2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eefc7a38-a270-41ee-b296-f8cd7fcb03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = chain_nv_template | retriever | RunnableLambda(lambda x :reciprocal_rank_fusion([x]))| long_reorder | docs2str\n",
    "# docs = retrieval_chain_rag_fusion.invoke(question)\n",
    "# print(len(docs))\n",
    "# print(docs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a7d01-bb01-4c4a-80c3-6dbbda6207b9",
   "metadata": {},
   "source": [
    "###  **Generation**\n",
    "- self reflect and retrieved data as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6fcf640-4396-41fd-989c-46f0b7afaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "prompt = ChatPromptTemplate.from_template(ChineseTemplate)\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion, \n",
    "        \"query\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt\n",
    "    | llm_nemo\n",
    "    | StrOutputParser()\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "# final_rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d235229d-32e2-4ade-a1d4-152da049225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##首先根据输入的消息进行检索\n",
    "    # retrieval = retrieval_chain_rag_fusion.invoke(message)\n",
    "    # print(\"the content of retrieval is:\")\n",
    "    # print(retrieval)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    ## 然后流式传输stream_chain的结果\n",
    "    print(\"Starting stream...\")\n",
    "    \n",
    "    for token in final_rag_chain.stream(message):\n",
    "        buffer += token\n",
    "        ## 优化信息打印的格式\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "    print(\"the content of buffer is:\")\n",
    "    print(buffer)\n",
    "    ##最后将聊天内容保存到对话内存缓冲区中\n",
    "    # save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a62e5e-0049-448e-9eee-f38f8f407b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the content of response_gen is:\n",
      "<generator object chat_gen at 0xfffec3c883c0>\n",
      "Starting stream...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar, List\n",
    "\n",
    "@dataclass\n",
    "class DefaultElement:\n",
    "    DEFAULT_MESSAGE: ClassVar[dict] = {\"text\": \"\"}\n",
    "    DEFAULT_HISTORY: ClassVar[List[List[str]]] = []\n",
    "    DEFAULT_DOCUMENT: ClassVar[List[str]] = []\n",
    "    DEFAULT_STATUS: ClassVar[str] = \"Ready!\"  \n",
    "\n",
    "    HELLO_MESSAGE: str = \"Hi 👋, how can I help you today?\"\n",
    "    EMPTY_MESSAGE: str = \"You need to enter your message!\"\n",
    "    ANSWERING_STATUS: str = \"Answering!\"\n",
    "    COMPLETED_STATUS: str = \"Completed!\"\n",
    "    ERROR_MESSAGE: str = \"Service is unavailable.\"\n",
    "\n",
    "\n",
    "class LLMResponse:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _yield_string(self, message: str):\n",
    "        for i in range(len(message)):\n",
    "            time.sleep(0.01)\n",
    "            yield (\n",
    "                DefaultElement.DEFAULT_MESSAGE,\n",
    "                [[None, message[: i + 1]]],\n",
    "                DefaultElement.DEFAULT_STATUS,\n",
    "            )\n",
    "\n",
    "    def welcome(self):\n",
    "        yield from self._yield_string(DefaultElement.HELLO_MESSAGE)\n",
    "\n",
    "    def empty_message(self):\n",
    "        yield from self._yield_string(DefaultElement.EMPTY_MESSAGE)\n",
    "\n",
    "    def stream_response(self, message: str, history: List[List[str]], response_gen):\n",
    "        for response in response_gen:\n",
    "            updated_history = history + [[message, response]]\n",
    "            yield (DefaultElement.DEFAULT_MESSAGE[\"text\"], updated_history, DefaultElement.ANSWERING_STATUS)\n",
    "        yield (DefaultElement.DEFAULT_MESSAGE[\"text\"], updated_history, DefaultElement.COMPLETED_STATUS)\n",
    "\n",
    "class LocalChatbotUI:\n",
    "    def __init__(self, avatar_images: List[str] = [\"/home/nvidia/NvHacker/assets/user.png\", \"/home/nvidia/NvHacker/assets/bot.png\"]):\n",
    "        self._avatar_images = [os.path.join(os.getcwd(), image) for image in avatar_images]\n",
    "        self._variant = \"panel\"\n",
    "        self._llm_response = LLMResponse()\n",
    "\n",
    "    def _get_response(self, message, chatbot: List[List[str]], progress=gr.Progress(track_tqdm=True)):\n",
    "        if isinstance(message, str):\n",
    "            message = {\"text\": message}\n",
    "\n",
    "        if message[\"text\"] in [None, \"\"]:\n",
    "            for m in self._llm_response.empty_message():\n",
    "                yield m\n",
    "        else:\n",
    "            response_gen = chat_gen(message[\"text\"], history=chatbot)\n",
    "            print(\"the content of response_gen is:\")\n",
    "            print(response_gen)\n",
    "            for m in self._llm_response.stream_response(message[\"text\"], chatbot, response_gen):\n",
    "                yield m\n",
    "\n",
    "    def _welcome(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], [[None, DefaultElement.HELLO_MESSAGE]], DefaultElement.DEFAULT_STATUS\n",
    "\n",
    "    def build(self):\n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Soft(primary_hue=\"slate\"),\n",
    "        ) as demo:\n",
    "            gr.Markdown(\"## ML Chatbot 🤖\")\n",
    "            with gr.Tab(\"Interface\"):\n",
    "                with gr.Row(variant=self._variant, equal_height=False):\n",
    "                    with gr.Column(scale=30, variant=self._variant):\n",
    "                        chatbot = gr.Chatbot(\n",
    "                            layout=\"bubble\",\n",
    "                            value=[],\n",
    "                            height=550,\n",
    "                            scale=2,\n",
    "                            show_copy_button=True,\n",
    "                            bubble_full_width=False,\n",
    "                            avatar_images=self._avatar_images,\n",
    "                        )\n",
    "                        with gr.Row(variant=self._variant):\n",
    "                            message = gr.Textbox(\n",
    "                                value=DefaultElement.DEFAULT_MESSAGE[\"text\"],\n",
    "                                placeholder=\"Enter your message:\",\n",
    "                                show_label=False,\n",
    "                                scale=6,\n",
    "                                lines=1,\n",
    "                            )\n",
    "                        with gr.Row(variant=self._variant):\n",
    "                            clear_btn = gr.Button(value=\"Clear\", min_width=20)\n",
    "                            reset_btn = gr.Button(value=\"Reset\", min_width=20)\n",
    "\n",
    "            clear_btn.click(self._clear_chat, outputs=[message, chatbot])\n",
    "            reset_btn.click(self._reset_chat, outputs=[message, chatbot])\n",
    "            message.submit(self._get_response, inputs=[message, chatbot], outputs=[message, chatbot])\n",
    "\n",
    "            demo.load(self._welcome, outputs=[message, chatbot])\n",
    "\n",
    "        return demo\n",
    "\n",
    "    def _clear_chat(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], DefaultElement.DEFAULT_HISTORY\n",
    "\n",
    "    def _reset_chat(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], DefaultElement.DEFAULT_HISTORY\n",
    "\n",
    "chatbot_ui = LocalChatbotUI()\n",
    "demo = chatbot_ui.build()\n",
    "demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c0c69-d8c5-4eef-a793-08bb0b05f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nvidia/NvHacker/retrive_module/embed\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3eecf6-27cd-4d77-aa87-54d80e0043b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 3354 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6b890-eb6e-40b5-b8cb-c32262021a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
