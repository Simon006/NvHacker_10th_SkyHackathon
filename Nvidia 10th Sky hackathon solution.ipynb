{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c9f8ce6-f90f-4e27-abb3-836231e98bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 2903 chunks\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch  \n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings  \n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import os\n",
    "\n",
    "nvidia_api_key = \"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\"\n",
    "# \"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\"\n",
    "# \"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\"\n",
    "# \"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\"\n",
    "\"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\"\n",
    "\n",
    "FAISS_path = \"/home/nvidia/NvHacker/retrive_module/embed/\"\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key\n",
    "\n",
    "# Prompt library\n",
    "EnglishTemplate_0 =\"\"\"\n",
    "You are an expert in the field of machine learning. \\\n",
    "You excel at responding to inquiries in English, particularly those related to machine learning concepts and applications. \\\n",
    "Your proficiency stems from your ability to deconstruct intricate questions into manageable components, \\\n",
    "address each part with precision, and subsequently recombine your insights into a coherent, English-language response that thoroughly addresses the overarching inquiry.\n",
    "\n",
    "Now, consider the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "EnglishTemplate = \"\"\"\n",
    "You are an expert in the field of machine learning. \\\n",
    "You excel at responding to inquiries in English, particularly those related to machine learning concepts and applications. \\\n",
    "Your proficiency stems from your ability to deconstruct intricate questions into manageable components, \\\n",
    "address each part with precision, and subsequently recombine your insights into a coherent, English-language response that thoroughly addresses the overarching inquiry.\n",
    "Related knowledge can also be found here:\n",
    "{context}\n",
    "Now, consider the following question:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ChineseTemplate_0 =  \"\"\"\n",
    "‰Ω†ÊòØ‰∏Ä‰∏™Á≤æÈÄöÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÁöÑÂä©ÊâãÔºåÂØπÊú∫Âô®Â≠¶‰π†ÁöÑÁü•ËØÜÂíåÁÆóÊ≥ïÊúâÁùÄÊ∑±ÂÖ•ÁöÑÁêÜËß£„ÄÇ\\\n",
    "‰Ω†ÊìÖÈïøÂ∞ÜÂ§çÊùÇÁöÑÊú∫Âô®Â≠¶‰π†ÈóÆÈ¢òÊãÜËß£ÊàêÊ∏ÖÊô∞ÊòìÊáÇÁöÑÂ≠êÈóÆÈ¢òÔºå\\\n",
    "Âπ∂ÈíàÂØπÊØè‰∏™Â≠êÈóÆÈ¢òÁªôÂá∫‰∏ì‰∏öÁöÑ‰∏≠ÊñáËß£Á≠îÔºåÊúÄÂêéÂ∞ÜËøô‰∫õËß£Á≠îÊï¥ÂêàÊàêÂÖ®Èù¢ËÄåÂáÜÁ°ÆÁöÑÂõûÁ≠î„ÄÇ\\\n",
    "\n",
    "Áé∞Âú®ÔºåËØ∑ÈíàÂØπ‰ª•‰∏ãÈóÆÈ¢òÁªôÂá∫‰Ω†ÁöÑÂàÜÊûêÂíåËß£Á≠îÔºö\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "ChineseTemplate = \"\"\"\n",
    "‰Ω†ÊòØ‰∏Ä‰∏™Á≤æÈÄöÊú∫Âô®Â≠¶‰π†È¢ÜÂüüÁöÑÂä©ÊâãÔºåÂØπÊú∫Âô®Â≠¶‰π†ÁöÑÁü•ËØÜÂíåÁÆóÊ≥ïÊúâÁùÄÊ∑±ÂÖ•ÁöÑÁêÜËß£„ÄÇ\\\n",
    "‰Ω†ÊìÖÈïøÂ∞ÜÂ§çÊùÇÁöÑÊú∫Âô®Â≠¶‰π†ÈóÆÈ¢òÊãÜËß£ÊàêÊ∏ÖÊô∞ÊòìÊáÇÁöÑÂ≠êÈóÆÈ¢òÔºå\\\n",
    "Âπ∂ÈíàÂØπÊØè‰∏™Â≠êÈóÆÈ¢òÁªôÂá∫‰∏ì‰∏öÁöÑ‰∏≠ÊñáËß£Á≠îÔºåÊúÄÂêéÂ∞ÜËøô‰∫õËß£Á≠îÊï¥ÂêàÊàêÂÖ®Èù¢ËÄåÂáÜÁ°ÆÁöÑÂõûÁ≠î„ÄÇ\\\n",
    "‰ª•‰∏ãÊòØÁõ∏ÂÖ≥Áü•ËØÜÔºö\n",
    "{context}\n",
    "Áé∞Âú®ÔºåËØ∑ÈíàÂØπ‰ª•‰∏ãÈóÆÈ¢òÁªôÂá∫‰Ω†ÁöÑÂàÜÊûêÂíåËß£Á≠îÔºö\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "\n",
    "prompt_templates = [EnglishTemplate_0, ChineseTemplate_0]  \n",
    "prompt_embeddings = embedder.embed_documents(prompt_templates)  \n",
    "\n",
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embedder.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    \n",
    "    # Chosen prompt \n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# example openai chain\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "vecstores =  [FAISS.load_local(folder_path=FAISS_path, embeddings=embedder)]  # ,allow_dangerous_deserialization=True\n",
    "\n",
    "embed_dims = len(embedder.embed_query(\"test\"))\n",
    "def default_FAISS():\n",
    "    '''Useful utility for making an empty FAISS vectorstore'''\n",
    "    return FAISS(\n",
    "        embedding_function=embedder,\n",
    "        index=IndexFlatL2(embed_dims),\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "        normalize_L2=False\n",
    "    )\n",
    "\n",
    "def aggregate_vstores(vectorstores):\n",
    "    ## ÂàùÂßãÂåñ‰∏Ä‰∏™Á©∫ÁöÑ FAISS Á¥¢ÂºïÂπ∂Â∞ÜÂÖ∂‰ªñÁ¥¢ÂºïÂêàÂπ∂Âà∞ÂÖ∂‰∏≠\n",
    "    agg_vstore = default_FAISS()\n",
    "    for vstore in vectorstores:\n",
    "        agg_vstore.merge_from(vstore)\n",
    "    return agg_vstore\n",
    "\n",
    "if 'docstore' not in globals():\n",
    "    docstore = aggregate_vstores(vecstores)\n",
    "\n",
    "\n",
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "latent_dim  = len(embedder.embed_query(\"test\"))\n",
    "\n",
    "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")\n",
    "\n",
    "\n",
    "''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "llm_nemo = ChatNVIDIA(model=\"ai-nemotron-4-340b-instruct\")\n",
    "\n",
    "chain_nv_template = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | llm_nemo\n",
    "    | StrOutputParser()\n",
    "    | RunnableLambda(lambda x: x[:512])\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# RERANKER\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results, k=5):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # ÂàùÂßãÂåñ‰∏Ä‰∏™Â≠óÂÖ∏Êù•‰øùÂ≠òÊØè‰∏™ÂîØ‰∏ÄÊñáÊ°£ÁöÑËûçÂêàÂàÜÊï∞\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Â∞ÜÊñáÊ°£ËΩ¨Êç¢‰∏∫Â≠óÁ¨¶‰∏≤Ê†ºÂºè‰ª•Áî®‰ΩúÈîÆÔºàÂÅáËÆæÊñáÊ°£ÂèØ‰ª•Â∫èÂàóÂåñ‰∏∫ JSONÔºâ\n",
    "            doc_str = dumps(doc)\n",
    "            # Â¶ÇÊûúÊñáÊ°£Â∞öÊú™Âú® fused_scores Â≠óÂÖ∏‰∏≠ÔºåÊ∑ªÂä†ÂàùÂßãÂàÜÊï∞0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            \n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Êõ¥Êñ∞rank\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "            \n",
    "    # print(\"fused_scores\")\n",
    "    # print(doc.page_content)\n",
    "    # ÊéíÂ∫è\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "retriever = docstore.as_retriever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77edb8-21d9-4924-b797-7840bebf427c",
   "metadata": {},
   "source": [
    "### **First generation on question(skip)** \n",
    "- fisrt stage with prompt selection\n",
    "- without RAG\n",
    "- str are clipped to 512 size "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fb110-abfa-4b1b-b67e-a861c4f096b4",
   "metadata": {},
   "source": [
    "API_KEY not enough amout, so just run the final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6ea7fa-d2ff-4c83-bcda-c1d23a83b2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks or questions into smaller, more manageable sub-tasks or sub-questions. This approach allows LLM agents to handle intricate problems more effectively by addressing each component individually and then combining the results to form a comprehensive solution. Here's a step-by-step explanation of task decomposition for LLM agents:\n",
      "\n",
      "1. **Identify the main task or question**: Begin by understanding the ove\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "result = chain_nv_template.invoke(question)\n",
    "print(\"result:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934579f2-7a2d-4bf4-b0db-d3f5a882968e",
   "metadata": {},
   "source": [
    "###  **Retrievial** \n",
    "- second stage with reranker\n",
    "- long recorder\n",
    "- doc2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eefc7a38-a270-41ee-b296-f8cd7fcb03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain_rag_fusion = chain_nv_template | retriever | RunnableLambda(lambda x :reciprocal_rank_fusion([x]))| long_reorder | docs2str\n",
    "# docs = retrieval_chain_rag_fusion.invoke(question)\n",
    "# print(len(docs))\n",
    "# print(docs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a7d01-bb01-4c4a-80c3-6dbbda6207b9",
   "metadata": {},
   "source": [
    "###  **Generation**\n",
    "- self reflect and retrieved data as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6fcf640-4396-41fd-989c-46f0b7afaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "prompt = ChatPromptTemplate.from_template(ChineseTemplate)\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion, \n",
    "        \"query\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt\n",
    "    | llm_nemo\n",
    "    | StrOutputParser()\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "# final_rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d235229d-32e2-4ade-a1d4-152da049225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "convstore = default_FAISS()\n",
    "\n",
    "def save_memory_and_get_output(d, vstore):\n",
    "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
    "    vstore.add_texts([\n",
    "        f\"User previously responded with {d.get('input')}\",\n",
    "        f\"Agent previously responded with {d.get('output')}\"\n",
    "    ])\n",
    "    return d.get('output')\n",
    "\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "    buffer = \"\"\n",
    "    ##È¶ñÂÖàÊ†πÊçÆËæìÂÖ•ÁöÑÊ∂àÊÅØËøõË°åÊ£ÄÁ¥¢\n",
    "    # retrieval = retrieval_chain_rag_fusion.invoke(message)\n",
    "    # print(\"the content of retrieval is:\")\n",
    "    # print(retrieval)\n",
    "    line_buffer = \"\"\n",
    "\n",
    "    ## ÁÑ∂ÂêéÊµÅÂºè‰º†Ëæìstream_chainÁöÑÁªìÊûú\n",
    "    ## ÁÑ∂ÂêéÊµÅÂºè‰º†Ëæìstream_chainÁöÑÁªìÊûú\n",
    "    print(\"Starting stream...\")\n",
    "    \n",
    "    for token in final_rag_chain.stream(message):\n",
    "        buffer += token\n",
    "        ## ‰ºòÂåñ‰ø°ÊÅØÊâìÂç∞ÁöÑÊ†ºÂºè\n",
    "        if not return_buffer:\n",
    "            line_buffer += token\n",
    "            if \"\\n\" in line_buffer:\n",
    "                line_buffer = \"\"\n",
    "            if ((len(line_buffer)>84 and token and token[0] == \" \") or len(line_buffer)>100):\n",
    "                line_buffer = \"\"\n",
    "                yield \"\\n\"\n",
    "                token = \"  \" + token.lstrip()\n",
    "        yield buffer if return_buffer else token\n",
    "    print(\"the content of buffer is:\")\n",
    "    print(buffer)\n",
    "    ##ÊúÄÂêéÂ∞ÜËÅäÂ§©ÂÜÖÂÆπ‰øùÂ≠òÂà∞ÂØπËØùÂÜÖÂ≠òÁºìÂÜ≤Âå∫‰∏≠\n",
    "    # save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a62e5e-0049-448e-9eee-f38f8f407b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:5000\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:5000/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the content of response_gen is:\n",
      "<generator object chat_gen at 0xfffec3c883c0>\n",
      "Starting stream...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar, List\n",
    "\n",
    "@dataclass\n",
    "class DefaultElement:\n",
    "    DEFAULT_MESSAGE: ClassVar[dict] = {\"text\": \"\"}\n",
    "    DEFAULT_HISTORY: ClassVar[List[List[str]]] = []\n",
    "    DEFAULT_DOCUMENT: ClassVar[List[str]] = []\n",
    "    DEFAULT_STATUS: ClassVar[str] = \"Ready!\"  \n",
    "\n",
    "    HELLO_MESSAGE: str = \"Hi üëã, how can I help you today?\"\n",
    "    EMPTY_MESSAGE: str = \"You need to enter your message!\"\n",
    "    ANSWERING_STATUS: str = \"Answering!\"\n",
    "    COMPLETED_STATUS: str = \"Completed!\"\n",
    "    ERROR_MESSAGE: str = \"Service is unavailable.\"\n",
    "\n",
    "\n",
    "class LLMResponse:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def _yield_string(self, message: str):\n",
    "        for i in range(len(message)):\n",
    "            time.sleep(0.01)\n",
    "            yield (\n",
    "                DefaultElement.DEFAULT_MESSAGE,\n",
    "                [[None, message[: i + 1]]],\n",
    "                DefaultElement.DEFAULT_STATUS,\n",
    "            )\n",
    "\n",
    "    def welcome(self):\n",
    "        yield from self._yield_string(DefaultElement.HELLO_MESSAGE)\n",
    "\n",
    "    def empty_message(self):\n",
    "        yield from self._yield_string(DefaultElement.EMPTY_MESSAGE)\n",
    "\n",
    "    def stream_response(self, message: str, history: List[List[str]], response_gen):\n",
    "        for response in response_gen:\n",
    "            updated_history = history + [[message, response]]\n",
    "            yield (DefaultElement.DEFAULT_MESSAGE[\"text\"], updated_history, DefaultElement.ANSWERING_STATUS)\n",
    "        yield (DefaultElement.DEFAULT_MESSAGE[\"text\"], updated_history, DefaultElement.COMPLETED_STATUS)\n",
    "\n",
    "class LocalChatbotUI:\n",
    "    def __init__(self, avatar_images: List[str] = [\"/home/nvidia/NvHacker/assets/user.png\", \"/home/nvidia/NvHacker/assets/bot.png\"]):\n",
    "        self._avatar_images = [os.path.join(os.getcwd(), image) for image in avatar_images]\n",
    "        self._variant = \"panel\"\n",
    "        self._llm_response = LLMResponse()\n",
    "\n",
    "    def _get_response(self, message, chatbot: List[List[str]], progress=gr.Progress(track_tqdm=True)):\n",
    "        if isinstance(message, str):\n",
    "            message = {\"text\": message}\n",
    "\n",
    "        if message[\"text\"] in [None, \"\"]:\n",
    "            for m in self._llm_response.empty_message():\n",
    "                yield m\n",
    "        else:\n",
    "            response_gen = chat_gen(message[\"text\"], history=chatbot)\n",
    "            print(\"the content of response_gen is:\")\n",
    "            print(response_gen)\n",
    "            for m in self._llm_response.stream_response(message[\"text\"], chatbot, response_gen):\n",
    "                yield m\n",
    "\n",
    "    def _welcome(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], [[None, DefaultElement.HELLO_MESSAGE]], DefaultElement.DEFAULT_STATUS\n",
    "\n",
    "    def build(self):\n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Soft(primary_hue=\"slate\"),\n",
    "        ) as demo:\n",
    "            gr.Markdown(\"## ML Chatbot ü§ñ\")\n",
    "            with gr.Tab(\"Interface\"):\n",
    "                with gr.Row(variant=self._variant, equal_height=False):\n",
    "                    with gr.Column(scale=30, variant=self._variant):\n",
    "                        chatbot = gr.Chatbot(\n",
    "                            layout=\"bubble\",\n",
    "                            value=[],\n",
    "                            height=550,\n",
    "                            scale=2,\n",
    "                            show_copy_button=True,\n",
    "                            bubble_full_width=False,\n",
    "                            avatar_images=self._avatar_images,\n",
    "                        )\n",
    "                        with gr.Row(variant=self._variant):\n",
    "                            message = gr.Textbox(\n",
    "                                value=DefaultElement.DEFAULT_MESSAGE[\"text\"],\n",
    "                                placeholder=\"Enter your message:\",\n",
    "                                show_label=False,\n",
    "                                scale=6,\n",
    "                                lines=1,\n",
    "                            )\n",
    "                        with gr.Row(variant=self._variant):\n",
    "                            clear_btn = gr.Button(value=\"Clear\", min_width=20)\n",
    "                            reset_btn = gr.Button(value=\"Reset\", min_width=20)\n",
    "\n",
    "            clear_btn.click(self._clear_chat, outputs=[message, chatbot])\n",
    "            reset_btn.click(self._reset_chat, outputs=[message, chatbot])\n",
    "            message.submit(self._get_response, inputs=[message, chatbot], outputs=[message, chatbot])\n",
    "\n",
    "            demo.load(self._welcome, outputs=[message, chatbot])\n",
    "\n",
    "        return demo\n",
    "\n",
    "    def _clear_chat(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], DefaultElement.DEFAULT_HISTORY\n",
    "\n",
    "    def _reset_chat(self):\n",
    "        return DefaultElement.DEFAULT_MESSAGE[\"text\"], DefaultElement.DEFAULT_HISTORY\n",
    "\n",
    "chatbot_ui = LocalChatbotUI()\n",
    "demo = chatbot_ui.build()\n",
    "demo.launch(debug=True, share=False, show_api=False, server_port=5000, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743c0c69-d8c5-4eef-a793-08bb0b05f877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nvidia/NvHacker/retrive_module/embed\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3eecf6-27cd-4d77-aa87-54d80e0043b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 3354 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6b890-eb6e-40b5-b8cb-c32262021a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
