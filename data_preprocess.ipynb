{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed aggregate docstore with 2903 chunks\n",
      "result: Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks or questions into smaller, more manageable sub-tasks or sub-questions. This approach allows LLM agents to handle intricate problems more effectively by addressing each component individually and then combining the results to form a comprehensive solution. Here's a step-by-step explanation of task decomposition for LLM agents:\n",
      "\n",
      "1. **Identify the main task or question**: Begin by understanding the ove\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch  \n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings  \n",
    "\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Quote from Document] (Document(metadata={'source': 'D:\\\\\\\\RAG NVIDIA\\\\\\\\RAG BOOK\\\\\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='building solutions to challenges in image classification, object detection, and image segmentation.\\\\nChapter 11, Modeling for NLP, focuses on the frequently encountered types of Kaggle challenges \\\\nrelated to natural language processing. We demonstrate how to build an end-to-end solution for \\\\npopular problems like open domain question answering.'), 0.16666666666666666)\\n[Quote from Document] (Document(metadata={'source': 'D:\\\\\\\\RAG NVIDIA\\\\\\\\RAG BOOK\\\\\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 415}, page_content='over, so we’ll begin with a somewhat more interesting variation on the problem: identifying \\\\nsentiment-supporting phrases in a tweet. We’ll proceed to describe an example solution to the \\\\nproblem of open domain question answering and conclude with a section on augmentation for \\\\nNLP problems, which is a topic that receives significantly less attention than its computer vision \\\\ncounterpart.'), 0.125)\\n[Quote from Document] (Document(metadata={'source': 'D:\\\\\\\\RAG NVIDIA\\\\\\\\RAG BOOK\\\\\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 461}, page_content='and use it to generate the next step; our posterior distribution becomes a prior for the next step.For a good explanation of the methods for approaching the general MAB problem, \\\\nthe reader is referred to https://lilianweng.github.io/lil-log/2018/01/23/\\\\nthe-multi-armed-bandit-problem-and-its-solutions.html .'), 0.14285714285714285)\\n[Quote from Document] (Document(metadata={'source': 'D:\\\\\\\\RAG NVIDIA\\\\\\\\RAG BOOK\\\\\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 423}, page_content='Chapter 11 397\\\\nWhile the improvements listed above will undoubtedly boost the performance of the model, the \\\\ncore elements of our pipeline are reusable:\\\\n• Data cleaning and pre-processing\\\\n• Creating text embeddings\\\\n• Incorporating recurrent layers and regularization in the target model architecture\\\\nWe’ll now move on to a discussion of open domain question answering, a frequent problem'), 0.2)\\n\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def docs2str(docs, title=\"Document\"):\n",
    "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
    "    out_str = \"\"\n",
    "    for doc in docs:\n",
    "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
    "        if doc_name:\n",
    "            out_str += f\"[Quote from {doc_name}] \"\n",
    "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
    "    return out_str\n",
    "\n",
    "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
    "\n",
    "\n",
    "b = long_reorder | docs2str\n",
    "b.invoke(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks or questions into smaller, more manageable sub-tasks or sub-questions. This approach allows the LLM agent to handle intricate problems by addressing each sub-task individually and then combining the results to form a comprehensive solution. Here\\'s a step-by-step explanation of task decomposition for LLM agents:\\n\\n1. **Identify the main task or question**: Begin by understanding the primary objective or question that the LLM agent needs to address. This could be a complex problem, such as open-domain question answering, sentiment analysis, or multi-armed bandit problems.\\n\\n2. **Break down the task into sub-tasks**: Analyze the main task and identify its constituent parts or sub-tasks. For instance, in open-domain question answering, the sub-tasks might include:\\n   - Data cleaning and pre-processing\\n   - Creating text embeddings\\n   - Incorporating recurrent layers and regularization in the target model architecture\\n   - Generating answers based on the processed data\\n\\n3. **Formulate sub-questions or sub-tasks**: Translate each sub-task into a clear and concise question or instruction that the LLM agent can understand and address. For example, for the sub-task \"Data cleaning and pre-processing,\" the sub-question could be, \"How can I clean and preprocess the text data to prepare it for analysis?\"\\n\\n4. **Address each sub-task**: Instruct the LLM agent to provide a solution for each sub-question or sub-task. The agent should leverage its knowledge and understanding of machine learning, natural language processing, and other relevant domains to generate accurate and helpful responses.\\n\\n5. **Integrate the solutions**: Combine the solutions to each sub-task to form a comprehensive answer to the original question or task. This may involve aggregating information, applying logical reasoning, or executing a series of steps in a specific order.\\n\\n6. **Evaluate and refine**: Assess the quality and accuracy of the final solution. If necessary, refine the task decomposition, sub-questions, or the LLM agent\\'s responses to improve the overall outcome.\\n\\nBy employing task decomposition, LLM agents can effectively tackle complex problems by dividing them into smaller, more manageable components, ensuring a more structured and accurate approach to problem-solving.'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "prompt = ChatPromptTemplate.from_template(ChineseTemplate)\n",
    "\n",
    "\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_rag_fusion, \n",
    "        \"query\": RunnablePassthrough()\n",
    "    } \n",
    "    | prompt\n",
    "    | llm_nemo\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# from llama_index.embeddings import LangchainEmbedding\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables.passthrough import RunnableAssign\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from faiss import IndexFlatL2\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# import gradio as gr\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "nvidia_api_key = \"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\"\n",
    "# \"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\"\n",
    "# \"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\"\n",
    "\n",
    "\n",
    "assert nvidia_api_key.startswith(\"nvapi-\"), f\"{nvidia_api_key[:5]}... is not a valid key\"\n",
    "os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nemo is a character from the animated film \"Finding Nemo,\" produced by Pixar Animation Studios and released by Walt Disney Pictures in 2003. Nemo is a young clownfish who gets separated from his father, Marlin, and ends up in a fish tank in a dentist's office in Sydney, Australia. The movie follows Marlin's journey to find and rescue Nemo, with the help of a forgetful blue tang fish named Dory. Nemo is a curious, adventurous, and determined character who learns important lessons about trust, independence, and the importance of family throughout the film.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatNVIDIA(model=\"ai-nemotron-4-340b-instruct\")\n",
    "result = llm.invoke(\"what is nemo\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0190887451171875,\n",
       " -0.017364501953125,\n",
       " -0.034393310546875,\n",
       " -0.005931854248046875,\n",
       " 0.03228759765625,\n",
       " 0.046234130859375,\n",
       " 0.0012292861938476562,\n",
       " -0.01788330078125,\n",
       " 0.0220184326171875,\n",
       " 0.004703521728515625,\n",
       " 0.08740234375,\n",
       " 0.0169830322265625,\n",
       " 0.0224151611328125,\n",
       " -0.03936767578125,\n",
       " 0.0138702392578125,\n",
       " 0.0213623046875,\n",
       " -0.0003600120544433594,\n",
       " -0.0188751220703125,\n",
       " -0.02789306640625,\n",
       " 0.0257110595703125,\n",
       " 0.014892578125,\n",
       " 0.0013837814331054688,\n",
       " 0.052215576171875,\n",
       " -0.00373077392578125,\n",
       " 0.0179595947265625,\n",
       " 0.0169830322265625,\n",
       " -0.01444244384765625,\n",
       " -0.034393310546875,\n",
       " 0.034576416015625,\n",
       " 0.0621337890625,\n",
       " -0.034637451171875,\n",
       " -0.0909423828125,\n",
       " 0.0003077983856201172,\n",
       " 0.00475311279296875,\n",
       " 0.0156402587890625,\n",
       " -0.0426025390625,\n",
       " 0.04205322265625,\n",
       " 0.0124969482421875,\n",
       " -0.034881591796875,\n",
       " 0.0009160041809082031,\n",
       " 0.00859832763671875,\n",
       " -0.0204315185546875,\n",
       " -0.00238037109375,\n",
       " 0.0194854736328125,\n",
       " -0.0233001708984375,\n",
       " 0.00341033935546875,\n",
       " -0.00611114501953125,\n",
       " -0.00788116455078125,\n",
       " 0.002643585205078125,\n",
       " -0.040863037109375,\n",
       " 0.042938232421875,\n",
       " -0.008880615234375,\n",
       " 0.03448486328125,\n",
       " -0.0023441314697265625,\n",
       " -0.014984130859375,\n",
       " 0.00989532470703125,\n",
       " 0.1051025390625,\n",
       " 0.066162109375,\n",
       " -0.07086181640625,\n",
       " -0.03619384765625,\n",
       " 0.0052032470703125,\n",
       " -0.10235595703125,\n",
       " 0.0173187255859375,\n",
       " 0.023040771484375,\n",
       " -0.0380859375,\n",
       " 0.04815673828125,\n",
       " -0.04193115234375,\n",
       " 0.03826904296875,\n",
       " -0.01280975341796875,\n",
       " -0.01461029052734375,\n",
       " -0.0010318756103515625,\n",
       " 0.0251922607421875,\n",
       " 0.00609588623046875,\n",
       " -0.02691650390625,\n",
       " -0.0215606689453125,\n",
       " -0.0477294921875,\n",
       " 0.015777587890625,\n",
       " -0.00936126708984375,\n",
       " 0.03851318359375,\n",
       " 0.046173095703125,\n",
       " 0.0499267578125,\n",
       " 0.045684814453125,\n",
       " -0.006832122802734375,\n",
       " 0.006870269775390625,\n",
       " 0.0234832763671875,\n",
       " -0.046966552734375,\n",
       " 0.06072998046875,\n",
       " -0.04840087890625,\n",
       " -0.020599365234375,\n",
       " 0.0017042160034179688,\n",
       " 0.005588531494140625,\n",
       " -0.0215606689453125,\n",
       " 0.00537109375,\n",
       " 0.0202484130859375,\n",
       " -0.00021719932556152344,\n",
       " -0.005279541015625,\n",
       " -0.01922607421875,\n",
       " 0.031524658203125,\n",
       " -0.015411376953125,\n",
       " -0.04034423828125,\n",
       " 0.00945281982421875,\n",
       " 0.01470947265625,\n",
       " 0.06707763671875,\n",
       " 0.01525115966796875,\n",
       " -0.037139892578125,\n",
       " -0.00420379638671875,\n",
       " 0.00873565673828125,\n",
       " -0.016876220703125,\n",
       " 0.0169830322265625,\n",
       " 0.0021381378173828125,\n",
       " -0.01776123046875,\n",
       " 0.017822265625,\n",
       " -0.0260009765625,\n",
       " 0.05078125,\n",
       " 0.04266357421875,\n",
       " -0.0228424072265625,\n",
       " -0.0285491943359375,\n",
       " 0.006191253662109375,\n",
       " 0.025726318359375,\n",
       " 0.053985595703125,\n",
       " 0.01094818115234375,\n",
       " -0.048858642578125,\n",
       " 0.016265869140625,\n",
       " 0.00533294677734375,\n",
       " -0.029876708984375,\n",
       " 0.049102783203125,\n",
       " -0.0019292831420898438,\n",
       " -0.02593994140625,\n",
       " -0.0282135009765625,\n",
       " 0.03179931640625,\n",
       " -0.0169677734375,\n",
       " -0.02435302734375,\n",
       " 0.0118865966796875,\n",
       " 0.0136566162109375,\n",
       " 0.06927490234375,\n",
       " -0.0386962890625,\n",
       " -0.11871337890625,\n",
       " -0.06524658203125,\n",
       " 0.003101348876953125,\n",
       " -0.007762908935546875,\n",
       " -0.03436279296875,\n",
       " -0.03826904296875,\n",
       " -0.03369140625,\n",
       " -0.00778961181640625,\n",
       " 0.05126953125,\n",
       " 0.0183563232421875,\n",
       " 0.0301513671875,\n",
       " 0.01137542724609375,\n",
       " -0.0177001953125,\n",
       " 0.03106689453125,\n",
       " -0.0163421630859375,\n",
       " -0.0226593017578125,\n",
       " 0.0023059844970703125,\n",
       " 0.03656005859375,\n",
       " 0.03021240234375,\n",
       " -0.076904296875,\n",
       " 0.01393890380859375,\n",
       " 0.01305389404296875,\n",
       " -0.0305023193359375,\n",
       " -0.017913818359375,\n",
       " -0.00897979736328125,\n",
       " -0.0011796951293945312,\n",
       " -0.008392333984375,\n",
       " 0.0015954971313476562,\n",
       " -0.0235595703125,\n",
       " -0.0251922607421875,\n",
       " 0.08319091796875,\n",
       " 0.0213775634765625,\n",
       " -0.00811767578125,\n",
       " 0.102294921875,\n",
       " 0.012603759765625,\n",
       " 0.02154541015625,\n",
       " 0.029937744140625,\n",
       " 0.043670654296875,\n",
       " 0.009979248046875,\n",
       " -0.05181884765625,\n",
       " 0.0255279541015625,\n",
       " -0.005649566650390625,\n",
       " 0.0238189697265625,\n",
       " 0.002338409423828125,\n",
       " -0.02691650390625,\n",
       " 0.0156402587890625,\n",
       " 0.038177490234375,\n",
       " 0.055511474609375,\n",
       " -0.006137847900390625,\n",
       " 0.040374755859375,\n",
       " 0.0017728805541992188,\n",
       " -0.02301025390625,\n",
       " 0.0083770751953125,\n",
       " -0.0665283203125,\n",
       " -0.0113067626953125,\n",
       " -0.0426025390625,\n",
       " 0.043365478515625,\n",
       " -0.030242919921875,\n",
       " -0.0219268798828125,\n",
       " 0.01165771484375,\n",
       " 0.0087738037109375,\n",
       " 0.0309600830078125,\n",
       " -0.047454833984375,\n",
       " -0.0202178955078125,\n",
       " -0.0313720703125,\n",
       " 0.0218963623046875,\n",
       " 0.05560302734375,\n",
       " 0.005451202392578125,\n",
       " 0.045928955078125,\n",
       " 0.049102783203125,\n",
       " 0.025054931640625,\n",
       " -0.017303466796875,\n",
       " 0.0184478759765625,\n",
       " -0.005451202392578125,\n",
       " -0.03936767578125,\n",
       " -0.020751953125,\n",
       " -0.059295654296875,\n",
       " 0.01096343994140625,\n",
       " -0.004657745361328125,\n",
       " -0.00533294677734375,\n",
       " 0.00496673583984375,\n",
       " -0.002452850341796875,\n",
       " -0.0377197265625,\n",
       " 0.05633544921875,\n",
       " 0.0177154541015625,\n",
       " 0.033843994140625,\n",
       " -0.004878997802734375,\n",
       " -0.0222015380859375,\n",
       " -0.0118865966796875,\n",
       " -0.03936767578125,\n",
       " -0.0287933349609375,\n",
       " 0.033416748046875,\n",
       " -0.03765869140625,\n",
       " -0.0021610260009765625,\n",
       " -0.02984619140625,\n",
       " 0.02618408203125,\n",
       " 0.01548004150390625,\n",
       " -0.005340576171875,\n",
       " -0.031646728515625,\n",
       " -0.007503509521484375,\n",
       " 0.0638427734375,\n",
       " 0.0031490325927734375,\n",
       " -0.0013980865478515625,\n",
       " 0.0260467529296875,\n",
       " 0.01617431640625,\n",
       " -0.0212860107421875,\n",
       " -0.00876617431640625,\n",
       " 0.01953125,\n",
       " 0.0003323554992675781,\n",
       " 0.0633544921875,\n",
       " 0.01030731201171875,\n",
       " 0.041748046875,\n",
       " 0.03704833984375,\n",
       " -0.00826263427734375,\n",
       " 0.021026611328125,\n",
       " 0.029876708984375,\n",
       " 0.016448974609375,\n",
       " -0.0243988037109375,\n",
       " 0.0216522216796875,\n",
       " -0.0218658447265625,\n",
       " 0.01611328125,\n",
       " -0.009857177734375,\n",
       " 0.0050506591796875,\n",
       " -0.0051422119140625,\n",
       " 0.0017786026000976562,\n",
       " 0.012908935546875,\n",
       " -0.0052642822265625,\n",
       " -0.01763916015625,\n",
       " 0.032073974609375,\n",
       " -0.0114898681640625,\n",
       " 0.0127716064453125,\n",
       " -0.007106781005859375,\n",
       " 0.07147216796875,\n",
       " -0.03106689453125,\n",
       " -0.0009083747863769531,\n",
       " -0.00997161865234375,\n",
       " -0.00991058349609375,\n",
       " -0.0200347900390625,\n",
       " 0.033203125,\n",
       " 0.03662109375,\n",
       " 0.0231170654296875,\n",
       " 0.03759765625,\n",
       " -0.002140045166015625,\n",
       " -0.04296875,\n",
       " -0.037628173828125,\n",
       " -0.043548583984375,\n",
       " 0.017364501953125,\n",
       " 0.02288818359375,\n",
       " -0.0355224609375,\n",
       " -0.01129913330078125,\n",
       " -0.060821533203125,\n",
       " -0.04168701171875,\n",
       " 0.0382080078125,\n",
       " 0.009613037109375,\n",
       " 0.0301513671875,\n",
       " 0.049346923828125,\n",
       " -0.01522064208984375,\n",
       " 0.0038394927978515625,\n",
       " -0.0360107421875,\n",
       " -0.046417236328125,\n",
       " 0.0305938720703125,\n",
       " -0.01421356201171875,\n",
       " 0.033172607421875,\n",
       " 0.056060791015625,\n",
       " -0.04095458984375,\n",
       " 0.0247344970703125,\n",
       " 0.03338623046875,\n",
       " -0.01488494873046875,\n",
       " -0.005886077880859375,\n",
       " -0.027313232421875,\n",
       " -0.035003662109375,\n",
       " -0.0151824951171875,\n",
       " -0.0204315185546875,\n",
       " 0.0728759765625,\n",
       " 0.053436279296875,\n",
       " -0.0732421875,\n",
       " 0.00455474853515625,\n",
       " -0.0325927734375,\n",
       " -0.04248046875,\n",
       " 0.006072998046875,\n",
       " 0.0223236083984375,\n",
       " 0.031768798828125,\n",
       " -0.00530242919921875,\n",
       " 0.016448974609375,\n",
       " 0.036865234375,\n",
       " 0.0201263427734375,\n",
       " -0.0252838134765625,\n",
       " -0.003086090087890625,\n",
       " 0.0095977783203125,\n",
       " -0.056854248046875,\n",
       " -0.032928466796875,\n",
       " 0.004085540771484375,\n",
       " 0.01947021484375,\n",
       " 0.044403076171875,\n",
       " -0.0234527587890625,\n",
       " 0.014556884765625,\n",
       " -0.01349639892578125,\n",
       " 0.0260467529296875,\n",
       " 0.01227569580078125,\n",
       " -0.002384185791015625,\n",
       " -0.0017852783203125,\n",
       " 0.03643798828125,\n",
       " -0.0303192138671875,\n",
       " -0.0119171142578125,\n",
       " 0.0108489990234375,\n",
       " -0.01241302490234375,\n",
       " 0.0030307769775390625,\n",
       " -0.0220489501953125,\n",
       " 0.006591796875,\n",
       " -0.007411956787109375,\n",
       " -0.006195068359375,\n",
       " 0.05419921875,\n",
       " -0.018035888671875,\n",
       " 0.02093505859375,\n",
       " -0.01087188720703125,\n",
       " 0.0357666015625,\n",
       " 0.00902557373046875,\n",
       " -0.0074615478515625,\n",
       " 0.007762908935546875,\n",
       " 0.0292510986328125,\n",
       " 0.0264739990234375,\n",
       " 0.0216064453125,\n",
       " -0.00875091552734375,\n",
       " 0.0186767578125,\n",
       " 0.055419921875,\n",
       " -0.049896240234375,\n",
       " -0.039886474609375,\n",
       " -0.04205322265625,\n",
       " 0.03887939453125,\n",
       " 0.01922607421875,\n",
       " -0.0168914794921875,\n",
       " -0.0106353759765625,\n",
       " 0.0010890960693359375,\n",
       " 0.02789306640625,\n",
       " 0.0418701171875,\n",
       " 0.0222320556640625,\n",
       " -0.034393310546875,\n",
       " 0.049835205078125,\n",
       " -0.033111572265625,\n",
       " -0.024169921875,\n",
       " 0.0223846435546875,\n",
       " 0.023651123046875,\n",
       " 0.009246826171875,\n",
       " 0.052581787109375,\n",
       " 0.04876708984375,\n",
       " -0.015655517578125,\n",
       " 0.018798828125,\n",
       " 0.02789306640625,\n",
       " -0.06878662109375,\n",
       " -0.02325439453125,\n",
       " 0.007579803466796875,\n",
       " -0.013397216796875,\n",
       " 0.0304412841796875,\n",
       " 0.00820159912109375,\n",
       " -0.004669189453125,\n",
       " -0.0025119781494140625,\n",
       " -0.0214080810546875,\n",
       " -0.00274658203125,\n",
       " 0.032684326171875,\n",
       " -0.0204010009765625,\n",
       " 0.02618408203125,\n",
       " -0.00664520263671875,\n",
       " -0.0040740966796875,\n",
       " 0.0288543701171875,\n",
       " 0.04266357421875,\n",
       " 0.03704833984375,\n",
       " -0.01280975341796875,\n",
       " -0.064453125,\n",
       " 0.037841796875,\n",
       " 0.0452880859375,\n",
       " -0.0159454345703125,\n",
       " -0.032318115234375,\n",
       " -0.016510009765625,\n",
       " 0.0286407470703125,\n",
       " 0.0570068359375,\n",
       " 0.05938720703125,\n",
       " 0.036468505859375,\n",
       " -0.04669189453125,\n",
       " -0.00909423828125,\n",
       " -0.027618408203125,\n",
       " 0.0286865234375,\n",
       " 0.008148193359375,\n",
       " 0.0017795562744140625,\n",
       " -0.018157958984375,\n",
       " -0.049835205078125,\n",
       " -0.02960205078125,\n",
       " 0.004482269287109375,\n",
       " 0.030120849609375,\n",
       " 0.006439208984375,\n",
       " 0.0218963623046875,\n",
       " 0.006816864013671875,\n",
       " -0.0163726806640625,\n",
       " -0.01456451416015625,\n",
       " -0.032135009765625,\n",
       " 0.005039215087890625,\n",
       " 0.0293121337890625,\n",
       " -0.016510009765625,\n",
       " -0.00800323486328125,\n",
       " -0.0615234375,\n",
       " -0.01332855224609375,\n",
       " -0.0262451171875,\n",
       " 0.03216552734375,\n",
       " 0.043792724609375,\n",
       " 0.005779266357421875,\n",
       " 0.032135009765625,\n",
       " 0.013427734375,\n",
       " -0.0198822021484375,\n",
       " -0.0127716064453125,\n",
       " -0.017913818359375,\n",
       " 0.01316070556640625,\n",
       " 0.05804443359375,\n",
       " -0.022705078125,\n",
       " -0.00879669189453125,\n",
       " 0.017913818359375,\n",
       " 0.017547607421875,\n",
       " -0.014129638671875,\n",
       " -0.01324462890625,\n",
       " -0.0081024169921875,\n",
       " -0.134521484375,\n",
       " 0.0147705078125,\n",
       " 0.006500244140625,\n",
       " 0.01995849609375,\n",
       " -0.033721923828125,\n",
       " -0.03497314453125,\n",
       " -0.017669677734375,\n",
       " -0.0198516845703125,\n",
       " 0.003528594970703125,\n",
       " -0.05126953125,\n",
       " -0.0263824462890625,\n",
       " -0.053985595703125,\n",
       " 0.004405975341796875,\n",
       " -0.08929443359375,\n",
       " 0.0166473388671875,\n",
       " -0.00785064697265625,\n",
       " 0.0411376953125,\n",
       " -0.0228424072265625,\n",
       " -0.00641632080078125,\n",
       " -0.051910400390625,\n",
       " 0.01316070556640625,\n",
       " 0.04254150390625,\n",
       " -0.0081939697265625,\n",
       " -0.047119140625,\n",
       " 0.008941650390625,\n",
       " -0.00852203369140625,\n",
       " 0.00397491455078125,\n",
       " 0.01448822021484375,\n",
       " 0.044036865234375,\n",
       " -0.040313720703125,\n",
       " -0.01776123046875,\n",
       " -0.01203155517578125,\n",
       " 0.0259552001953125,\n",
       " -0.005573272705078125,\n",
       " -0.0033283233642578125,\n",
       " 0.004055023193359375,\n",
       " -0.00763702392578125,\n",
       " 0.00864410400390625,\n",
       " -0.0419921875,\n",
       " -0.049285888671875,\n",
       " 0.045654296875,\n",
       " -0.0098876953125,\n",
       " -0.00554656982421875,\n",
       " 0.03167724609375,\n",
       " 0.0165557861328125,\n",
       " 0.01226806640625,\n",
       " 0.02728271484375,\n",
       " 0.0038700103759765625,\n",
       " -0.0023651123046875,\n",
       " 0.0008077621459960938,\n",
       " 0.0125274658203125,\n",
       " -0.0174560546875,\n",
       " -0.02447509765625,\n",
       " 0.006839752197265625,\n",
       " -0.031646728515625,\n",
       " -0.00907135009765625,\n",
       " 0.08160400390625,\n",
       " -0.0287933349609375,\n",
       " -0.0126190185546875,\n",
       " -0.0017290115356445312,\n",
       " 0.00756072998046875,\n",
       " -0.013763427734375,\n",
       " 0.01885986328125,\n",
       " 0.024078369140625,\n",
       " -0.01488494873046875,\n",
       " 0.0247344970703125,\n",
       " -0.0300750732421875,\n",
       " -0.048583984375,\n",
       " 0.00719451904296875,\n",
       " 0.0217132568359375,\n",
       " -0.025146484375,\n",
       " -0.00431060791015625,\n",
       " 0.01727294921875,\n",
       " 0.01126861572265625,\n",
       " 0.0204925537109375,\n",
       " -0.00470733642578125,\n",
       " -0.01763916015625,\n",
       " -0.00855255126953125,\n",
       " -0.040191650390625,\n",
       " 0.004199981689453125,\n",
       " 0.0214080810546875,\n",
       " -0.004665374755859375,\n",
       " -0.006561279296875,\n",
       " -0.045928955078125,\n",
       " -0.0001741647720336914,\n",
       " 0.04144287109375,\n",
       " 0.019561767578125,\n",
       " 0.012664794921875,\n",
       " -0.01416015625,\n",
       " 0.0638427734375,\n",
       " 0.0234832763671875,\n",
       " 0.0288848876953125,\n",
       " -0.06396484375,\n",
       " -0.0237884521484375,\n",
       " -0.01168060302734375,\n",
       " -0.00020647048950195312,\n",
       " 0.0595703125,\n",
       " -0.04608154296875,\n",
       " -0.0114593505859375,\n",
       " -0.01224517822265625,\n",
       " -0.0067596435546875,\n",
       " -0.0015964508056640625,\n",
       " 0.0089569091796875,\n",
       " 0.0137176513671875,\n",
       " 0.01154327392578125,\n",
       " 0.007564544677734375,\n",
       " -0.00864410400390625,\n",
       " 0.035430908203125,\n",
       " 0.0264892578125,\n",
       " -0.00023424625396728516,\n",
       " -0.018096923828125,\n",
       " 0.01019287109375,\n",
       " -0.00298309326171875,\n",
       " -0.01436614990234375,\n",
       " -0.0135040283203125,\n",
       " -0.03759765625,\n",
       " 0.006938934326171875,\n",
       " -0.0010423660278320312,\n",
       " -0.03826904296875,\n",
       " 0.00933837890625,\n",
       " 0.0105743408203125,\n",
       " 0.002193450927734375,\n",
       " -0.0270843505859375,\n",
       " -0.0031490325927734375,\n",
       " -0.04376220703125,\n",
       " -0.03466796875,\n",
       " 0.022705078125,\n",
       " -0.006366729736328125,\n",
       " -0.0157623291015625,\n",
       " -0.010467529296875,\n",
       " -0.01200103759765625,\n",
       " -0.04888916015625,\n",
       " -0.002201080322265625,\n",
       " 0.009674072265625,\n",
       " 0.021575927734375,\n",
       " -0.036376953125,\n",
       " 0.017364501953125,\n",
       " 0.026275634765625,\n",
       " -1.519918441772461e-05,\n",
       " 0.01444244384765625,\n",
       " -0.037017822265625,\n",
       " 0.01053619384765625,\n",
       " -0.0009946823120117188,\n",
       " -0.0518798828125,\n",
       " 0.0157012939453125,\n",
       " -0.0131378173828125,\n",
       " 0.003284454345703125,\n",
       " -0.031890869140625,\n",
       " -0.0013294219970703125,\n",
       " 0.0007457733154296875,\n",
       " -0.05804443359375,\n",
       " 0.059356689453125,\n",
       " -0.008148193359375,\n",
       " 0.03302001953125,\n",
       " 0.0318603515625,\n",
       " 0.0026226043701171875,\n",
       " 0.05126953125,\n",
       " -0.00992584228515625,\n",
       " 0.023773193359375,\n",
       " 0.004322052001953125,\n",
       " 0.00948333740234375,\n",
       " 0.044708251953125,\n",
       " 0.0186309814453125,\n",
       " 0.030487060546875,\n",
       " 0.0287017822265625,\n",
       " 0.000232696533203125,\n",
       " -0.03375244140625,\n",
       " 0.028350830078125,\n",
       " -0.0223236083984375,\n",
       " 0.03912353515625,\n",
       " -0.0137176513671875,\n",
       " -0.043731689453125,\n",
       " -0.0171051025390625,\n",
       " -0.0012063980102539062,\n",
       " -0.06060791015625,\n",
       " 0.01435089111328125,\n",
       " -0.0309600830078125,\n",
       " 0.016876220703125,\n",
       " 0.019866943359375,\n",
       " -0.021514892578125,\n",
       " 0.004253387451171875,\n",
       " 0.008880615234375,\n",
       " -0.0214691162109375,\n",
       " -0.040069580078125,\n",
       " 0.0182342529296875,\n",
       " 0.0205535888671875,\n",
       " 0.03411865234375,\n",
       " 0.0120086669921875,\n",
       " -0.01255035400390625,\n",
       " 0.060638427734375,\n",
       " 0.0038299560546875,\n",
       " 0.048431396484375,\n",
       " -0.025726318359375,\n",
       " -0.06781005859375,\n",
       " -0.003261566162109375,\n",
       " -0.01462554931640625,\n",
       " -0.024444580078125,\n",
       " 0.045745849609375,\n",
       " -0.0263671875,\n",
       " -0.00124359130859375,\n",
       " -0.006748199462890625,\n",
       " -0.031829833984375,\n",
       " 0.046600341796875,\n",
       " -0.03179931640625,\n",
       " -0.00772857666015625,\n",
       " -0.033172607421875,\n",
       " 0.046630859375,\n",
       " 0.009124755859375,\n",
       " 0.00525665283203125,\n",
       " -0.00571441650390625,\n",
       " 0.0203704833984375,\n",
       " 0.2030029296875,\n",
       " 0.05615234375,\n",
       " 0.007312774658203125,\n",
       " -0.09307861328125,\n",
       " -0.032012939453125,\n",
       " -0.033538818359375,\n",
       " -0.0044097900390625,\n",
       " 0.0006666183471679688,\n",
       " -0.0307769775390625,\n",
       " -0.003692626953125,\n",
       " -0.0158843994140625,\n",
       " 0.04241943359375,\n",
       " 0.05975341796875,\n",
       " 0.007328033447265625,\n",
       " 0.0182647705078125,\n",
       " 0.08154296875,\n",
       " -0.0007753372192382812,\n",
       " -0.047393798828125,\n",
       " 0.01312255859375,\n",
       " 0.0065155029296875,\n",
       " 0.0214996337890625,\n",
       " 0.048675537109375,\n",
       " -0.01078033447265625,\n",
       " -0.009796142578125,\n",
       " 0.01715087890625,\n",
       " 0.00620269775390625,\n",
       " 0.01617431640625,\n",
       " -0.01268768310546875,\n",
       " 0.0260162353515625,\n",
       " 0.0347900390625,\n",
       " -0.038818359375,\n",
       " -0.00601959228515625,\n",
       " -0.02093505859375,\n",
       " -0.004093170166015625,\n",
       " -0.0171661376953125,\n",
       " 0.01096343994140625,\n",
       " 0.0114593505859375,\n",
       " 0.0225372314453125,\n",
       " -0.02447509765625,\n",
       " 0.01849365234375,\n",
       " -0.0225830078125,\n",
       " -0.0002923011779785156,\n",
       " -0.00037217140197753906,\n",
       " 0.0139923095703125,\n",
       " -0.006725311279296875,\n",
       " -0.0025157928466796875,\n",
       " -0.0015516281127929688,\n",
       " 0.0055999755859375,\n",
       " -0.02294921875,\n",
       " -0.0174102783203125,\n",
       " -0.0264434814453125,\n",
       " 0.0163421630859375,\n",
       " -0.0027790069580078125,\n",
       " -0.045501708984375,\n",
       " -0.0335693359375,\n",
       " 0.040679931640625,\n",
       " 0.007572174072265625,\n",
       " -0.052490234375,\n",
       " -0.02178955078125,\n",
       " -0.056610107421875,\n",
       " 0.01027679443359375,\n",
       " -0.01172637939453125,\n",
       " 0.062103271484375,\n",
       " -0.0086822509765625,\n",
       " -0.01934814453125,\n",
       " 0.04095458984375,\n",
       " -0.0114593505859375,\n",
       " 0.0694580078125,\n",
       " 0.0038299560546875,\n",
       " -0.02703857421875,\n",
       " 0.033172607421875,\n",
       " 0.019439697265625,\n",
       " -0.0149688720703125,\n",
       " -0.00014579296112060547,\n",
       " 0.040557861328125,\n",
       " 0.0369873046875,\n",
       " 0.0061492919921875,\n",
       " 0.0292510986328125,\n",
       " -0.0184173583984375,\n",
       " 0.007808685302734375,\n",
       " 0.004192352294921875,\n",
       " -0.0009765625,\n",
       " -0.027435302734375,\n",
       " -0.06640625,\n",
       " -0.0200958251953125,\n",
       " 0.00859832763671875,\n",
       " 0.041015625,\n",
       " 0.007129669189453125,\n",
       " 0.04522705078125,\n",
       " -0.04705810546875,\n",
       " -0.016204833984375,\n",
       " 0.0465087890625,\n",
       " 0.0257110595703125,\n",
       " -0.0211944580078125,\n",
       " -0.006130218505859375,\n",
       " 0.0221405029296875,\n",
       " -0.00457763671875,\n",
       " 0.007526397705078125,\n",
       " -0.030120849609375,\n",
       " 0.01654052734375,\n",
       " 0.044219970703125,\n",
       " 0.03179931640625,\n",
       " -0.03887939453125,\n",
       " 0.0160980224609375,\n",
       " -0.02899169921875,\n",
       " -0.00714111328125,\n",
       " 0.053070068359375,\n",
       " -0.030059814453125,\n",
       " -0.09906005859375,\n",
       " -0.0006222724914550781,\n",
       " 0.01250457763671875,\n",
       " -0.050018310546875,\n",
       " -0.0228729248046875,\n",
       " -0.08709716796875,\n",
       " -0.021270751953125,\n",
       " 0.009002685546875,\n",
       " -0.0238189697265625,\n",
       " -0.003925323486328125,\n",
       " 0.02447509765625,\n",
       " -0.021331787109375,\n",
       " 0.0149383544921875,\n",
       " -0.050933837890625,\n",
       " 0.01837158203125,\n",
       " -0.057403564453125,\n",
       " -0.0306243896484375,\n",
       " -0.03448486328125,\n",
       " 0.0215911865234375,\n",
       " 0.0036220550537109375,\n",
       " 0.0017576217651367188,\n",
       " -0.04412841796875,\n",
       " -0.01290130615234375,\n",
       " 0.028411865234375,\n",
       " 0.03802490234375,\n",
       " 0.01214599609375,\n",
       " 0.0491943359375,\n",
       " -0.019195556640625,\n",
       " 0.004169464111328125,\n",
       " 0.01800537109375,\n",
       " -0.022918701171875,\n",
       " 0.0118408203125,\n",
       " 0.01267242431640625,\n",
       " -0.03271484375,\n",
       " -0.03533935546875,\n",
       " 0.04534912109375,\n",
       " -0.03509521484375,\n",
       " 0.0150146484375,\n",
       " 0.039764404296875,\n",
       " -0.0189056396484375,\n",
       " 0.0262298583984375,\n",
       " -0.004924774169921875,\n",
       " -0.035858154296875,\n",
       " -0.01326751708984375,\n",
       " -0.00989532470703125,\n",
       " 0.00836944580078125,\n",
       " -0.01904296875,\n",
       " 0.002216339111328125,\n",
       " 0.01142120361328125,\n",
       " 0.00733184814453125,\n",
       " -0.02130126953125,\n",
       " 0.0169677734375,\n",
       " 0.0139007568359375,\n",
       " 0.0265350341796875,\n",
       " 0.022369384765625,\n",
       " 0.02752685546875,\n",
       " 0.005054473876953125,\n",
       " 0.00778961181640625,\n",
       " 0.042816162109375,\n",
       " -0.037567138671875,\n",
       " -0.0023326873779296875,\n",
       " 0.0032196044921875,\n",
       " -0.015899658203125,\n",
       " 0.06329345703125,\n",
       " 0.00606536865234375,\n",
       " 0.00852203369140625,\n",
       " -0.0836181640625,\n",
       " -0.02313232421875,\n",
       " 0.0212860107421875,\n",
       " -0.01335906982421875,\n",
       " 0.0125274658203125,\n",
       " -0.01141357421875,\n",
       " -0.031707763671875,\n",
       " 0.0117645263671875,\n",
       " -0.034576416015625,\n",
       " 0.0369873046875,\n",
       " -0.04583740234375,\n",
       " -0.05938720703125,\n",
       " 0.016387939453125,\n",
       " 0.00943756103515625,\n",
       " 0.03857421875,\n",
       " 0.004116058349609375,\n",
       " -0.00601959228515625,\n",
       " 0.021209716796875,\n",
       " 0.0006489753723144531,\n",
       " 0.025146484375,\n",
       " -0.006160736083984375,\n",
       " 0.039154052734375,\n",
       " -0.0015773773193359375,\n",
       " 0.009490966796875,\n",
       " 0.0211944580078125,\n",
       " -0.03955078125,\n",
       " 0.047393798828125,\n",
       " -0.03118896484375,\n",
       " 0.023529052734375,\n",
       " 0.0249481201171875,\n",
       " 0.03155517578125,\n",
       " 0.036163330078125,\n",
       " 0.027008056640625,\n",
       " 0.054534912109375,\n",
       " -0.00600433349609375,\n",
       " 0.050201416015625,\n",
       " 0.019805908203125,\n",
       " -0.03533935546875,\n",
       " -0.032501220703125,\n",
       " 0.02020263671875,\n",
       " 0.039581298828125,\n",
       " 0.055908203125,\n",
       " -0.036376953125,\n",
       " -0.0021495819091796875,\n",
       " -0.0207977294921875,\n",
       " -0.0048828125,\n",
       " -0.028778076171875,\n",
       " 0.043548583984375,\n",
       " 0.0181884765625,\n",
       " -0.0283050537109375,\n",
       " 0.0079345703125,\n",
       " 0.018096923828125,\n",
       " 0.032684326171875,\n",
       " -0.049285888671875,\n",
       " -0.037261962890625,\n",
       " -0.0035343170166015625,\n",
       " 0.0269012451171875,\n",
       " 0.01155853271484375,\n",
       " -0.028656005859375,\n",
       " 0.005008697509765625,\n",
       " -0.03546142578125,\n",
       " 0.0135040283203125,\n",
       " 0.0105438232421875,\n",
       " -0.0253753662109375,\n",
       " 0.0251007080078125,\n",
       " -0.0301513671875,\n",
       " -0.045654296875,\n",
       " 0.01971435546875,\n",
       " -0.0418701171875,\n",
       " -0.006160736083984375,\n",
       " -0.040679931640625,\n",
       " -0.03662109375,\n",
       " 0.03240966796875,\n",
       " 0.11163330078125,\n",
       " -0.011962890625,\n",
       " -0.019073486328125,\n",
       " -0.0032329559326171875,\n",
       " 0.01477813720703125,\n",
       " -0.00238800048828125,\n",
       " -0.016937255859375,\n",
       " 0.055145263671875,\n",
       " -0.038787841796875,\n",
       " -0.01520538330078125,\n",
       " -0.00116729736328125,\n",
       " -0.00647735595703125,\n",
       " -0.0141754150390625,\n",
       " -0.004520416259765625,\n",
       " 0.046783447265625,\n",
       " 0.032928466796875,\n",
       " 0.00033402442932128906,\n",
       " -0.028411865234375,\n",
       " -0.017578125,\n",
       " 0.0010042190551757812,\n",
       " -0.03839111328125,\n",
       " -0.0022678375244140625,\n",
       " -0.040252685546875,\n",
       " -0.04644775390625,\n",
       " -0.024261474609375,\n",
       " 0.0191650390625,\n",
       " 0.0179290771484375,\n",
       " 0.004283905029296875,\n",
       " -0.00405120849609375,\n",
       " -0.041107177734375,\n",
       " -0.005924224853515625,\n",
       " -0.035980224609375,\n",
       " 0.0066680908203125,\n",
       " 0.058837890625,\n",
       " 0.02349853515625,\n",
       " -0.0268402099609375,\n",
       " 0.0266265869140625,\n",
       " -0.046173095703125,\n",
       " 0.0335693359375,\n",
       " 0.0008349418640136719,\n",
       " -0.0202484130859375,\n",
       " -0.03363037109375,\n",
       " -0.0177154541015625,\n",
       " -0.040008544921875,\n",
       " 0.004146575927734375,\n",
       " 0.0031280517578125,\n",
       " 0.049346923828125,\n",
       " -0.03387451171875,\n",
       " -0.005527496337890625,\n",
       " 0.002941131591796875,\n",
       " 0.0003936290740966797,\n",
       " -0.04412841796875,\n",
       " 0.016510009765625,\n",
       " 0.029937744140625,\n",
       " 0.054656982421875,\n",
       " 0.006866455078125,\n",
       " -0.03826904296875,\n",
       " 0.022857666015625,\n",
       " -0.01904296875,\n",
       " -0.0139007568359375,\n",
       " 0.002452850341796875,\n",
       " 0.0303192138671875,\n",
       " 0.0179290771484375,\n",
       " 0.0012493133544921875,\n",
       " 0.0246734619140625,\n",
       " 0.0025310516357421875,\n",
       " -0.0171661376953125,\n",
       " 0.01593017578125,\n",
       " 0.0023708343505859375,\n",
       " 0.0623779296875,\n",
       " 0.0218963623046875,\n",
       " 0.0141754150390625,\n",
       " 0.0033512115478515625,\n",
       " -0.033172607421875,\n",
       " 0.031646728515625,\n",
       " -0.018218994140625,\n",
       " 0.03466796875,\n",
       " 0.049713134765625,\n",
       " -0.0285491943359375,\n",
       " -0.01507568359375,\n",
       " 0.0009236335754394531,\n",
       " -0.019012451171875,\n",
       " 0.032318115234375,\n",
       " 0.03216552734375,\n",
       " 0.025909423828125,\n",
       " -0.0172882080078125,\n",
       " -0.005558013916015625,\n",
       " -0.0064544677734375,\n",
       " ...]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = NVIDIAEmbeddings(model=\"ai-embed-qa-4\")\n",
    "embedder.embed_query(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "import faiss\n",
    "# create my own uuid\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, separator=\" \")\n",
    "docs = []\n",
    "metadatas = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='ai-arctic', model_type='chat', api_type=None, model_name='snowflake/arctic', client='ChatNVIDIA', path='7408b6b5-09e7-4ae5-a3fe-2db063e4e609'),\n",
       " Model(id='ai-codegemma-7b', model_type='chat', api_type=None, model_name='google/codegemma-7b', client='ChatNVIDIA', path='7dfc10a8-3cc4-448e-97c1-2213308dc222'),\n",
       " Model(id='ai-codellama-70b', model_type='chat', api_type=None, model_name='meta/codellama-70b', client='ChatNVIDIA', path='f6b06895-d073-4714-8bb2-26c09e9f6597'),\n",
       " Model(id='ai-fuyu-8b', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='e598bfc1-b058-41af-869d-556d3c7e1b48'),\n",
       " Model(id='ai-gemma-2b', model_type='chat', api_type=None, model_name='google/gemma-2b', client='ChatNVIDIA', path='04174188-f742-4069-9e72-d77c2b77d3cb'),\n",
       " Model(id='ai-gemma-7b', model_type='chat', api_type=None, model_name='google/gemma-7b', client='ChatNVIDIA', path='a13e3bed-ca42-48f8-b3f1-fbc47b9675f9'),\n",
       " Model(id='ai-google-deplot', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='784a8ca4-ea7d-4c93-bb46-ec027c3fae47'),\n",
       " Model(id='ai-llama2-70b', model_type='chat', api_type=None, model_name='meta/llama2-70b', client='ChatNVIDIA', path='2fddadfb-7e76-4c8a-9b82-f7d3fab94471'),\n",
       " Model(id='ai-llama3-70b', model_type='chat', api_type=None, model_name='meta/llama3-70b-instruct', client='ChatNVIDIA', path='a88f115a-4a47-4381-ad62-ca25dc33dc1b'),\n",
       " Model(id='ai-llama3-8b', model_type='chat', api_type=None, model_name='meta/llama3-8b-instruct', client='ChatNVIDIA', path='a5a3ad64-ec2c-4bfc-8ef7-5636f26630fe'),\n",
       " Model(id='ai-microsoft-kosmos-2', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='6018fed7-f227-48dc-99bc-3fd4264d5037'),\n",
       " Model(id='ai-mistral-7b-instruct-v2', model_type='chat', api_type=None, model_name='mistralai/mistral-7b-instruct-v0.2', client='ChatNVIDIA', path='d7618e99-db93-4465-af4d-330213a7f51f'),\n",
       " Model(id='ai-mistral-large', model_type='chat', api_type=None, model_name='mistralai/mistral-large', client='ChatNVIDIA', path='767b5b9a-3f9d-4c1d-86e8-fa861988cee7'),\n",
       " Model(id='ai-mixtral-8x22b-instruct', model_type='chat', api_type=None, model_name='mistralai/mixtral-8x22b-instruct-v0.1', client='ChatNVIDIA', path='710c92d0-7c98-46d6-b5ae-07e84bcaa5d3'),\n",
       " Model(id='ai-mixtral-8x7b-instruct', model_type='chat', api_type=None, model_name='mistralai/mixtral-8x7b-instruct-v0.1', client='ChatNVIDIA', path='a1e53ece-bff4-44d1-8b13-c009e5bf47f6'),\n",
       " Model(id='ai-neva-22b', model_type='image_in', api_type=None, model_name=None, client='ChatNVIDIA', path='bc205f8e-1740-40df-8d32-c4321763498a'),\n",
       " Model(id='ai-phi-3-mini', model_type='chat', api_type=None, model_name='microsoft/phi-3-mini-128k-instruct', client='ChatNVIDIA', path='4a58c6cb-a9b4-4014-99de-3e704d4ae687'),\n",
       " Model(id='ai-recurrentgemma-2b', model_type='chat', api_type=None, model_name='google/recurrentgemma-2b', client='ChatNVIDIA', path='2f495340-a99f-4b4b-89bd-1beb003dd896'),\n",
       " Model(id='ai-a2x-service', model_type=None, api_type=None, model_name=None, client=None, path='462f7853-60e8-474a-9728-7b598e58472c'),\n",
       " Model(id='ai-ai-weather-forecasting', model_type=None, api_type=None, model_name=None, client=None, path='9cec444c-db1c-4525-9c6f-f40e4a5b11ce'),\n",
       " Model(id='ai-arctic-embed-l', model_type=None, api_type=None, model_name=None, client=None, path='1528a0ad-205a-46ac-a783-94e2372586a9'),\n",
       " Model(id='ai-bge-m3', model_type=None, api_type=None, model_name=None, client=None, path='6acccd83-8f46-42b2-a191-529be72bb8de'),\n",
       " Model(id='ai-breeze-7b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='14919f3d-8dc9-4d19-a6da-6dbf99360156'),\n",
       " Model(id='ai-chatqa-1-5-70b', model_type=None, api_type=None, model_name=None, client=None, path='46594287-38b9-481c-a37f-baa02f2d3ba1'),\n",
       " Model(id='ai-chatqa-1-5-8b', model_type=None, api_type=None, model_name=None, client=None, path='3c7d7cee-90ba-433c-97fc-a092864c7ccf'),\n",
       " Model(id='ai-codegemma-1-1-7b', model_type=None, api_type=None, model_name=None, client=None, path='e2d298c5-204e-4213-b921-9f492cc9011b'),\n",
       " Model(id='ai-codestral-22b-instruct-v01', model_type=None, api_type=None, model_name=None, client=None, path='9a10b012-e6df-46fd-83b2-700dcbc75814'),\n",
       " Model(id='ai-dbrx-instruct', model_type=None, api_type=None, model_name=None, client=None, path='3d6c2ff8-8bfc-4d10-8fd0-b7337288e869'),\n",
       " Model(id='ai-deepseek-coder-6_7b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='e503b15c-62b0-4d69-b532-a88f0bfa2656'),\n",
       " Model(id='ai-diffdock', model_type=None, api_type=None, model_name=None, client=None, path='f3dda972-561a-4772-8c09-873594b6fb72'),\n",
       " Model(id='ai-esmfold', model_type=None, api_type=None, model_name=None, client=None, path='a68c59e0-47a6-4a50-bf64-6d88766d56bf'),\n",
       " Model(id='ai-example', model_type=None, api_type=None, model_name=None, client=None, path='80a5d6c6-7658-49c5-b2b0-105bfb210282'),\n",
       " Model(id='ai-gemma-2-27b-it', model_type=None, api_type=None, model_name=None, client=None, path='6369bc8b-727e-4591-b274-908ac3c80c59'),\n",
       " Model(id='ai-gemma-2-9b-it', model_type=None, api_type=None, model_name=None, client=None, path='1136f83f-7706-49f3-970b-67499c26724e'),\n",
       " Model(id='ai-google-paligemma', model_type=None, api_type=None, model_name=None, client=None, path='a70e7356-c643-41b3-9a7e-b89ea1e7dea1'),\n",
       " Model(id='ai-granite-34b-code-instruct', model_type=None, api_type=None, model_name=None, client=None, path='4df48b4f-e3c5-4ade-82c7-c06b65e25d18'),\n",
       " Model(id='ai-granite-8b-code-instruct', model_type=None, api_type=None, model_name=None, client=None, path='af7b6f03-f615-4c5f-86c6-388bd35cede0'),\n",
       " Model(id='ai-llava16-34b', model_type=None, api_type=None, model_name=None, client=None, path='72b74644-576f-4df1-8b90-70c2fc2ec5f8'),\n",
       " Model(id='ai-llava16-mistral-7b', model_type=None, api_type=None, model_name=None, client=None, path='5fac3d03-018f-4f41-ba47-72f4f04f6f63'),\n",
       " Model(id='ai-mistral-7b-instruct-v03', model_type=None, api_type=None, model_name=None, client=None, path='cd89bd68-13e3-47a9-861e-9a62e6e14b05'),\n",
       " Model(id='ai-molmim-generate', model_type=None, api_type=None, model_name=None, client=None, path='72be0b68-179f-412c-ac03-9a481f78cb9f'),\n",
       " Model(id='ai-nemotron-4-340b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='b0fcd392-e905-4ab4-8eb9-aeae95c30b37'),\n",
       " Model(id='ai-nemotron-4-340b-reward', model_type=None, api_type=None, model_name=None, client=None, path='c53ee0e9-bad9-4e09-b365-52c9d6b71254'),\n",
       " Model(id='ai-nv-embed-v1', model_type=None, api_type=None, model_name=None, client=None, path='4c134d37-17f9-4fc6-9f84-1f3a8a03c52c'),\n",
       " Model(id='ai-nvidia-cuopt', model_type=None, api_type=None, model_name=None, client=None, path='b0ac1378-3d00-43cb-a8d9-0f0c37ef36c0'),\n",
       " Model(id='ai-ocdrnet', model_type=None, api_type=None, model_name=None, client=None, path='5ab7a112-a78f-4218-9612-e02a0fffcc03'),\n",
       " Model(id='ai-parabricks-deepvariant', model_type=None, api_type=None, model_name=None, client=None, path='8a9b3cdd-4eab-4c0f-aaaf-fcfad7d060c1'),\n",
       " Model(id='ai-parabricks-fq2bam', model_type=None, api_type=None, model_name=None, client=None, path='bb6bb23e-9861-4aa5-9ef9-1ec69bbc8e9c'),\n",
       " Model(id='ai-parakeet-ctc-riva', model_type=None, api_type=None, model_name=None, client=None, path='22164014-a6cc-4a6f-b048-f3a303e745bb'),\n",
       " Model(id='ai-phi-3-medium-128k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='2975892e-2acd-49a5-8c4b-b76baf352bed'),\n",
       " Model(id='ai-phi-3-medium-4k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='971a7ad5-009e-47a1-941a-22b74d742304'),\n",
       " Model(id='ai-phi-3-mini-4k', model_type=None, api_type=None, model_name=None, client=None, path='ad974453-80d4-46df-a02d-6f7dae20c010'),\n",
       " Model(id='ai-phi-3-small-128k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='2ae1bc0b-a204-42fc-9edc-bd5f7fb2340d'),\n",
       " Model(id='ai-phi-3-small-8k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='5affddf9-9f6a-44f5-baa7-8bc7ab5dc398'),\n",
       " Model(id='ai-phi-3-vision-128k-instruct', model_type=None, api_type=None, model_name=None, client=None, path='20f2537e-8593-4eb9-ad40-60eee3bbaa55'),\n",
       " Model(id='ai-retail-object-detection', model_type=None, api_type=None, model_name=None, client=None, path='c0b05e1d-3847-467a-80d5-85e3e5b05f17'),\n",
       " Model(id='ai-rfdiffusion', model_type=None, api_type=None, model_name=None, client=None, path='e469fc55-91c0-460d-a444-15d9921ac277'),\n",
       " Model(id='ai-sdxl-lightning', model_type=None, api_type=None, model_name=None, client=None, path='994958c4-09f3-4232-8777-e4105f90a38b'),\n",
       " Model(id='ai-sdxl-turbo', model_type=None, api_type=None, model_name=None, client=None, path='f886140c-424e-4c82-a841-99e23f9ae35d'),\n",
       " Model(id='ai-sea-lion-7b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='02f84bf4-c1a1-489b-a9de-ac3e8dcdec14'),\n",
       " Model(id='ai-seallm-7b', model_type=None, api_type=None, model_name=None, client=None, path='e71e675e-a53d-4f2b-a441-e1287bd17dc8'),\n",
       " Model(id='ai-solar-10_7b-instruct', model_type=None, api_type=None, model_name=None, client=None, path='fb60b849-1282-42a4-b790-977d88db3550'),\n",
       " Model(id='ai-stable-diffusion-3-medium', model_type=None, api_type=None, model_name=None, client=None, path='1a89a4d9-bbf6-432c-95ca-4ab78fb02f28'),\n",
       " Model(id='ai-stable-diffusion-xl', model_type=None, api_type=None, model_name=None, client=None, path='c1b63bb0-448b-4e53-b2a7-fb0b3723cbe2'),\n",
       " Model(id='ai-stable-video-diffusion', model_type=None, api_type=None, model_name=None, client=None, path='8cd594f1-6a4d-4f8f-82b4-d1bf89adae98'),\n",
       " Model(id='ai-starcoder2-15b', model_type=None, api_type=None, model_name=None, client=None, path='d9cfe8a2-44df-44a0-ba51-3fc4a202c11c'),\n",
       " Model(id='ai-starcoder2-7b', model_type=None, api_type=None, model_name=None, client=None, path='dd7b01e7-732d-4da5-8e8d-315f79165a23'),\n",
       " Model(id='ai-vista-3d', model_type=None, api_type=None, model_name=None, client=None, path='72311276-923f-4478-a506-d5b80914728a'),\n",
       " Model(id='ai-visual-changenet', model_type=None, api_type=None, model_name=None, client=None, path='60eb6443-f7fa-464b-8a38-60b41db269f4'),\n",
       " Model(id='ai-yi-large', model_type=None, api_type=None, model_name=None, client=None, path='23bd454d-b225-49a3-8118-582a62fc51b8')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb   \n",
    "import numpy as np\n",
    "from langchain import hub\n",
    "from langchain_community.llms import ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import re\n",
    "from typing import List\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from numpy import ndarray\n",
    "from typing import List, Dict, Tuple, Type, Union\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_documents(document_dir):\n",
    "    \"\"\"Loads PDF documents from the specified directory, handling errors and splitting PDFs.\"\"\"\n",
    "\n",
    "    loader_cls = PyPDFLoader  # Only use PyPDFLoader for this function\n",
    "    files = os.listdir(document_dir)\n",
    "    for index, filename in enumerate(tqdm(files, desc=\"Processing files\", total=len(files))):\n",
    "        if os.path.splitext(filename)[1].lower() == \".pdf\":  # Check for lowercase \".pdf\" extension\n",
    "            try:\n",
    "                loader = loader_cls(os.path.join(document_dir, filename))             \n",
    "                doc = loader.load()\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=400,chunk_overlap=30)\n",
    "                chunks = text_splitter.split_documents(doc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")  # Log any errors\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [01:37<00:00, 19.41s/it]\n"
     ]
    }
   ],
   "source": [
    "docs = load_documents(r\"D:\\RAG NVIDIA\\RAG BOOK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 1}, page_content='The Kaggle Book\\nData analysis and machine learning for competitive data science\\nKonrad Banachewicz\\nLuca Massaron\\nBIRMINGHAM—MUMBAI\\nPackt and this book are not officially connected with Kaggle. This book is an effort from the Kaggle \\ncommunity of experts to help more developers.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 2}, page_content='The Kaggle Book\\nCopyright © 2022 Packt Publishing\\nAll rights reserved . No part of this book may be reproduced, stored in a retrieval system, or transmitted in \\nany form or by any means, without the prior written permission of the publisher, except in the case of brief \\nquotations embedded in critical articles or reviews.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 2}, page_content='Every effort has been made in the preparation of this book to ensure the accuracy of the information \\npresented. However, the information contained in this book is sold without warranty, either express or \\nimplied. Neither the authors, nor Packt Publishing or its dealers and distributors, will be held liable for any'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 2}, page_content='damages caused or alleged to have been caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the companies and products \\nmentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee \\nthe accuracy of this information.\\nProducer:  Tushar Gupta\\nAcquisition Editor – Peer Reviews:  Saby Dsilva'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 2}, page_content='Project Editor:  Parvathy Nair\\nContent Development Editor:  Lucy Wan\\nCopy Editor:  Safi s Editing\\nTechnical Editor:  Karan Sonawane\\nProofreader:  Safi s Editing\\nIndexer:  Sejal Dsilva\\nPresentation Designer:  Pranit Padwal\\nFirst published: April 2022\\nProduction reference: 3141022Published by Packt Publishing Ltd.\\nLivery Place\\n35 Livery Street\\nBirminghamB3 2PB, UK.\\nISBN 978-1-80181-747-9'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 2}, page_content='ISBN 978-1-80181-747-9\\nwww.packt.com'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='Foreword\\nI had a background in econometrics but became interested in machine learning techniques, ini -\\ntially as an alternative approach to solving forecasting problems. As I started discovering my \\ninterest, I found the field intimidating to enter: I didn’t know the techniques, the terminology, \\nand didn’t have the credentials that would allow me to break in.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='It was always my dream that Kaggle would allow people like me the opportunity to break into \\nthis powerful new field. Perhaps the thing I’m proudest of is the extent to which Kaggle has made \\ndata science and machine learning more accessible. We’ve had many Kagglers go from newbies \\nto top machine learners, being hired at places like NVIDIA, Google, and OpenAI, and starting'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='companies like DataRobot.\\nLuca and Konrad’s book helps make Kaggle even more accessible. It offers a guide to both how \\nKaggle works, as well as many of the key learnings that they have taken out of their time on the \\nsite. Collectively, they’ve been members of Kaggle for over 20 years, entered 330 competitions,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='made over 2,000 posts to Kaggle forums, and shared over 100 notebooks and 50 datasets. They \\nare both top-ranked users and well-respected members of the Kaggle community.\\nThose who complete this book should expect to be able to engage confidently on Kaggle – and \\nengaging confidently on Kaggle has many rewards.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='Firstly, it’s a powerful way to stay on top of the most pragmatic developments in machine learn -\\ning. Machine learning is moving very quickly. In 2019, over 300 peer reviewed machine learning \\npapers were published per day. This volume of publishing makes it impossible to be on top of \\nthe literature. Kaggle ends up being a very valuable way to filter what developments matter on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 3}, page_content='real-world problems – and Kaggle is useful for more than keeping up with the academic litera-\\nture. Many of the tools that have become standard in the industry have spread via Kaggle. For \\nexample, XGBoost in 2014 and Keras in 2015 both spread through the community before making \\ntheir way into industry.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 4}, page_content='Secondly, Kaggle offers users a way to “learn by doing.” I’ve heard active Kagglers talk about com -\\npeting regularly as “weight training” for machine learning. The variety of use cases and problems \\nthey tackle on Kaggle makes them well prepared when they encounter similar problems in indus -\\ntry. And because of competition deadlines, Kaggle trains the muscle of iterating quickly. There’s'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 4}, page_content='probably no better way to learn than to attempt a problem and then see how top performers \\ntackled the same problem (it’s typical for winners to share their approaches after the competition).\\nSo, for those of you who are reading this book and are new to Kaggle, I hope it helps make Kaggle \\nless intimidating. And for those who have been on Kaggle for a while and are looking to level up,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 4}, page_content='I hope this book from two of Kaggle’s strongest and most respected members helps you get more \\nout of your time on the site.\\nAnthony Goldbloom\\nKaggle Founder and CEO'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 5}, page_content='Contributors\\nAbout the authors\\nKonrad Banachewicz  holds a PhD in statistics from Vrije Universiteit Amsterdam. During his \\nperiod in academia, he focused on problems of extreme dependency modeling in credit risk. In \\naddition to his research activities, Konrad was a tutor and supervised master’s students. Starting'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 5}, page_content='from classical statistics, he slowly moved toward data mining and machine learning (this was \\nbefore the terms “data science” or “big data” became ubiquitous). \\nIn the decade after his PhD, Konrad worked in a variety of financial institutions on a wide array \\nof quantitative data analysis problems. In the process, he became an expert on the entire lifetime'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 5}, page_content='of a data product cycle. He has visited different ends of the frequency spectrum in finance (from \\nhigh-frequency trading to credit risk, and everything in between), predicted potato prices, and \\nanalyzed anomalies in the performance of large-scale industrial equipment. \\nAs a person who himself stood on the shoulders of giants, Konrad believes in sharing knowledge'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 5}, page_content='with others. In his spare time, he competes on Kaggle (“the home of data science”).\\nI would like to thank my brother for being a fixed point in a chaotic world and continuing to provide \\ninspiration and motivation. Dzięki, Braciszku.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 6}, page_content='Luca Massaron  is a data scientist with more than a decade of experience in transforming data \\ninto smarter artifacts, solving real-world problems, and generating value for businesses and \\nstakeholders. He is the author of bestselling books on AI, machine learning, and algorithms. Luca \\nis also a Kaggle Grandmaster who reached no. 7 in the worldwide user rankings for his perfor-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 6}, page_content='mance in data science competitions, and a Google Developer Expert  (GDE) in machine learning.\\nMy warmest thanks go to my family, Yukiko and Amelia, for their support and loving patience as I prepared \\nthis new book in a long series.\\nMy deepest thanks to Anthony Goldbloom for kindly writing the foreword for this book and to all the Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 6}, page_content='Masters and Grandmasters who have so enthusiastically contributed to its making with their interviews, \\nsuggestions, and help.\\nFinally, I would like to thank Tushar Gupta, Parvathy Nair, Lucy Wan, Karan Sonawane, and all of the \\nPackt Publishing editorial and production staff for their support on this writing effort.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='About the reviewer\\nDr. Andrey Kostenko  is a data science and machine learning professional with extensive expe -\\nrience across a variety of disciplines and industries, including hands-on coding in R and Python \\nto build, train, and serve time series models for forecasting and other applications. He believes'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='that lifelong learning and open-source software are both critical for innovation in advanced \\nanalytics and artificial intelligence.\\nAndrey recently assumed the role of Lead Data Scientist at Hydroinformatics Institute  (H2i.sg), a \\nspecialized consultancy and solution services provider for all aspects of water management. Prior'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='to joining H2i, Andrey had worked as Senior Data Scientist at IAG InsurTech Innovation Hub for \\nover 3 years. Before moving to Singapore in 2018, he worked as Data Scientist at TrafficGuard.\\nai, an Australian AdTech start-up developing novel data-driven algorithms for mobile ad fraud \\ndetection. In 2013, Andrey received his doctorate degree in Mathematics and Statistics from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='Monash University, Australia. By then, he already had an MBA degree from the UK and his first \\nuniversity degree from Russia.\\nIn his spare time, Andrey is often found engaged in competitive data science projects, learning new \\ntools across R and Python ecosystems, exploring the latest trends in web development, solving \\nchess puzzles, or reading about the history of science and mathematics.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='Dr. Firat Gonen  is the Head of Data Science and Analytics at Getir. Gonen leads the data sci -\\nence and data analysis teams delivering innovative and cutting edge Machine Learning projects. \\nBefore Getir, Dr. Gonen was managing Vodafone Turkey’s AI teams. Prior to Vodafone Turkey, he \\nwas the Principal Data Scientist at Dogus Group (one of Turkey’s largest conglomerates). Gonen'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='holds extensive educational qualifications including a PhD degree in NeuroScience and Neural \\nNetworks from University of Houston and is an expert in Machine Learning, Deep Learning, Visual \\nAttention, Decision-Making & Genetic Algorithms with over more than 12 years in the field. He \\nhas authored several peer-review journal papers. He’s also a Kaggle Triple GrandMaster and has'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 7}, page_content='more than 10 international data competition medals. He was also selected as the 2020 Z by HP \\nData Science Global Ambassador.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 8}, page_content='About the interviewees\\nWe were fortunate enough to be able to collect interviews from 31 talented Kagglers across the \\nKaggle community, who we asked to reflect on their time on the platform. You will find their \\nanswers scattered across the book. They represent a broad range of perspectives, with many in-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 8}, page_content='sightful responses that are as similar as they are different. We read each one of their contributions \\nwith great interest and hope the same is true for you, the reader. We give thanks to all of them \\nand list them in alphabetical order below.\\nAbhishek Thakur , who is currently building AutoNLP at Hugging Face.\\nAlberto Danese , Head of Data Science at Nexi.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 8}, page_content='Andrada Olteanu, Data Scientist at Endava, Dev Expert at Weights and Biases, and Z by HP Global \\nData Science Ambassador.\\nAndrew Maranhão , Senior Data Scientist at Hospital Albert Einstein in São Paulo.\\nAndrey Lukyanenko, Machine Learning Engineer and TechLead at MTS Group.\\nBojan Tunguz , Machine Learning Modeler at NVIDIA.\\nChris Deotte, Senior Data Scientist and Researcher at NVIDIA.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 8}, page_content='Dan Becker, VP Product, Decision Intelligence at DataRobot.\\nDmitry Larko, Chief Data Scientist at H2O.ai.\\nFirat Gonen , Head of Data Science and Analytics at Getir and Z by HP Global Data Science Am-\\nbassador.\\nGabriel Preda, Principal Data Scientist at Endava.\\nGilberto Titericz, Senior Data Scientist at NVIDIA.\\nGiuliano Janson , Senior Applied Scientist for ML and NLP at Zillow Group.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 8}, page_content='Jean-François Puget, Distinguished Engineer, RAPIDS at NVIDIA, and the manager of the NVIDIA \\nKaggle Grandmaster team.\\nJeong-Yoon Lee, Senior Research Scientist in the Rankers and Search Algorithm Engineering \\nteam at Netflix Research.\\nKazuki Onodera , Senior Deep Learning Data Scientist at NVIDIA and member of the NVIDIA \\nKGMON team.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 9}, page_content='Laura Fink , Head of Data Science at Micromata.\\nMartin Henze, PhD Astrophysicist and Data Scientist at Edison Software.\\nMikel Bober-Irizar, Machine Learning Scientist at ForecomAI and Computer Science student at \\nthe University of Cambridge.\\nOsamu Akiyama , Medical Doctor at Osaka University.\\nParul Pandey, Data Scientist at H2O.ai.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 9}, page_content='Paweł Jankiewicz, Chief Data Scientist & AI Engineer as well as Co-founder of LogicAI.\\nRob Mulla , Senior Data Scientist at Biocore LLC.\\nRohan Rao , Senior Data Scientist at H2O.ai.\\nRuchi Bhatia , Data Scientist at OpenMined, Z by HP Global Data Science Ambassador, and grad-\\nuate student at Carnegie Mellon University.\\nRyan Chesler , Data Scientist at H2O.ai.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 9}, page_content='Shotaro Ishihara, Data Scientist and Researcher at a Japanese news media company.\\nSudalai Rajkumar , an AI/ML advisor for start-up companies.\\nXavier Conort , Founder and CEO at Data Mapping and Engineering.\\nYifan Xie, Co-founder of Arion Ltd, a data science consultancy firm.\\nYirun Zhang , final-year PhD student at King’s College London in applied machine learning.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 10}, page_content='Join our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors: \\nhttps://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 11}, page_content='Table of Contents\\nPreface   xix\\nPart I: Introduction to Competitions   1\\nChapter 1: Introducing Kaggle and Other Data Science Competitions   3\\nThe rise of data science competition platforms  ...................................................................  4\\nThe Kaggle competition platform • 7\\nA history of Kaggle • 7\\nOther competition platforms • 10'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 11}, page_content='Introducing Kaggle  ...........................................................................................................  12\\nStages of a competition • 12\\nTypes of competitions and examples • 17\\nSubmission and leaderboard dynamics • 22\\nExplaining the Common Task Framework paradigm • 23\\nUnderstanding what can go wrong in a competition • 24\\nComputational resources • 26\\nKaggle Notebooks • 27'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 11}, page_content='Kaggle Notebooks • 27\\nTeaming and networking • 28\\nPerformance tiers and rankings • 32\\nCriticism and opportunities • 33\\nSummary  ..........................................................................................................................  35'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Table of Contents xii\\nChapter 2: Organizing Data with Datasets   37\\nSetting up a dataset  ..........................................................................................................  37\\nGathering the data  ............................................................................................................  42'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Working with datasets  ......................................................................................................  48\\nUsing Kaggle Datasets in Google Colab  .............................................................................  49\\nLegal caveats  ......................................................................................................................  51'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Summary  ..........................................................................................................................  52\\nChapter 3: Working and Learning with Kaggle Notebooks   53\\nSetting up a Notebook  .......................................................................................................  54'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Running your Notebook  ....................................................................................................  58\\nSaving Notebooks to GitHub  .............................................................................................  60\\nGetting the most out of Notebooks  ...................................................................................  63'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Upgrading to Google Cloud Platform (GCP) • 64\\nOne step beyond • 66\\nKaggle Learn courses  ........................................................................................................  73\\nSummary  ..........................................................................................................................  77\\nChapter 4: Leveraging Discussion Forums   79'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='How forums work  .............................................................................................................  79\\nExample discussion approaches  ........................................................................................  86\\nNetiquette  .........................................................................................................................  92'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Summary  ..........................................................................................................................  93\\nPart II: Sharpening Your Skills for Competitions   95\\nChapter 5: Competition Tasks and Metrics   97\\nEvaluation metrics and objective functions  ......................................................................  98'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 12}, page_content='Basic types of tasks  .........................................................................................................  100\\nRegression • 100'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='Table of Contents xiii\\nClassification • 100\\nOrdinal • 101\\nThe Meta Kaggle dataset  .................................................................................................  102\\nHandling never-before-seen metrics ...............................................................................  105'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='Metrics for regression (standard and ordinal)  .................................................................  109\\nMean squared error (MSE) and R squared • 109\\nRoot mean squared error (RMSE) • 111\\nRoot mean squared log error (RMSLE) • 112\\nMean absolute error (MAE) • 113\\nMetrics for classification (label prediction and probability)  ............................................  114'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='Accuracy • 114\\nPrecision and recall • 116\\nThe F1 score • 119\\nLog loss and ROC-AUC • 119\\nMatthews correlation coefficient (MCC) • 121\\nMetrics for multi-class classification  ...............................................................................  122\\nMetrics for object detection problems  .............................................................................  129'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='Intersection over union (IoU) • 131\\nDice • 132\\nMetrics for multi-label classification and recommendation problems  .............................  133\\nMAP@{K} • 134\\nOptimizing evaluation metrics  ........................................................................................  135\\nCustom metrics and custom objective functions • 136\\nPost-processing your predictions • 139'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='Predicted probability and its adjustment • 141\\nSummary  ........................................................................................................................  146\\nChapter 6: Designing Good Validation   149\\nSnooping on the leaderboard  ..........................................................................................  150'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 13}, page_content='The importance of validation in competitions  .................................................................  153\\nBias and variance • 156'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Table of Contents xiv\\nTrying different splitting strategies  .................................................................................  159\\nThe basic train-test split • 160\\nProbabilistic evaluation methods • 161\\nk-fold cross-validation • 161\\nSubsampling • 171\\nThe bootstrap • 171'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='The bootstrap • 171\\nTuning your model validation system  ..............................................................................  176\\nUsing adversarial validation .............................................................................................  179\\nExample implementation • 181\\nHandling different distributions of training and test data • 183'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Handling leakage  .............................................................................................................  187\\nSummary  .........................................................................................................................  192\\nChapter 7: Modeling for Tabular Competitions   195'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='The Tabular Playground Series  .......................................................................................  196\\nSetting a random state for reproducibility  ......................................................................  202\\nThe importance of EDA  ...................................................................................................  203'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Dimensionality reduction with t-SNE and UMAP • 205\\nReducing the size of your data  .......................................................................................  208\\nApplying feature engineering  ..........................................................................................  210\\nEasily derived features • 211\\nMeta-features based on rows and columns • 213\\nTarget encoding • 215'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Target encoding • 215\\nUsing feature importance to evaluate your work • 220\\nPseudo-labeling  ..............................................................................................................  224\\nDenoising with autoencoders  .........................................................................................  226'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Neural networks for tabular competitions  .......................................................................  231\\nSummary  ........................................................................................................................  238\\nChapter 8: Hyperparameter Optimization   241'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 14}, page_content='Basic optimization techniques  ........................................................................................  242\\nGrid search • 243'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 15}, page_content='Table of Contents xv\\nRandom search • 245\\nHalving search • 246\\nKey parameters and how to use them  .............................................................................  249\\nLinear models • 250\\nSupport-vector machines • 250\\nRandom forests and extremely randomized trees • 251\\nGradient tree boosting • 253\\nLightGBM • 253\\nXGBoost • 255\\nCatBoost • 257\\nHistGradientBoosting • 258'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 15}, page_content='HistGradientBoosting • 258\\nBayesian optimization  .....................................................................................................  261\\nUsing Scikit-optimize • 262\\nCustomizing a Bayesian optimization search • 268\\nExtending Bayesian optimization to neural architecture search • 276\\nCreating lighter and faster models with KerasTuner • 285\\nThe TPE approach in Optuna • 295'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 15}, page_content='Summary  ........................................................................................................................  301\\nChapter 9: Ensembling with Blending and Stacking Solutions   303\\nA brief introduction to ensemble algorithms  ..................................................................  304'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 15}, page_content='Averaging models into an ensemble  ................................................................................  307\\nMajority voting • 309\\nAveraging of model predictions • 312\\nWeighted averages • 314\\nAveraging in your cross-validation strategy • 315\\nCorrecting averaging for ROC-AUC evaluations • 316'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 15}, page_content='Blending models using a meta-model  ..............................................................................  317\\nBest practices for blending • 318\\nStacking models together  ...............................................................................................  323\\nStacking variations • 327'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Table of Contents xvi\\nCreating complex stacking and blending solutions  .........................................................  329\\nSummary  ........................................................................................................................  333\\nChapter 10: Modeling for Computer Vision   335'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Augmentation strategies  .................................................................................................  335\\nKeras built-in augmentations • 341\\nImageDataGenerator approach • 341\\nPreprocessing layers • 345\\nalbumentations • 346\\nClassification  ..................................................................................................................  349'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Object detection  ...............................................................................................................  357\\nSemantic segmentation  ...................................................................................................  371\\nSummary  ........................................................................................................................  388'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Chapter 11: Modeling for NLP   389\\nSentiment analysis  ..........................................................................................................  389\\nOpen domain Q&A  ..........................................................................................................  398'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Text augmentation strategies  .........................................................................................  414\\nBasic techniques • 415\\nnlpaug • 420\\nSummary  ........................................................................................................................  423\\nChapter 12: Simulation and Optimization Competitions   425'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='Connect X ........................................................................................................................  426\\nRock-paper-scissors  .........................................................................................................  431\\nSanta competition 2020  .................................................................................................  435'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 16}, page_content='The name of the game  .....................................................................................................  439\\nSummary  ........................................................................................................................  444'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 17}, page_content='Table of Contents xvii\\nPart III: Leveraging Competitions for Your Career   445\\nChapter 13: Creating Your Portfolio of Projects and Ideas   447\\nBuilding your portfolio with Kaggle ................................................................................  447\\nLeveraging Notebooks and discussions • 452\\nLeveraging Datasets • 455'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 17}, page_content='Leveraging Datasets • 455\\nArranging your online presence beyond Kaggle  ..............................................................  460\\nBlogs and publications • 460\\nGitHub • 463\\nMonitoring competition updates and newsletters  ..........................................................  465'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 17}, page_content='Summary  ........................................................................................................................  467\\nChapter 14: Finding New Professional Opportunities   469\\nBuilding connections with other competition data scientists  .........................................  470'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 17}, page_content='Participating in Kaggle Days and other Kaggle meetups  .................................................  481\\nGetting spotted and other job opportunities  ...................................................................  482\\nThe STAR approach • 483\\nSummary (and some parting words)  ..............................................................................  485'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 17}, page_content='Other Books You May Enjoy   489\\nIndex   495'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='Preface\\nHaving competed on Kaggle for over ten years, both of us have experienced highs and lows over \\nmany competitions. We often found ourselves refocusing our efforts on different activities relating \\nto Kaggle. Over time, we devoted ourselves not just to competitions but also to creating content \\nand code based on the demands of the data science market and our own professional aspirations.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='At this point in our journey, we felt that our combined experience and still-burning passion for \\ncompetitions could really help other participants who have just started, or who would like to get \\ninspired, to get hold of the essential expertise they need, so they can start their own journey in \\ndata science competitions.\\nWe then decided to work on this book with a purpose:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='• To offer, in a single place, the best tips for being competitive and approaching most of \\nthe problems you may find when participating on Kaggle and also other data science \\ncompetitions.\\n• To offer enough suggestions to allow anyone to reach at least the Expert level in any Kaggle \\ndiscipline: Competitions, Datasets, Notebooks, or Discussions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='• To provide tips on how to learn the most from Kaggle and leverage this experience for \\nprofessional growth in data science.\\n• To gather in a single source the largest number of perspectives on the experience of partic -\\nipating in competitions, by interviewing Kaggle Masters and Grandmasters and listening \\nto their stories.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='to their stories.\\nIn short, we have written a book that demonstrates how to participate in competitions success-\\nfully and make the most of all the opportunities that Kaggle offers. The book is also intended as a \\npractical reference that saves you time and effort, through its selection of many competition tips'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 19}, page_content='and tricks that are hard to learn about and find on the internet or on Kaggle forums. Nevertheless, \\nthe book doesn’t limit itself to providing practical help; it also aspires to help you figure out how \\nto boost your career in data science by participating in competitions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='Preface xx\\nPlease be aware: this book doesn’t teach you data science from the basics. We don’t explain in \\ndetail how linear regression or random forests or gradient boosting work, but how to use them \\nin the best way and obtain the best results from them in a data problem. We expect solid foun-\\ndations and at least a basic proficiency in data science topics and Python usage from our readers.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='If you are still a data science beginner, you need to supplement this book with other books on \\ndata science, machine learning, and deep learning, and train up on online courses, such as those \\noffered by Kaggle itself or by MOOCs such as edX or Coursera.\\nIf you want to start learning data science in a practical way, if you want to challenge yourself'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='with tricky and intriguing data problems and simultaneously build a network of great fellow \\ndata scientists as passionate about their work in data as you are, this is indeed the book for you. \\nLet’s get started!\\nWho this book is for\\nAt the time of completion of this book, there are 96,190 Kaggle novices (users who have just reg -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='istered on the website) and 67,666 Kaggle contributors (users who have just filled in their profile) \\nenlisted in Kaggle competitions. This book has been written for all of them and for anyone else \\nwanting to break the ice and start taking part in competitions on Kaggle and learning from them.\\nWhat this book covers\\nPart 1: Introduction to Competitions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='Chapter 1, Introducing Kaggle and Other Data Science Competitions , discusses how competitive \\nprogramming evolved into data science competitions. It explains why the Kaggle platform is \\nthe most popular site for these competitions and provides you with an idea about how it works.\\nChapter 2, Organizing Data with Datasets, introduces you to Kaggle Datasets, the standard method'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='of data storage on the platform. We discuss setup, gathering data, and utilizing it in your work \\non Kaggle.\\nChapter 3, Working and Learning with Kaggle Notebooks , discusses Kaggle Notebooks, the baseline \\ncoding environment. We talk about the basics of Notebook usage, as well as how to leverage the \\nGCP environment, and using them to build up your data science portfolio.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 20}, page_content='Chapter 4, Leveraging Discussion Forums , allows you to familiarize yourself with discussion forums, \\nthe primary manner of communication and idea exchange on Kaggle.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='Preface xxi\\nPart 2: Sharpening Your Skills for Competitions\\nChapter 5, Competition Tasks and Metrics , details how evaluation metrics for certain kinds of prob -\\nlems strongly influence the way you can operate when building your model solution in a data \\nscience competition. The chapter also addresses the large variety of metrics available in Kaggle \\ncompetitions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='competitions.\\nChapter 6, Designing Good Validation , will introduce you to the importance of validation in data \\ncompetitions, discussing overfitting, shake-ups, leakage, adversarial validation, different kinds \\nof validation strategies, and strategies for your final submissions.\\nChapter 7, Modeling for Tabular Competitions , discusses tabular competitions, mostly focusing on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='the more recent reality of Kaggle, the Tabular Playground Series. Tabular problems are standard \\npractice for the majority of data scientists around and there is a lot to learn from Kaggle.\\nChapter 8, Hyperparameter Optimization , explores how to extend the cross-validation approach \\nto find the best hyperparameters for your models – in other words, those that can generalize in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='the best way on the private leaderboard – under the pressure and scarcity of time and resources \\nthat you experience in Kaggle competitions.\\nChapter 9, Ensembling with Blending and Stacking Solutions , explains ensembling techniques for \\nmultiple models such as averaging, blending, and stacking. We will provide you with some the -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='ory, some practice, and some code examples you can use as templates when building your own \\nsolutions on Kaggle.\\nChapter 10, Modeling for Computer Vision, we discuss problems related to computer vision, one of the \\nmost popular topics in AI in general, and on Kaggle specifically. We demonstrate full pipelines for'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='building solutions to challenges in image classification, object detection, and image segmentation.\\nChapter 11, Modeling for NLP, focuses on the frequently encountered types of Kaggle challenges \\nrelated to natural language processing. We demonstrate how to build an end-to-end solution for \\npopular problems like open domain question answering.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='Chapter 12, Simulation and Optimization Competitions , provides an overview of simulation compe -\\ntitions, a new class of contests gaining popularity on Kaggle over the last few years.\\nPart 3: Leveraging Competitions for Your Career\\nChapter 13, Creating Your Portfolio of Projects and Ideas , explores ways you can stand out by show -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 21}, page_content='casing your work on Kaggle itself and other sites in an appropriate way.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='Preface xxii\\nChapter 14 , Finding New Professional Opportunities , concludes the overview of how Kaggle can \\npositively affect your career by discussing the best ways to leverage all your Kaggle experience \\nin order to find new professional opportunities.\\nTo get the most out of this book\\nThe Python code in this book has been designed to be run on a Kaggle Notebook, without any'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='installation on a local computer. Therefore, don’t worry about what machine you have available \\nor what version of Python packages you should install. \\nAll you need is a computer with access to the internet and a free Kaggle account. In fact, to run \\nthe code on a Kaggle Notebook (you will find instructions about the procedure in Chapter 3), you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='first need to open an account on Kaggle. If you don’t have one yet, just go to www.kaggle.com  and \\nfollow the instructions on the website.\\nWe link out to many different resources throughout the book that we think you will find useful. \\nWhen referred to a link, explore it: you will find code available on public Kaggle Notebooks that you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='can reuse, or further materials to illustrate concepts and ideas that we have discussed in the book.\\nDownload the example code files\\nThe code bundle for the book is hosted on GitHub at https://github.com/PacktPublishing/\\nThe-Kaggle-Book . We also have other code bundles from our rich catalog of books and videos \\navailable at https://github.com/PacktPublishing/ . Check them out!'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='Download the color images\\nWe also provide a PDF file that has color images of the screenshots/diagrams used in this book. \\nYou can download it here: https://static.packt-cdn.com/downloads/9781801817479_\\nColorImages.pdf .\\nConventions used\\nThere are a few text conventions used throughout this book.\\nCodeInText : Indicates code words in text, database table names, folder names, filenames, file ex -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 22}, page_content='tensions, pathnames, dummy URLs, user input, and Twitter handles. For example; “ The dataset \\nwill be downloaded to the Kaggle folder as a .zip archive – unpack it and you are good to go.”'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 23}, page_content=\"Preface xxiii\\nA block of code is set as follows:\\nfrom google.colab import drive\\ndrive.mount( '/content/gdrive' )\\nAny command-line input or output is written as follows:\\nI genuinely have no idea what the output of this sequence of words will be \\n- it will be interesting to find out what nlpaug can do with this!\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 23}, page_content='Bold: Indicates a new term, an important word, or words that you see on the screen, for example, \\nin menus or dialog boxes. For example: “ The specific limits at the time of writing are 100 GB per \\nprivate dataset and a 100 GB total quota.”\\nGet in touch\\nFeedback from our readers is always welcome.\\nGeneral feedback : Email feedback@packtpub.com , and mention the book’s title in the subject of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 23}, page_content='your message. If you have questions about any aspect of this book, please email us at questions@\\npacktpub.com .\\nErrata: Although we have taken every care to ensure the accuracy of our content, mistakes do \\nhappen. If you have found a mistake in this book, we would be grateful if you would report this \\nto us. Please visit http://www.packtpub.com/submit-errata , selecting your book, clicking on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 23}, page_content='the Errata Submission Form link, and entering the details.\\nPiracy: If you come across any illegal copies of our works in any form on the Internet, we would \\nbe grateful if you would provide us with the location address or website name. Please contact us \\nat copyright@packtpub.com  with a link to the material.Further notes, references, and links to useful places appear like this.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 23}, page_content='Tips and tricks appear like this.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 24}, page_content='Preface xxiv\\nShare your thoughts\\nOnce you’ve read The Kaggle Book , we’d love to hear your thoughts! Please click here to go \\nstraight to the Amazon review page  for this book and share your feedback.\\nYour review is important to us and the tech community and will help us make sure we’re deliv -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 24}, page_content='ering excellent quality content.If you are interested in becoming an author: If there is a topic that you have expertise in and you \\nare interested in either writing or contributing to a book, please visit http://authors.packtpub.\\ncom.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 25}, page_content=\"Preface xxv\\nDownload a free PDF copy of this book\\nThanks for purchasing this book!\\nDo you like to read on the go but are unable to carry your print books everywhere?\\nIs your eBook purchase not compatible with the device of your choice?\\nDon't worry, now with every Packt book you get a DRM-free PDF version of that book at no cost.\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 25}, page_content=\"Read anywhere, any place, on any device. Searc h, copy, and paste code from your favorite technical \\nbooks directly into your application. ɰ\\nThe perks don't stop there, you can get exclusive access to discounts, newsletters, and great free \\ncontent in your inbox daily\\nFollow these simple steps to get the bene fi ts:\\n1. Scan the QR code or visit the link below\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 25}, page_content=\"https://packt.link/free-ebook/9781801817479\\n2. Submit your proof of purchase\\n3. That's it! We'll send your free PDF and other bene fi ts to your email directly\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 27}, page_content='Part I\\nIntroduction to \\nCompetitions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 29}, page_content='1\\nIntroducing Kaggle and Other \\nData Science Competitions\\nData science competitions have long been around and they have experienced growing success \\nover time, starting from a niche community of passionate competitors, drawing more and more \\nattention, and reaching a much larger audience of millions of data scientists. As longtime com-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 29}, page_content='petitors on the most popular data science competition platform, Kaggle, we have witnessed and \\ndirectly experienced all these changes through the years.\\nAt the moment, if you look for information about Kaggle and other competition platforms, you \\ncan easily find a large number of meetups, discussion panels, podcasts, interviews, and even'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 29}, page_content='online courses explaining how to win in such competitions (usually telling you to use a variable \\nmixture of grit, computational resources, and time invested). However, apart from the book that \\nyou are reading now, you won’t find any structured guides about how to navigate so many data \\nscience competitions and how to get the most out of them – not just in terms of score or ranking,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 29}, page_content='but also professional experience.\\nIn this book, instead of just packaging up a few hints about how to win or score highly on Kaggle \\nand other data science competitions, our intention is to present you with a guide on how to com-\\npete better on Kaggle and get back the maximum possible from your competition experiences,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 29}, page_content='particularly from the perspective of your professional life. Also accompanying the contents of the \\nbook are interviews with Kaggle Masters and Grandmasters. We hope they will offer you some \\ndifferent perspectives and insights on specific aspects of competing on Kaggle, and inspire the \\nway you will test yourself and learn doing competitive data science.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='Introducing Kaggle and Other Data Science Competitions 4\\nBy the end of this book, you’ll have absorbed the knowledge we drew directly from our own ex -\\nperiences, resources, and learnings from competitions, and everything you need to pave a way \\nfor yourself to learn and grow, competition after competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='As a starting point, in this chapter, we will explore how competitive programming evolved into \\ndata science competitions, why the Kaggle platform is the most popular site for such competi-\\ntions, and how it works.\\nWe will cover the following topics:\\n• The rise of data science competition platforms\\n• The Common Task Framework paradigm \\n• The Kaggle platform and some other alternatives'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='• How a Kaggle competition works: stages, competition types, submission and leaderboard \\ndynamics, computational resources, networking, and more\\nThe rise of data science competition platforms\\nCompetitive programming has a long history, starting in the 1970s with the first iterations of the \\nICPC, the International Collegiate Programming Contest . In the original ICPC, small teams'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='from universities and companies participated in a competition that required solving a series of \\nproblems using a computer program (at the beginning, participants coded in FORTRAN). In order \\nto achieve a good final rank, teams had to display good skills in team working, problem solving, \\nand programming.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='and programming.\\nThe experience of participating in the heat of such a competition and the opportunity to stand in \\na spotlight for recruiting companies provided the students with ample motivation and it made \\nthe competition popular for many years. Among ICPC finalists, a few have become renowned: \\nthere is Adam D’Angelo, the former CTO of Facebook and founder of Quora, Nikolai Durov, the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='co-founder of Telegram Messenger, and Matei Zaharia, the creator of Apache Spark. Together \\nwith many other professionals, they all share the same experience: having taken part in an ICPC.\\nAfter the ICPC, programming competitions flourished, especially after 2000 when remote par -\\nticipation became more feasible, allowing international competitions to run more easily and at a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 30}, page_content='lower cost. The format is similar for most of these competitions: there is a series of problems and \\nyou have to code a solution to solve them. The winners are given a prize, but also make themselves \\nknown to recruiting companies or simply become famous.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='Chapter 1 5\\nTypically, problems in competitive programming range from combinatorics and number theory \\nto graph theory, algorithmic game theory, computational geometry, string analysis, and data \\nstructures. Recently, problems relating to artificial intelligence have successfully emerged, in \\nparticular after the launch of the KDD Cup, a contest in knowledge discovery and data mining,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='held by the Association for Computing Machinery’s ( ACM’s) Special Interest Group ( SIG) \\nduring its annual conference ( https://kdd.org/conferences ).\\nThe first KDD Cup, held in 1997, involved a problem about direct marketing for lift curve optimi-\\nzation and it started a long series of competitions that continues today. You can find the archives'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='containing datasets, instructions, and winners at https://www.kdd.org/kdd-cup . Here is the lat -\\nest available at the time of writing: https://ogb.stanford.edu/kddcup2021/ . KDD Cups proved \\nquite effective in establishing best practices, with many published papers describing solutions, \\ntechniques, and competition dataset sharing, which have been useful for many practitioners for'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='experimentation, education, and benchmarking.\\nThe successful examples of both competitive programming events and the KDD Cup inspired \\ncompanies (such as Netflix) and entrepreneurs (such as Anthony Goldbloom, the founder of Kaggle) \\nto create the first data science competition platforms, where companies can host data science'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='challenges that are hard to solve and might benefit from crowdsourcing. In fact, given that there \\nis no golden approach that works for all the problems in data science, many problems require a \\ntime-consuming approach that can be summed up as try all that you can try.\\nIn fact, in the long run, no algorithm can beat all the others on all problems, as'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='stated by the No Free Lunch theorem by David Wolpert and William Macready. \\nThe theorem tells you that each machine learning algorithm performs if and only \\nif its hypothesis space comprises the solution. Consequently, as you cannot know \\nbeforehand if a machine learning algorithm can best tackle your problem, you have'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='to try it, testing it directly on your problem before being assured that you are doing \\nthe right thing. There are no theoretical shortcuts or other holy grails of machine \\nlearning – only empirical experimentation can tell you what works.\\nFor more details, you can look up the No Free Lunch theorem for a theoretical expla-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 31}, page_content='nation of this practical truth. Here is a complete article from Analytics India Magazine \\non the topic: https://analyticsindiamag.com/what-are-the-no-free-lunch-\\ntheorems-in-data-science/ .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='Introducing Kaggle and Other Data Science Competitions 6\\nCrowdsourcing proves ideal in such conditions where you need to test algorithms and data trans -\\nformations extensively to find the best possible combinations, but you lack the manpower and \\ncomputer power for it. That’s why, for instance, governments and companies resort to competi-\\ntions in order to advance in certain fields:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='• On the government side, we can quote DARPA and its many competitions surrounding \\nself-driving cars, robotic operations, machine translation, speaker identification, finger -\\nprint recognition, information retrieval, OCR, automatic target recognition, and many \\nothers.\\n• On the business side, we can quote a company such as Netflix, which entrusted the out -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='come of a competition to improve its algorithm for predicting user movie selection.\\nThe Netflix competition was based on the idea of improving existing collaborative filtering. The \\npurpose of this was simply to predict the potential rating a user would give a film, solely based \\non the ratings that they gave other films, without knowing specifically who the user was or what'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='the films were. Since no user description or movie title or description were available (all being \\nreplaced with identity codes), the competition required entrants to develop smart ways to use the \\npast ratings available. The grand prize of US $1,000,000 was to be awarded only if the solution \\ncould improve the existing Netflix algorithm, Cinematch, above a certain threshold.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='The competition ran from 2006 to 2009 and saw victory for a team made up of the fusion of \\nmany previous competition teams: a team from Commendo Research & Consulting GmbH, An-\\ndreas Töscher and Michael Jahrer , quite renowned also in Kaggle competitions; two researchers \\nfrom AT&T Labs; and two others from Yahoo!. In the end, winning the competition required so'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='much computational power and the ensembling of different solutions that teams were forced to \\nmerge in order to keep pace. This situation was also reflected in the actual usage of the solution \\nby Netflix, who preferred not to implement it, but simply took the most interesting insight from \\nit in order to improve its existing Cinematch algorithm. You can read more about it in this Wired'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='article: https://www.wired.com/2012/04/netflix-prize-costs/ .\\nAt the end of the Netflix competition, what mattered was not the solution per se, which was \\nquickly superseded by the change in business focus of Netflix from DVDs to online movies. The \\nreal benefit for both the participants, who gained a huge reputation in collaborative filtering, and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 32}, page_content='the company, who could transfer its improved recommendation knowledge to its new business, \\nwere the insights that were gained from the competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='Chapter 1 7\\nThe Kaggle competition platform\\nCompanies other than Netflix have also benefitted from data science competitions. The list is \\nlong, but we can quote a few examples where the company running the competition reported a \\nclear benefit from it. For instance:\\n• The insurance company Allstate was able to improve its actuarial models built by their'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='own experts, thanks to a competition involving hundreds of data scientists ( https://www.\\nkaggle.com/c/ClaimPredictionChallenge )\\n• As another well-documented example, General Electric was able to improve by 40% on \\nthe industry-standard performance (measured by the root mean squared error metric) \\nfor predicting arrival times of airline flights, thanks to a similar competition ( https://'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='www.kaggle.com/c/flight )\\nThe Kaggle competition platform has to this day held hundreds of competitions, and these two \\nare just a couple of examples of companies that used them successfully. Let’s take a step back from \\nspecific competitions for a moment and talk about the Kaggle company, which is the common \\nthread through this book.\\nA history of Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='A history of Kaggle\\nKaggle took its  first steps in February 2010, thanks to Anthony Goldbloom, an Australian trained \\neconomist with a degree in Economics and Econometrics. After working at Australia’s Depart -\\nment of the Treasury and the Research department at the Reserve Bank of Australia, Goldbloom \\ninterned in London at The Economist, the international weekly newspaper on current affairs,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='international business, politics, and technology. At The Economist, he had occasion to write an \\narticle about big data, which inspired his idea to build a competition platform that could crowd-\\nsource the best analytical experts to solve interesting machine learning problems ( https://www.\\nsmh.com.au/technology/from-bondi-to-the-big-bucks-the-28yearold-whos-making-data-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='science-a-sport-20111104-1myq1.html ). Since the crowdsourcing dynamics played a relevant \\npart in the business idea for this platform, he derived the name Kaggle, which recalls by rhyme \\nthe term gaggle, a flock of geese, the goose also being the symbol of the platform.\\nAfter moving to Silicon Valley in the USA, his Kaggle start-up received $11.25 million in Series A'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='funding from a round led by Khosla Ventures and Index Ventures, two renowned venture capital \\nfirms. The first competitions were rolled out, the community grew, and some of the initial com-\\npetitors came to be quite prominent, such as Jeremy Howard, the Australian data scientist and \\nentrepreneur, who, after winning a couple of competitions on Kaggle, became the President and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 33}, page_content='Chief Scientist of the company.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='Introducing Kaggle and Other Data Science Competitions 8\\nJeremy Howard left his position as President in December 2013 and established a new start-up, \\nfast.ai (www.fast.ai ), offering machine learning courses and a deep learning library for coders.\\nAt the time, there were some other prominent Kagglers (the name indicating frequent partici -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='pants of competitions held by Kaggle) such as Jeremy Achin and Thomas de Godoy . After reaching \\nthe top 20 global rankings on the platform, they promptly decided to retire and to found their \\nown company, DataRobot. Soon after, they started hiring their employees from among the best \\nparticipants in the Kaggle competitions in order to instill the best machine learning knowledge'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='and practices into the software they were developing. Today, DataRobot is one of the leading \\ncompanies in developing AutoML solutions (software for automatic machine learning).\\nThe Kaggle competitions claimed more and more attention from a growing audience. Even Geoffrey \\nHinton, the “godfather” of deep learning, participated in (and won) a Kaggle competition hosted'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='by Merck in 2012 ( https://www.kaggle.com/c/MerckActivity/overview/winners ). Kaggle was \\nalso the platform where François Chollet launched his deep learning package Keras during the \\nOtto Group Product Classification Challenge ( https://www.kaggle.com/c/otto-group-product-\\nclassification-challenge/discussion/13632 ) and Tianqi Chen launched XGBoost, a speedier'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='and more accurate version of gradient boosting machines, in the Higgs Boson Machine Learning \\nChallenge ( https://www.kaggle.com/c/higgs-boson/discussion/10335 ).\\nCompetition after competition, the community revolving around Kaggle grew to touch one mil -\\nlion in 2017, the same year as, during her keynote at Google Next, Fei-Fei Li, Chief Scientist at'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='Google, announced that Google Alphabet was going to acquire Kaggle. Since then, Kaggle has \\nbeen part of Google.Besides Keras, François Chollet has also provided the most useful and insightful \\nperspective on how to win a Kaggle competition in an answer of his on the Quo -\\nra website: https://www.quora.com/Why-has-Keras-been-so-successful-\\nlately-at-Kaggle-competitions .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='Fast iterations of multiple attempts, guided by empirical (more than theoretical) \\nevidence, are actually all that you need. We don’t think that there are many more \\nsecrets to winning a Kaggle competition than the ones he pointed out in his answer. \\nNotably, François Chollet also hosted his own competition on Kaggle ( https://'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 34}, page_content='www.kaggle.com/c/abstraction-and-reasoning-challenge/ ), which is widely \\nrecognized as being the first general AI competition in the world.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 35}, page_content='Chapter 1 9\\nToday, the Kaggle community is still active and growing. In a tweet of his ( https://twitter.\\ncom/antgoldbloom/status/1400119591246852096 ), Anthony Goldbloom reported that most of \\nits users, other than participating in a competition, have downloaded public data (Kaggle has \\nbecome an important data hub), created a public Notebook in Python or R, or learned something'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 35}, page_content='new in one of the courses offered:\\nFigure 1.1: A bar chart showing how users used Kaggle in 2020, 2019, and 2018\\nThrough the years, Kaggle has offered many of its participants even more opportunities, such as:\\n• Creating their own company\\n• Launching machine learning software and packages\\n• Getting interviews in magazines ( https://www.wired.com/story/solve-these-tough-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 35}, page_content='data-problems-and-watch-job-offers-roll-in/ )\\n• Writing machine learning books ( https://twitter.com/antgoldbloom/status  \\n/745662719588589568 )\\n• Finding their dream job\\nAnd, most importantly, learning more about the skills and technicalities involved in data science.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='Introducing Kaggle and Other Data Science Competitions 10\\nOther competition platforms\\nThough this book focuses on competitions on Kaggle, we cannot forget that many data compe -\\ntitions are held on private platforms or on other competition platforms. In truth, most of the \\ninformation you will find in this book will also hold for other competitions, since they essentially'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='all operate under similar principles and the benefits for the participants are more or less the same.\\nAlthough many other platforms are localized in specific countries or are specialized only for \\ncertain kinds of competitions, for completeness we will briefly introduce some of them, at least \\nthose we have some experience and knowledge of:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='• DrivenData  (https://www.drivendata.org/competitions/ ) is a crowdsourcing com-\\npetition platform devoted to social challenges (see https://www.drivendata.co/blog/\\nintro-to-machine-learning-social-impact/ ). The company itself is a social enterprise \\nwhose aim is to bring data science solutions to organizations tackling the world’s biggest'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='challenges, thanks to data scientists building algorithms for social good. For instance, as \\nyou can read in this article, https://www.engadget.com/facebook-ai-hate-speech-\\ncovid-19-160037191.html , Facebook has chosen DrivenData for its competition on build -\\ning models against hate speech and misinformation.\\n• Numerai ( https://numer.ai/ ) is an AI-powered, crowdsourced hedge fund based in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='San Francisco. It hosts  a weekly tournament in which you can submit your predictions \\non hedge fund obfuscated data and earn your prizes in the company’s cryptocurrency, \\nNumeraire.\\n• CrowdANALYTIX ( https://www.crowdanalytix.com/community ) is a bit less active now, \\nbut this platform used to host quite a few challenging competitions a short while ago, as'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='you can read from this blog post: https://towardsdatascience.com/how-i-won-top-\\nfive-in-a-deep-learning-competition-753c788cade1 . The community blog is quite \\ninteresting for getting an idea of what challenges you can find on this platform: https://\\nwww.crowdanalytix.com/jq/communityBlog/listBlog.html .\\n• Signate ( https://signate.jp/competitions ) is a Japanese data science competition'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 36}, page_content='platform. It is quite rich in contests and it offers a ranking system similar to Kaggle’s \\n(https://signate.jp/users/rankings ).\\n• Zindi (https://zindi.africa/competitions ) is a  data science competition platform  from \\nAfrica. It hosts competitions focused on solving Africa’s most pressing social, economic, \\nand environmental problems.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='Chapter 1 11\\n• Alibaba Cloud ( https://www.alibabacloud.com/campaign/tianchi-competitions ) \\nis a Chinese cloud computer and AI provider that has launched the Tianchi Academic \\ncompetitions, partnering with academic conferences such as SIGKDD, IJCAI-PRICAI, and \\nCVPR and featuring challenges such as image-based 3D shape retrieval, 3D object recon-\\nstruction, and instance segmentation.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='• Analytics Vidhya ( https://datahack.analyticsvidhya.com/ ) is the largest Indian com -\\nmunity for data science, offering a platform for data science hackathons.\\n• CodaLab ( https://codalab.lri.fr/ ) is a French-based data science competition plat -\\nform, created  as a joint venture between Microsoft and Stanford University in 2013. They'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='feature a free cloud-based notebook called Worksheets ( https://worksheets.codalab.\\norg/) for knowledge sharing and reproducible modeling.\\nOther minor platforms are CrowdAI ( https://www.crowdai.org/ ) from École Polytechnique \\nFédérale de Lausanne in Switzerland, InnoCentive ( https://www.innocentive.com/ ), Grand-Chal -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='lenge (https://grand-challenge.org/ ) for biomedical imaging, DataFountain ( https://www.\\ndatafountain.cn/business?lang=en-US ), OpenML ( https://www.openml.org/ ), and the list \\ncould go on. You can always find a large list of ongoing major competitions at the Russian com-\\nmunity Open Data Science ( https://ods.ai/competitions ) and even discover new competition \\nplatforms from time to time.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='platforms from time to time.\\nKaggle is always the best platform where you can find the most interesting competitions and ob -\\ntain the widest recognition for your competition efforts. However, picking up a challenge outside \\nof it makes sense, and we recommend it as a strategy, when you find a competition matching'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='your personal and professional interests. As you can see, there are quite a lot of alternatives and \\nopportunities besides Kaggle, which means that if you consider more competition platforms \\nalongside Kaggle, you can more easily find a competition that might interest you because of its \\nspecialization or data.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='specialization or data.\\nIn addition, you can expect less competitive pressure during these challenges (and consequently \\na better ranking or even winning something), since they are less known and advertised. Just ex -\\npect less sharing among participants, since no other competition platform has reached the same'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 37}, page_content='richness of sharing and networking opportunities as Kaggle.You can see an overview of running competitions on the mlcontests.com  website, \\nalong with the current costs for renting GPUs. The website is often updated and it \\nis an easy way to get a glance at what’s going on with data science competitions \\nacross different platforms.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 38}, page_content='Introducing Kaggle and Other Data Science Competitions 12\\nIntroducing Kaggle\\nAt this  point, we need to delve more deeply into how Kaggle in particular works. In the following \\nparagraphs, we will discuss the various aspects of the Kaggle platform and its competitions, and \\nyou’ll get a flavor of what it means to be in a competition on Kaggle. Afterward, we’ll come back'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 38}, page_content='to discuss many of these topics in much more detail, with more suggestions and strategies in the \\nremaining chapters of the book.\\nStages of a competition\\nA competition on Kaggle is arranged into different steps. By having a look at each of them, you can \\nget a better understanding of how a data science competition works and what to expect from it.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 38}, page_content='When a competition is launched, there are usually some posts on social media, for instance on \\nthe Kaggle Twitter profile, https://twitter.com/kaggle , that announce it, and a new tab will \\nappear in the Kaggle section about Active Competitions  on the Competitions  page (https://\\nwww.kaggle.com/competitions ). If you click on a particular competition’s tab, you’ll be taken'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 38}, page_content='to its page. At a glance, you can check if the competition will have prizes (and if it awards points \\nand medals, a secondary consequence of participating in a competition), how many teams are \\ncurrently involved, and how much time is still left for you to work on a solution:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 39}, page_content='Chapter 1 13\\nFigure 1.2: A competition’s page on Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='Introducing Kaggle and Other Data Science Competitions 14\\nThere, you can explore the Overview menu first, which provides information about:\\n• The topic of the competition\\n• Its evaluation metric (that your models will be evaluated against)\\n• The timeline of the competition\\n• The prizes\\n• The legal or competition requirements'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='Usually the timeline is a bit overlooked, but it should be one of the first things you check; it \\ndoesn’t tell you simply when the competition starts and ends, but it will provide you with the \\nrule acceptance deadline, which is usually from seven days to two weeks before the competition \\ncloses. The rule acceptance deadline marks the last day you can join the competition (by accepting'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='its rules). There is also  the team merger deadline: you can arrange to combine your team with \\nanother competitor’s one at any point before that deadline, but after that it won’t be possible.\\nThe Rules menu is  also quite often overlooked (with people just jumping to Data), but it is im -\\nportant to check it because it can tell you about the requirements of the competition. Among the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='key information you can get from the rules, there is:\\n• Your eligibility for a prize\\n• Whether you can use external data to improve your score\\n• How many submissions (tests of your solution) a day you get\\n• How many final solutions you can choose\\nOnce you have accepted the rules, you can download any data from the Data menu or directly start'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='working on Kaggle Notebooks (online, cloud-based notebooks) from the Code menu, reusing \\ncode that others have made available or creating your own code from scratch.\\nIf you decide to download the data, also consider that you have a Kaggle API that can help you to \\nrun downloads and submissions in an almost automated way. It is an important tool if you are'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 40}, page_content='running your models on your local computer or on your cloud instance. You can find more details \\nabout the API at https://www.kaggle.com/docs/api  and you can get the code from GitHub at \\nhttps://github.com/Kaggle/kaggle-api .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 41}, page_content='Chapter 1 15\\nIf you check the Kaggle GitHub repo closely, you can also find all the Docker images they use for \\ntheir online notebooks, Kaggle Notebooks:\\nFigure 1.3: A Kaggle Notebook ready to be coded\\nAt this point, as  you develop your solution, it is our warm suggestion not to continue in solitude, \\nbut to contact other competitors through the Discussion forum, where you can ask and answer'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 41}, page_content='questions specific to the competition. Often you will also find useful hints about specific problems \\nwith the data or even ideas to help improve your own solution. Many successful Kagglers have \\nreported finding ideas on the forums that have helped them perform better and, more importantly, \\nlearn more about modeling in data science.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 41}, page_content='Once your solution is ready, you can submit it to the Kaggle evaluation engine, in adherence to \\nthe specifications of the competition. Some competitions will accept a CSV file as a solution, oth-\\ners will require you to code and produce results in a Kaggle Notebook. You can keep submitting \\nsolutions throughout the competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 41}, page_content='Every time you submit a solution, soon after, the leaderboard will provide you with a score and a \\nposition among the competitors (the wait time varies depending on the computations necessary \\nfor the score evaluation). That position is only roughly indicative, because it reflects the perfor -\\nmance of your model on a part of the test set, called the public test set, since your performance'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 41}, page_content='on it is made public during the competition for everyone to know.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 42}, page_content='Introducing Kaggle and Other Data Science Competitions 16\\nBefore the competition closes, each competitor can choose a number (usually two) of their solu-\\ntions for the final evaluation.\\nFigure 1.4: A diagram demonstrating how data turns into scores for the public and private \\nleaderboard\\nOnly when the competition closes, based on the models the contestants have decided to be scored,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 42}, page_content='is their score on another part of the test set, called the private test set, revealed. This new leader -\\nboard, the private leaderboard, constitutes the final, effective scores for the competition, but it is \\nstill not official and definitive in its rankings. In fact, the Kaggle team will take some time to check'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 42}, page_content='that everything is correct and that all contestants have respected the rules of the competition.\\nAfter a while (and sometimes after some changes in the rankings due to disqualifications), the \\nprivate leaderboard will become official and definitive, the winners will be declared, and many par -\\nticipants will unveil their strategies, their solutions, and their code on the competition discussion'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 42}, page_content='forum. At this point, it is up to you to check the other solutions and try to improve your own. We \\nstrongly recommend that you do so, since this is another important source of learning in Kaggle.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='Chapter 1 17\\nTypes of competitions and examples\\nKaggle competitions are categorized based on competition categories, and each category has a \\ndifferent implication in terms of how to compete and what to expect. The type of data, difficulty \\nof the problem, awarded prizes, and competition dynamics are quite diverse inside the categories,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='therefore it is important to understand beforehand what each implies.\\nHere are the official categories that you can use to filter out the different competitions:\\n• Featured\\n• Masters\\n• Annuals\\n• Research\\n• Recruitment\\n• Getting Started\\n• Playground\\n• Analytics\\n• Community\\nFeatured are the most common type of competitions, involving a business-related problem from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='a sponsor company and a prize for the top performers. The winners will grant a non-exclusive \\nlicense of their work to the sponsor company; they will have to prepare a detailed report of their \\nsolution and sometimes even participate in meetings with the sponsor company.\\nThere are examples of  Featured competitions every time you visit Kaggle. At the moment, many of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='them are problems relating to the application of deep learning methods to unstructured data like \\ntext, images, videos, or sound. In the past, tabular data competitions were commonly seen, that \\nis, competitions based on problems relating to structured data that can be found in a database. \\nFirst by using random forests, then gradient boosting methods with clever feature engineering,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='tabular data solutions derived from Kaggle could really improve an existing solution. Nowadays, \\nthese competitions are run much less often, because a crowdsourced solution won’t often be \\nmuch better than what a good team of data scientists or even AutoML software can do. Given \\nthe spread of better software and good practices, the increase in result quality obtainable from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 43}, page_content='competitions is indeed marginal. In the unstructured data world, however, a good deep learning \\nsolution could still make a big difference. For instance, pre-trained networks such as BERT brought \\nabout double-digit increases in previous standards for many well-known NLP task benchmarks.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='Introducing Kaggle and Other Data Science Competitions 18\\nMasters  are less common now, but they are private, invite-only competitions. The purpose was to \\ncreate competitions only for experts (generally competitors ranked as Masters or Grandmasters, \\nbased on Kaggle medal rankings), based on their rankings on Kaggle.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='Annuals are competitions that always appear during a certain period of the year. Among the \\nAnnuals, we have the Santa Claus competitions (usually based on an algorithmic optimization \\nproblem) and the March Machine Learning Mania  competition, run every year since 2014 during \\nthe US College Basketball Tournaments.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='Research competitions imply a research or science purpose  instead of a business one, sometimes \\nfor serving the public good. That’s why these competitions do not always offer prizes. In addi-\\ntion, these competitions sometimes require the winning participants to release their solution as \\nopen-source.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='open-source.\\nGoogle has released a few Research competitions in the past, such as Google Landmark Recognition \\n2020 (https://www.kaggle.com/c/landmark-recognition-2020 ), where the goal was to label \\nfamous (and not-so-famous) landmarks in images.\\nSponsors that want to test the ability of potential job candidates hold Recruitment competitions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='These competitions are limited to teams of one and offer to best-placed competitors an interview \\nwith the sponsor as a prize. The competitors have to upload their CV at the end of the competition \\nif they want to be considered for being contacted.\\nExamples of Recruitment competitions have been:\\n• The Facebook Recruiting Competition ( https://www.kaggle.com/c/FacebookRecruiting );'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='Facebook have held a few of this kind\\n• The Yelp Recruiting Competition  (https://www.kaggle.com/c/yelp-recruiting )\\nGetting Started competitions do not offer any prizes, but friendly and easy problems for beginners \\nto get accustomed to Kaggle principles and dynamics. They are usually semi-permanent com -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='petitions whose leaderboards are refreshed from time to time. If you are looking for a tutorial in \\nmachine learning, these competitions are the right places to start, because you can find a highly \\ncollaborative environment and there are many Kaggle Notebooks available showing you how to \\nprocess the data and create different types of machine learning models.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 44}, page_content='Famous ongoing Getting Started competitions are:\\n• Digit Recognizer  (https://www.kaggle.com/c/digit-recognizer )'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='Chapter 1 19\\n• Titanic — Machine Learning from Disaster  (https://www.kaggle.com/c/titanic )\\n• House Prices — Advanced Regression Techniques  (https://www.kaggle.com/c/house-\\nprices-advanced-regression-techniques )\\nPlayground competitions are a little bit more  difficult than the Getting Started ones, but they are'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='also meant for competitors to learn and test their abilities without the pressure of a fully-fledged \\nFeatured competition (though in Playground competitions sometimes the heat of the competition \\nmay also turn quite high). The usual prizes for such competitions are just swag (an acronym for \\n“Stuff We All Get,” such as, for instance, a cup, a t-shirt, or socks branded by Kaggle; see https://'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='www.kaggle.com/general/68961 ) or a bit of money.\\nOne famous Playground competition is the original Dogs vs. Cats competition ( https://www.\\nkaggle.com/c/dogs-vs-cats ), where the task is to create an algorithm to distinguish dogs from \\ncats.\\nMentions should be given to Analytics competitions, where the evaluation is qualitative and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='participants are required to provide ideas, drafts of solutions, PowerPoint slides, charts, and so \\non; and Community (previously known as InClass) competitions, which are held by academic \\ninstitutions as well as Kagglers. You can read about the launch of the  Community competitions \\nat https://www.kaggle.com/product-feedback/294337  and you can get tips about running'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='one of your own at https://www.kaggle.com/c/about/host  and at https://www.kaggle.com/\\ncommunity-competitions-setup-guide .\\nParul Pandey\\nhttps://www.kaggle.com/parulpandey\\nWe spoke to Parul Pandey, Kaggle Notebooks Grandmaster, Datasets \\nMaster, and data scientist at H2O.ai, about her experience with Ana-\\nlytics competitions and more. \\nWhat’s your favorite kind of competition and why?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='In terms of techniques and solving approaches, what is your specialty \\non Kaggle?\\nI really enjoy the Data Analytics competitions, which require you to analyze the data and provide a \\ncomprehensive analysis report at the end. These include the Data Science for Good competitions (DS4G), \\nsports analytics competitions (NFL etc.), and the general survey challenges. Unlike the traditional com-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 45}, page_content='petitions, these competitions don’t have a leaderboard to track your performance compared to others; \\nnor do you get any medals or points.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='Introducing Kaggle and Other Data Science Competitions 20\\nOn the other hand, these competitions demand end-to-end solutions touching on multi-faceted aspects \\nof data science like data cleaning, data mining, visualizations, and conveying insights. Such problems \\nprovide a way to mimic real-life scenarios and provide your insights and viewpoints. There may not be'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='a single best answer to solve the problem, but it gives you a chance to deliberate and weigh up potential \\nsolutions, and imbibe them into your solution.\\nHow do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work? \\nMy first step is always to analyze the data as part of EDA (exploratory data analysis). It is something that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='I also follow as part of my work routine. Typically, I explore the data to look for potential red flags like \\ninconsistencies in data, missing values, outliers, etc., which might pose problems later. The next step is to \\ncreate a good and reliable cross-validation strategy. Then I read the discussion forums and look at some'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='of the Notebooks shared by people. It generally acts as a good starting point, and then I can incorporate \\nthings in this workflow from my past experiences. It is also essential to track the model performance.\\nFor an Analytics competition, however, I like to break down the problem into multiple steps. For instance,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='the first part could be related to understanding the problem, which may require a few days. After that, I \\nlike to explore the data, followed by creating a basic baseline solution. Then I continue enhancing this \\nsolution by adding a piece at a time. It might be akin to adding Lego bricks one part at a time to create \\nthat final masterpiece.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='that final masterpiece.\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.\\nAs I mentioned, I mostly like to compete in Analytics competitions, even though occasionally I also try \\nmy hand in the regular ones too. I’d like to point out a very intriguing Data Science for Good competi -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='tion titled Environmental Insights Explorer (https://www.kaggle.com/c/ds4g-environmental-insights- \\nexplorer). The task was to use remote sensing techniques to understand environmental emissions instead \\nof calculating emissions factors from current methodologies.\\nWhat really struck me was the use case. Our planet is grappling with climate change issues, and this compe -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='tition touched on this very aspect. While researching for my competition, I was amazed to find the amount \\nof progress being made in this field of satellite imagery and it gave me a chance to understand and dive more \\ndeeply into the topic. It gave me a chance to understand how satellites like Landsat, Modis, and Sentinel'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 46}, page_content='worked, and how they make the satellite data available. This was a great competition to learn about a field I \\nknew very little about before the competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='Chapter 1 21\\nCross-sectional to this taxonomy of Kaggle competitions, you also have to consider that competi -\\ntions may have different formats. The usual format is the so-called Simple format where  you pro-\\nvide a solution and it is evaluated as we previously described. More sophisticated, the two-stage'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='competition  splits the contest into two parts, and the final dataset is released only after the first \\npart has finished and only to the participants of the first part. The two-stage competition format \\nhas emerged in order to limit the chance of some competitors cheating and infringing the rules,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='since the evaluation is done on a completely untried test set that is available for a short time only. \\nContrary to the original Kaggle competition format, in this case, competitors have a much shorter \\namount of time and much fewer submissions to figure out any useful patterns from the test set. In your experience, what do inexperienced Kagglers often overlook?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='What do you know now that you wish you’d known when you first \\nstarted?\\nI will cite some of the mistakes that I made in my initial years on Kaggle. \\nFirstly, most of the newbies think of Kaggle as a competitions-only platform. If you love competitions, there are \\nplenty here, but Kaggle also has something for people with other specialties. You can write code and share it'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='with others, indulge in healthy discussions, and network. Curate and share good datasets with the community. I \\ninitially only used Kaggle for downloading datasets, and it was only a couple of years ago that I actually became \\nactive. Now when I look back, I couldn’t have been more wrong. A lot of people get intimidated by competi -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='tions. You can first get comfortable with the platform and then slowly start participating in the competitions.\\nAnother important thing that I would like to mention is that many people work in isolation, lose motivation, \\nand quit. Teaming up on Kaggle has many unseen advantages. It teaches you to work in a team, learn from \\nthe experiences, and work towards a common goal in a limited time frame.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='Do you use other competition platforms? How do they compare to \\nKaggle?\\nWhile most of my current time is spent on Kaggle, in the past I have used Zindi, a data science competition \\nplatform focused on African use cases. It’s a great place to access datasets focused on Africa. Kaggle is a versa -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 47}, page_content='tile platform, but there is a shortage of problem statements from different parts of the world. Of late, we have \\nseen some diversified problems too, like the recently held chaii competition\\u200a—\\u200aan NLP competition focusing on \\nIndian languages. I believe similar competitions concentrating on different countries will be helpful for the \\nresearch and the general data science community as well .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='Introducing Kaggle and Other Data Science Competitions 22\\nFor the same reason, the Code competitions have recently appeared, where all submissions are \\nmade from a Kaggle Notebook, and any direct upload of submissions is disabled.\\nFor Kagglers at different stages of their competition careers, there are no restrictions at all in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='taking on any kind of competition. However, we have some suggestions against or in favor of \\nthe format or type of competition depending on your level of experience in data science and your \\ncomputational resources:\\n• For complete beginners, the Getting Started or the Playground competitions are good \\nplaces to begin, since you can easily get more confident about how Kaggle works with-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='out facing high competitive pressure. That being said, many beginners have successfully \\nstarted from Featured and Research competitions, because being under pressure helped \\nthem to learn faster. Our suggestion is therefore to decide based on your learning style: \\nsome Kagglers need to learn by exploring and collaborating (and the Getting Started'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='or the Playground competitions are ideal for that), others need the heat of a fast-paced \\ncompetition to find their motivation. \\n• For Featured and Research  competitions, also take into account that these competitions \\nare often about fringe applications of AI and machine learning and, consequently, you \\noften need a solid background or the willingness to study all the relevant research in the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='field of application of the competition.\\nFinally, keep in mind that most competitions require you to have access to computational resources \\nthat are often not available to most data scientists in the workplace. This can turn into growing \\nexpenses if you use a cloud platform outside the Kaggle one. Code competitions and competitions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='with time or resource limitations might then be the ideal place to spend your efforts, since they \\nstrive to put all the participants on the same resource level.\\nSubmission and leaderboard dynamics\\nThe way Kaggle works seems simple: the test set is hidden to participants; you fit your model; \\nif your model is the best in predicting on the test set, then you score highly and you possibly'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 48}, page_content='win. Unfortunately, this description renders the inner workings of Kaggle competitions in an \\noverly simplistic way. It doesn’t take into account that there are dynamics regarding the direct \\nand indirect interactions of competitors, or the nuances of the problem you are facing and of its \\ntraining and test set.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='Chapter 1 23\\nExplaining the Common Task Framework paradigm\\nA more comprehensive description of how Kaggle works is actually given by Professor Da-\\nvid Donoho, professor of statistics at Stanford University ( https://web.stanford.edu/dept/\\nstatistics/cgi-bin/donoho/ ), in his paper 50 Years of Data Science . It first appeared in the Jour -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='nal of Computational and Graphical Statistics  and was subsequently posted on the MIT Computer \\nScience and Artificial Intelligence Laboratory (see http://courses.csail.mit.edu/18.337/2015/\\ndocs/50YearsDataScience.pdf ).\\nProfessor Donoho does not refer to Kaggle specifically, but to all data science competition plat -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='forms. Quoting computational linguist Mark Liberman, he refers to data science competitions \\nand platforms as being part of a Common Task Framework  (CTF) paradigm that has been si -\\nlently and steadily progressing data science in many fields during the last decades. He states \\nthat a CTF can work incredibly well at improving the solution of a problem in data science from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='an empirical point of view, quoting the Netflix competition and many DARPA competitions as \\nsuccessful examples. The CTF paradigm has contributed to reshaping the best-in-class solutions \\nfor problems in many fields.\\nA CTF is composed of ingredients and a secret sauce. The ingredients are simply:\\n1. A publicly available dataset and a related prediction task'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='2. A set of competitors who share the common task of producing the best prediction for \\nthe task\\n3. A system for scoring the predictions by the participants in a fair and objective way, with-\\nout providing hints about the solution that are too specific (or limiting them, at least)\\nThe system works the best if the task is well defined and the data is of good quality. In the long'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='run, the performance of solutions improves by small gains until it reaches an asymptote. The \\nprocess can be sped up by allowing a certain amount of sharing among participants (as happens \\non Kaggle by means of discussions, and sharing Kaggle Notebooks and extra data provided by the \\ndatasets found in the Datasets section). According to the CTF paradigm, competitive pressure in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 49}, page_content='a competition suffices to produce always-improving solutions. When the competitive pressure \\nis paired with some degree of sharing among participants, the improvement happens at an even \\nfaster rate – hence why Kaggle introduced many incentives for sharing.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='Introducing Kaggle and Other Data Science Competitions 24\\nThis is because  the secret sauce in the CTF paradigm is the competition itself, which, within the \\nframework of a practical problem whose empirical performance has to be improved, always leads \\nto the emergence of new benchmarks, new data and modeling solutions, and in general to an im -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='proved application of machine learning to the problem posed by the competition. A competition \\ncan therefore provide a new way to solve a prediction problem, new ways of feature engineering, \\nand new algorithmic or modeling solutions. For instance, deep learning did not simply  emerge \\nfrom academic research, but it first gained a great boost because of successful competitions that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='signaled its efficacy (we have already mentioned, for instance, the Merck competition, won by \\nGeoffrey Hinton’s team: https://www.kaggle.com/c/MerckActivity/overview/winners ).\\nCoupled with the open software movement, which allows everyone access to powerful analytical \\ntools (such as Scikit-learn, TensorFlow, or PyTorch), the CTF paradigm brings about even better'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='results because all competitors are on the same level at the start. On the other hand, the reliance \\nof a solution to a competition on specialized or improved hardware can limit achievable results, \\nbecause it can prevent competitors without access to such resources from properly participating \\nand contributing directly to the solution, or indirectly by exercising competitive pressure on the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='other participants. Understandably, this is the reason why Kaggle started offering cloud services \\nfree to participants of its competitions, the Kaggle Notebooks we will introduce in the Compu-\\ntational resources section. It can flatten some differences in hardware-intense competitions (as \\nmost deep learning ones are) and increase the overall competitive pressure.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='Understanding what can go wrong in a competition\\nGiven our previous description of the CTF paradigm, you may be tempted to imagine that all a \\ncompetition needs is to be set up on a proper platform, and good results such as positive involve -\\nment for participants and outstanding models for the sponsor company will automatically come'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='in. However, there are also things that can go wrong and instead lead to a disappointing result \\nin a competition, both for the participants and the institution running it:\\n• Leakage from the data\\n• Probing from the leaderboard (the scoring system)\\n• Overfitting and consequent leaderboard shake-up\\n• Private sharing'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='• Private sharing\\nYou have leakage from data when part of the solution can be retraced in the data itself. For instance, \\ncertain variables could be posterior to the target variable, so they reveal something about it. This \\nhappens in fraud detection when you use variables that are updated after a fraud happens, or in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 50}, page_content='sales forecasting when you process information relating to the effective distribution of a product \\n(more distribution implies more requests for the product, hence more sales).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='Chapter 1 25\\nAnother issue could be that the training and test examples are ordered in a predictable way or \\nthat the values of the identifiers of the examples hint at the solution. Examples are, for instance, \\nwhen the identifier is based on the ordering of the target, or the identifier value is correlated with \\nthe flow of time and time affects the probability of the target.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='Such solution leakage, sometimes named golden features  by competitors (because getting a hint \\nof such nuances in the data can turn into gold prizes for the participants), invariably leads to a \\nsolution that is not reusable. This also implies a sub-optimal result for the sponsor, but they at \\nleast are able to learn something about leaking features that can affect solutions to their problem.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='Another problem is the possibility of probing a solution  from the leaderboard. In this situation, \\nyou can take advantage of the evaluation metrics shown to you and snoop the solution by repeated \\nsubmission trials on the leaderboard. Again, in this case the solution is completely unusable in \\ndifferent circumstances. A clear example of this happened in the competition Don’t Overfit II . The'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='winning participant, Zachary Mayers, submitted every individual variable as a single submission, \\ngaining information about the possible weight of each variable that allowed him to estimate \\nthe correct coefficients for his model (you can read Zach’s detailed solution here: https://www.\\nkaggle.com/c/dont-overfit-ii/discussion/91766 ). Generally, time series problems, or other'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='problems where there are systematic shifts in the test data, may be seriously affected by probing, \\nsince they can help competitors to successfully define some kind of post-processing  (like multiply -\\ning their predictions by a constant) that is most suitable for scoring highly on the specific test set.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='Another form of leaderboard snooping (that is, getting a hint about the test set and overfitting \\nto it) happens when participants rely more on the feedback from the public leaderboard than \\ntheir own tests. Sometimes this turns into a complete failure of the competition, causing a wild \\nshake-up – a complete and unpredictable reshuffling of the positions on the final leaderboard.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='The winning solutions, in such a case, may turn out to be not so optimal for the problem or even \\njust dictated by chance. This has led to the diffusion of techniques analyzing the potential gap \\nbetween the training set and the public test set. This kind of analysis, called adversarial testing, \\ncan provide insight about how much to rely on the leaderboard and whether there are features that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 51}, page_content='are so different between the training and test set that it would be better to avoid them completely. \\nFor an example, you can have a look at this Notebook by Bojan Tunguz : https://www.kaggle.\\ncom/tunguz/adversarial-ieee .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='Introducing Kaggle and Other Data Science Competitions 26\\nAnother kind of defense against leaderboard overfitting is choosing safe strategies to avoid submit -\\nting solutions that are based too much on the leaderboard results. For instance, since (typically) \\ntwo solutions are allowed to be chosen by each participant for final evaluation, a good strategy is'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='to submit the best performing one based on the leaderboard, and the best performing one based \\non your own cross-validation tests.\\nIn order to avoid problems with leaderboard probing and overfitting, Kaggle has recently intro -\\nduced different innovations based on Code competitions, where the evaluation is split into two'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='distinct stages, as we previously discussed, with participants being completely blind to the actual \\ntest data so they are forced to consider their own local validation tests more.\\nFinally, another possible distortion of a competition is due to private sharing (sharing ideas \\nand solutions in a closed circle of participants) and other illicit moves such as playing through'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='multiple accounts or playing in multiple teams and stealing ideas. All such actions create an \\nasymmetry of information between participants that can be favorable to a few and detrimental \\nto most. Again, the resulting solution may be affected because sharing has been imperfect during \\nthe competition and fewer teams have been able to exercise full competitive pressure. Moreover,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='if these situations become evident to participants (for instance, see https://www.kaggle.com/c/\\nashrae-energy-prediction/discussion/122503 ), it can lead to distrust and less involvement \\nin the competition or subsequent competitions.\\nComputational resources\\nSome competitions pose limitations in order to render feasible  solutions available to production.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='For instance, the Bosch Production Line Performance competition ( https://www.kaggle.com/c/\\nbosch-production-line-performance ) had strict limits on execution time, model file output, and \\nmemory limit for solutions. Notebook-based (previously known as Kernel-Only) competitions, \\nwhich require both training and inference to be executed on the Kaggle Notebooks, do not pose'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='a problem for the resources you have to use. This is because Kaggle will provide you with all the \\nresources you need (and this is also intended as a way to put all participants on the same start \\nline for a better competition result).\\nProblems arise when you have competitions that only limit the use of Notebooks to inference'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 52}, page_content='time. In these cases, you can train your models on your own machine and the only limit is then \\nat test time, on the number and complexity of models you produce. Since most competitions at \\nthe moment require deep learning solutions, you have to be aware that you will need  specialized \\nhardware, such as GPUs, in order to achieve a competitive result.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='Chapter 1 27\\nEven in some of the now-rare tabular competitions, you’ll soon realize that you need a strong \\nmachine with quite a number of processors and a lot of memory in order to easily apply feature \\nengineering to data, run experiments, and build models quickly.\\nStandards change rapidly, so it is difficult to specify a standard hardware that you should have'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='in order to compete at least in the same league as other teams. We can get hints about the cur -\\nrent standard by looking at what other competitors are using, either as their own machine or a \\nmachine on the cloud.\\nFor instance, HP launched a program where it awarded an HP Z4 or Z8 to a few selected Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='participants in exchange for brand visibility. For instance, a Z8 machine has up to 72 cores, 3 TB \\nof memory, 48 TB of storage (a good share by solid storage hard drive standards), and usually \\ndual NVIDIA RTX as the GPU. We understand that this may be a bit out of reach for many; even \\nrenting a similar machine for a short time on a cloud instance such as Google’s GCP or Amazon’s'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='AWS is out of the discussion, given the expenses for even moderate usage.\\nOur suggestion, as you start your journey to climb to the top rankings of Kaggle participants, is \\ntherefore to go with the machines provided free by Kaggle, Kaggle Notebooks (previously known \\nas Kaggle Kernels).\\nKaggle Notebooks\\nKaggle Notebooks are versioned computational environments, based on Docker containers run -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='ning in cloud machines, that allow you to write and execute both scripts and notebooks in the R \\nand Python languages. Kaggle Notebooks:\\n• Are integrated into the Kaggle environment (you can make submissions from them and \\nkeep track of what submission refers to what Notebook)\\n• Come with most data science packages pre-installed'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='• Allow some customization (you can download files and install further packages)\\nThe basic Kaggle Notebook is just CPU-based, but you can have versions boosted by an NVIDIA \\nTesla P100 or a TPU v3-8. TPUs are hardware accelerators specialized for deep learning tasks. The cloud costs for each competition naturally depend on the amount of data to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 53}, page_content='process and on the number and type of models you build. Free credit giveaways in \\nKaggle competitions for both GCP and AWS cloud platforms usually range from US \\n$200 to US $500.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='Introducing Kaggle and Other Data Science Competitions 28\\nThough bound by a usage number and time quota limit, Kaggle Notebooks give you access to the \\ncomputational workhorse to build your baseline solutions on Kaggle competitions:\\nNotebook \\ntypeCPU \\ncoresMemory Number of notebooks that \\ncan be run at a timeWeekly quota\\nCPU 4 16 GB 10 Unlimited\\nGPU 2 13 GB 2 30 hours\\nTPU 4 16 GB 2 30 hours'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='TPU 4 16 GB 2 30 hours\\nBesides the total runtime, CPU and GPU notebooks can run for a maximum of 12 hours per session \\nbefore stopping (TPU notebooks for just 9 hours) meaning you won’t get any results from the \\nrun apart from what you have saved on disk. You have a 20 GB disk saving allowance to store'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='your models and results, plus an additional scratchpad disk that can exceed 20 GB for temporary \\nusage during script running.\\nIn certain cases, the GPU-enhanced machine provided by Kaggle Notebooks may not be enough. \\nFor instance, the recent Deepfake Detection Challenge ( https://www.kaggle.com/c/deepfake-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='detection-challenge ) required the processing of data consisting of around 500 GB of videos. \\nThat is especially challenging because of the 30-hour time limit of weekly usage, and because of \\nthe fact that you cannot have more than two machines with GPUs running at the same time. Even \\nif you can double your machine time by changing your code to leverage the usage of TPUs instead'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='of GPUs (which you can find some guidance for easily achieving here: https://www.kaggle.com/\\ndocs/tpu ), that may still not prove enough for fast experimentation in a data-heavy competition \\nsuch as the Deepfake Detection Challenge.\\nFor this reason, in Chapter 3, Working and Learning with Kaggle Notebooks , we are going to provide'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='you with tips for successfully coping with these limitations to produce decent results without \\nhaving to buy a heavy-performing machine. We are also going to show you how to integrate \\nKaggle Notebooks with GCP or, alternatively, in Chapter 2, Organizing Data with Datasets , how to \\nmove all your work into another cloud-based solution, Google Colab.\\nTeaming and networking'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='Teaming and networking\\nWhile computational power plays its part, only human expertise and ability can make the real \\ndifference in a Kaggle competition. For a competition to be handled successfully, it sometimes \\nrequires the collaborative efforts of a team of contestants. Apart from Recruitment competitions,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 54}, page_content='where the sponsor may require individual participants for a better evaluation of their abilities, \\nthere is typically no restriction against forming teams. Usually, teams can be made up of a max -\\nimum of five contestants.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='Chapter 1 29\\nTeaming has its own advantages because it can multiply efforts to find a better solution. A team \\ncan spend more time on the problem together and different skills can be of great help; not all data \\nscientists will have the same skills or the same level of skill when it comes to different models \\nand data manipulation.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='and data manipulation.\\nHowever, teaming is not all positive. Coordinating different individuals and efforts toward a com -\\nmon goal may prove not so easy, and some suboptimal situations may arise. A common problem \\nis when some of the participants are not involved or are simply idle, but no doubt the worst is'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='when someone infringes the rules of the competition – to the detriment of everyone, since the \\nwhole team could be disqualified – or even spies on the team in order to give an advantage to \\nanother team, as we mentioned earlier.\\nIn spite of any negatives, teaming in a Kaggle competition is a great opportunity to get to know'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='other data scientists better, to collaborate for a purpose, and to achieve more, since Kaggle rules \\ndo reward teams over lonely competitors. In fact, for smaller teams you get a percentage of the \\ntotal that is higher than an equal share. Teaming up is not the only possibility for networking in \\nKaggle, though it is certainly more profitable and interesting for the participants. You can also'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='network with others through discussions on the forums, or by sharing Datasets and Notebooks \\nduring competitions. All these opportunities on the platform can help you get to know other data \\nscientists and be recognized in the community.\\nThere are also many occasions to network with other Kagglers outside of the Kaggle platform'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='itself. First of all, there are a few Slack channels that can be helpful. For instance, KaggleNoobs \\n(https://www.kaggle.com/getting-started/20577 ) is a channel, opened up in 2016, that fea-\\ntures many discussions about Kaggle competitions. They have a supportive community that can \\nhelp you if you have some specific problem with code or models.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='There are quite a few other channels devoted to exchanging opinions about Kaggle competitions \\nand data science-related topics. Some channels are organized on a regional or national basis, for \\ninstance, the Japanese  channel Kaggler-ja  (http://kaggler-ja-wiki.herokuapp.com/ ) or the \\nRussian community Open Data Science Network  (https://ods.ai/ ), created in 2015, which'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 55}, page_content='later opened also to non-Russian speaking participants. The Open Data Science Network doesn’t \\noffer simply a Slack channel but also courses on how to win competitions, events, and reporting \\non active competitions taking place on all known data science platforms (see https://ods.ai/\\ncompetitions ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='Introducing Kaggle and Other Data Science Competitions 30\\nAside from Slack channels, quite a few local meetups themed around Kaggle in general or around \\nspecific competitions have sprung up, some just on a temporary basis, others in a more estab -\\nlished form. A meetup focused on Kaggle competitions, usually built around a presentation from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='a competitor who wants to share their experience or suggestions, is the best way to meet other \\nKagglers in person, to exchange opinions, and to build alliances for participating in data science \\ncontests together.\\nIn this league, a mention should be given to Kaggle Days ( https://kaggledays.com/ ), built by'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='Maria Parysz  and Paweł Jankiewicz . The Kaggle Days organization arranged a few events in major \\nlocations around the world ( https://kaggledays.com/about-us/ ) with the aim of bringing \\ntogether a conference of Kaggle experts. It also created a network of local meetups in different \\ncountries, which are still quite active ( https://kaggledays.com/meetups/ ).\\nPaweł Jankiewicz'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='Paweł Jankiewicz\\nhttps://www.kaggle.com/paweljankiewicz\\nWe had the opportunity to catch up with Paweł about his experiences \\nwith Kaggle. He is a Competitions Grandmaster and a co-founder of \\nLogicAI.\\nWhat’s your favourite kind of competition and \\nwhy? In terms of techniques and solving approaches, what is your \\nspecialty on Kaggle?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='specialty on Kaggle?\\nCode competitions are my favourite type of competition because working in a limited environment forces \\nyou to think about different kinds of budgets: time, CPU, memory. Too many times in previous compe -\\ntitions I needed to utilize even up to 3-4 strong virtual machines. I didn’t like that in order to win I had'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='to utilize such resources, because it makes it a very uneven competition.\\nHow do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?\\nI approach every competition a little bit differently. I tend to always build a framework for each compe -\\ntition that allows me to create as many experiments as possible. For example, in one competition where'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='we needed to create a deep learning convolutional neural network, I created a way to configure neural \\nnetworks by specifying them in the format C4-MP4-C3-MP3 (where each letter stands for a different \\nlayer). It was many years ago, so the configuration of neural networks is probably now done by selecting'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 56}, page_content='the backbone model. But the rule still applies. You should create a framework that allows you to change \\nthe most sensitive parts of the pipeline quickly.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='Chapter 1 31\\nDay-to-day work has some overlap with Kaggle competitions in terms of modeling approach and proper \\nvalidation. What Kaggle competitions taught me is the importance of validation, data leakage preven-\\ntion, etc. For example, if data leaks happen in so many competitions, when people who prepare them'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='are the best in the field, you can ask yourself what percentage of production models have data leaks in \\ntraining; personally, I think 80%+ of production models are probably not validated correctly, but don’t \\nquote me on that.\\nAnother important difference in day-to-day work is that no one really tells you how to define the modeling \\nproblem. For instance:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='problem. For instance:\\n1. Should the metric you report or optimize be RMSE, RMSLE, SMAPE, or MAPE? \\n2. If the problem is time-based, how can you split the data to evaluate the model as realistically as \\npossible? \\nAnd these are not the only important things for the business. You also must be able to communicate your \\nchoices and why you made them.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='Tell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.\\nThe most challenging and interesting was the Mercari Price Prediction  Code competition. It was very \\ndifferent from any other competition because it was limited to 1 hour of computation time and only 4'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='cores with 16 GB of memory. Overcoming these limitations was the most exciting part of the challenge. \\nMy takeaway from this competition was to believe more in networks for tabular data. Before merging \\nwith my teammate Konstantin Lopukhin (https://www.kaggle.com/lopuhin ), I had a bunch of \\ncomplicated models including neural networks, but also some other boosting algorithms. After merging, it'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='turned out that Konstantin was using only one architecture which was very optimized (number of epochs, \\nlearning rate). Another aspect of this competition that was quite unique was that it wasn’t enough to just \\naverage solutions from the team. We had to reorganize our workflow so that we had a single coherent'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='solution and not something quickly put together. It took us three weeks to combine our solutions together.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?\\nSoftware engineering skills are probably underestimated a lot. Every competition and problem is slightly'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 57}, page_content='different and needs some framework to streamline the solution (look at https://github.com/bestfitting/\\ninstance_level_recognition and how well their code is organized). Good code organization helps you to \\niterate faster and eventually try more things.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='Introducing Kaggle and Other Data Science Competitions 32\\nPerformance tiers and rankings\\nApart from monetary prizes and other material items, such as cups, t-shirts, hoodies, and stick -\\ners, Kaggle offers many immaterial awards. Kagglers spend a whole lot of time and effort during \\ncompetitions (not to mention in developing the skills they use to compete that are, in truth,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='quite rare in the general population). The monetary prizes usually cover the efforts of the top \\nfew Kagglers, if not only the one in the top spot, leaving the rest with an astonishing number of \\nhours voluntarily spent with little return. In the long term, participating in competitions with \\nno tangible results may lead to disaffection and disinterest, lowering the competitive intensity.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='Hence, Kaggle has found a way to reward competitors with an honor system based on medals \\nand points. The idea is that the more medals and the more points you have, the more relevant \\nyour skills are, leaving you open for opportunities in your job search or any other relevant activity \\nbased on your reputation.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='based on your reputation.\\nFirst, there is a general leaderboard, that combines all the leaderboards of the individual com-\\npetitions ( https://www.kaggle.com/rankings ). Based on the position they attain in each com-\\npetition, Kagglers are awarded some number of points that, all summed together, provide their'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='ranking on the general leaderboard. At first glance, the formula for the scoring of the points in a \\ncompetition may look a bit complex:\\n[100000\\n√𝑁𝑁𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡]∗[𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅−0.75]∗[log10(1+log 10(𝑁𝑁𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡𝑡))]∗[𝑒𝑒−𝑡𝑡𝑡500] \\nNevertheless, in reality it is simply based on a few ingredients:\\n• Your rank in a competition\\n• Your team size\\n• The popularity of the competition'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='• How old the competition is\\nIntuitively, ranking highly in popular competitions brings many points. Less intuitively, the size \\nof your team matters in a non-linear way. That’s due to the inverse square root part of the formula, \\nsince the proportion of points you have to give up grows with the number of people involved. What’s the most important thing someone should keep in mind or do'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 58}, page_content='when they’re entering a competition?\\nThe most important thing is to have fun.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='Chapter 1 33\\nIt is still quite favorable if your team is relatively small (2, max 3 people) due to the advantage in \\nwits and computational power brought about by collaboration.\\nAnother point to keep in mind is that points decay with time. The decay is not linear, but as a \\nrule of thumb keep in mind that, after a year, very little is left of the points you gained. Therefore,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='glory on the general leaderboard of Kaggle is ephemeral unless you keep on participating in \\ncompetitions with similar results to before. As a consolation, on your profile you’ll always keep \\nthe highest rank you ever reach.\\nMore longer-lasting is  the medal system that covers all four aspects of competing in Kaggle. You'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='will be awarded medals for Competitions, Notebooks, Discussion, and Datasets based on your \\nresults. In Competitions, medals are awarded based on your position on the leaderboard. In the \\nother three areas, medals are awarded based on the upvotes of other competitors (which can \\nactually lead to some sub-optimal situations, since upvotes are a less objective metric and also'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='depend on popularity). The more medals you get, the higher the ranks of Kaggle mastery you \\ncan enter. The ranks are Novice, Contributor , Expert, Master, and Grandmaster. The page at \\nhttps://www.kaggle.com/progression  explains everything about how to get medals and how \\nmany and what kinds are needed to access the different ranks.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='Keep in mind that these ranks and honors are always relative and that they do change in time. A \\nfew years ago, in fact, the scoring system and the ranks were quite different. Most probably in \\nthe future, the ranks will change again in order to keep the higher ones rarer and more valuable.\\nCriticism and opportunities'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='Criticism and opportunities\\nKaggle has drawn quite a few criticisms since it began. Participation in data science competitions \\nis still a subject of debate today, with many different opinions out there, both positive and negative.\\nOn the side of  negative criticism:\\n• Kaggle provides a false perception of what machine learning really is since it is just focused \\non leaderboard dynamics'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='on leaderboard dynamics\\n• Kaggle is just a game of hyperparameter optimization and ensembling many models just \\nfor scraping a little more accuracy (while in reality overfitting the test set)\\n• Kaggle is filled with inexperienced enthusiasts who are ready to try anything under the \\nsun in order to get a score and a spotlight in hopes of being spotted by recruiters'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 59}, page_content='• As a further consequence, competition solutions are too complicated and often too specific \\nto a test set to be implemented'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='Introducing Kaggle and Other Data Science Competitions 34\\nMany perceive Kaggle, like many other data science competition platforms, to be far from what \\ndata science is in reality. The point the critics raise is that business problems do not come from \\nnowhere and you seldom already have a well-prepared dataset to start with, since you usually'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='build it along the way based on refining business specifications and the understanding of the \\nproblem at hand. Moreover, many critics emphasize that Kagglers don’t learn or excel at creating \\nproduction-ready models, since a winning solution cannot be constrained by resource limits or \\nconsiderations about technical debt (though this is not always true for all competitions).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='All such criticism is related, in the end, to how Kaggle standings can be compared to other kinds \\nof experience in the eyes of an employer, especially relative to data science education and work \\nexperience. One persistent myth is that Kaggle competitions won’t help to get you a job or a better \\njob in data science, and that they do not put you on another plane compared to data scientists'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='that do not participate at all.\\nOur stance on this is that it is a misleading belief that Kaggle rankings do not have an automatic \\nvalue beyond the Kaggle community. For instance, in a job search, Kaggle can provide you with \\nsome very useful competencies in modeling data and problems and effective model testing. It'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='can also expose you to many techniques and different data/business problems, beyond your \\nactual experience and comfort zone, but it cannot supplement you with everything you need to \\nsuccessfully place yourself as a data scientist in a company.\\nYou can use Kaggle for learning (there is also a section on the website, Courses, devoted to just'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='learning) and for differentiating yourself from other candidates in a job search; however, how this \\nwill be considered varies considerably from company to company. Regardless, what you learn on \\nKaggle will invariably prove useful throughout your career and will provide you a hedge when \\nyou have to solve complex and unusual problems with data modeling; by participating in Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='competitions, you build up strong competencies in modeling and validating. You also network \\nwith other data scientists, which can get you a reference for a job more easily and provide you \\nwith another way to handle difficult problems beyond your skills, because you will have access \\nto other people’s competencies and opinions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='Hence, our opinion is that Kaggle functions in a more indirect way to help you in your career as \\na data scientist, in a variety of different ways. Of course, sometimes Kaggle will help you to be \\ncontacted directly as a job candidate based on your successes, but more often Kaggle will provide \\nyou with the intellectual skills and experience you need to succeed, first as a candidate and then'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 60}, page_content='as a practitioner.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='Chapter 1 35\\nIn fact, after playing with data and models on Kaggle for a while, you’ll have had the chance to \\nsee enough different datasets, problems, and ways to deal with them under time pressure that \\nwhen faced with similar problems in real settings you’ll be skilled in finding solutions quickly \\nand effectively.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='and effectively.\\nThis latter opportunity for a skill upgrade is why we were motivated to write this book in the first \\nplace, and what this book is actually about. You won’t find a guide purely on how to win or score \\nhighly in Kaggle competitions, but you absolutely will find a guide about how to compete better \\non Kaggle and how to get the most back from your competition experiences.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='Use Kaggle and other competition platforms in a smart way. Kaggle is not a passepartout  – being \\nfirst in a competition won’t assure you a highly paid job or glory beyond the Kaggle community. \\nHowever, consistently participating in competitions is a card to be played smartly to show in -\\nterest and passion in your data science job search, and to improve some specific skills that can'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='differentiate you as a data scientist and not make you obsolete in front of AutoML solutions.\\nIf you follow us through this book, we will show you how.\\nSummary\\nIn this starting chapter, we first discussed how data science competition platforms have risen and \\nhow they actually work, both for competitors and for the institutions that run them, referring in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='particular to the convincing CTF paradigm as discussed by Professor David Donoho.\\nWe illustrated how Kaggle works, without forgetting to mention other notable competition plat -\\nforms and how it could be useful for you to take on challenges outside Kaggle as well. With regards \\nto Kaggle, we detailed how the different stages of a competition work, how competitions differ'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 61}, page_content='from each other, and what resources the Kaggle platform can offer you.\\nIn the next few chapters, we will begin to explore Kaggle in more detail, starting with how to \\nwork with Datasets.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 62}, page_content='Introducing Kaggle and Other Data Science Competitions 36\\nJoin our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors: \\nhttps://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 63}, page_content='2\\nOrganizing Data with Datasets\\nIn his story The Adventure of the Copper Beeches, Arthur Conan Doyle has Sherlock Holmes shout \\n“Data! Data! Data! I cannot make bricks without clay.” This mindset, which served the most famous \\ndetective in literature so well, should be adopted by every data scientist. For that reason, we begin'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 63}, page_content='the more technical part of this book with a chapter dedicated to data: specifically, in the Kaggle \\ncontext, leveraging the power of the Kaggle Datasets functionality for our purposes.\\nIn this chapter, we will cover the following topics:\\n• Setting up a dataset\\n• Gathering the data\\n• Working with datasets\\n• Using Kaggle Datasets in Google Colab\\n• Legal caveats\\nSetting up a dataset'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 63}, page_content='Setting up a dataset\\nIn principle, any data you can use you can upload to Kaggle (subject to limitations; see the Legal \\ncaveats section later on). The specific limits at the time of writing are 100 GB per private dataset \\nand a 100 GB total quota. Keep in mind that the size limit per single dataset is calculated un -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 63}, page_content='compressed; uploading compressed versions speeds up the transfer but does not help against \\nthe limits. You can check the most recent documentation for the datasets at this link: https://\\nwww.kaggle.com/docs/datasets .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 64}, page_content='Organizing Data with Datasets 38\\nKaggle promotes itself as a “home of data science” and the impressive collection of datasets avail -\\nable from the site certainly lends some credence to that claim. Not only can you find data on topics  \\nranging from oil prices to anime recommendations, but it is also impressive how quickly data'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 64}, page_content='ends up there. When the emails of Anthony Fauci  were released under the Freedom of Information \\nAct in May 2021 ( https://www.washingtonpost.com/politics/interactive/2021/tony-fauci-\\nemails/), they were uploaded as a Kaggle dataset a mere 48 hours later.\\nFigure 2.1: Trending and popular datasets on Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 64}, page_content='Before uploading the data for your project into a dataset, make sure to check the existing content. \\nFor several popular applications (image classification, NLP, financial time series), there is a chance \\nit has already been stored there.\\nFor the sake of this introduction, let us assume the kind of data you will be using in your project'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 64}, page_content='is not already there, so you need to create a new dataset. When you head to the menu with three \\nlines on the left-hand side and click on Data, you will be redirected to the Datasets page:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 65}, page_content='Chapter 2 39\\nFigure 2.2: The Datasets page\\nWhen you click on + New Dataset , you will be prompted for the basics: uploading the actual data \\nand giving it a title:\\nFigure 2.3: Entering dataset details'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 66}, page_content='Organizing Data with Datasets 40\\nThe icons on the left-hand side correspond to the different sources you can utilize for your dataset. \\nWe describe them in the order they are shown on the page:\\n• Upload a file from a local drive (shown in the figure)\\n• Create from a remote URL\\n• Import a GitHub repository \\n• Use output files from an existing Notebook\\n• Import a Google Cloud Storage file'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 66}, page_content='An important point about the GitHub option : This feature is particularly handy when it comes \\nto experimental libraries. While frequently offering hitherto unavailable functionality, they are \\nusually not included in the Kaggle environment, so if you want to use such a library in your code, \\nyou can import it as a dataset, as demonstrated below:\\n1. Go to Datasets and click + New Dataset.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 66}, page_content='2. Select the GitHub icon.\\n3. Insert the link to the repository, as well as the title for the dataset.\\n4. Click on Create at the bottom right:\\nFigure 2.4: Dataset from GitHub repository'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 67}, page_content='Chapter 2 41\\nNext to the Create button, there is another one marked Private. By default, any dataset you create \\nis private: only you, its creator, can view and edit it. It is probably a good idea to leave this setting \\nat default at the dataset creation stage and only at a later stage make it public (available to either \\na select list of contributors, or everyone).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 67}, page_content='Keep in mind that Kaggle is a popular platform and many people upload their datasets – including \\nprivate ones – so try to think of a non-generic title. This will increase the chance of your dataset \\nactually being noticed.\\nOnce you have completed all the steps and clicked Create, voilà! Your first dataset is ready. You \\ncan then head to the Data  tab:\\nFigure 2.5: The Data tab'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 67}, page_content='Figure 2.5: The Data tab\\nThe screenshot above demonstrates the different information you can provide about your data-\\nset; the more you do provide, the higher the usability index. This  index is a synthetic measure \\nsummarizing how well your dataset is described. Datasets with higher usability indexes appear'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 67}, page_content='higher up in the search results. For each dataset, the usability index is based on several factors, \\nincluding the level of documentation, the availability of related public content like Notebooks \\nas references, file types, and coverage of key metadata.\\nIn principle, you do not have to fill out all the fields shown in the image above; your newly created'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 67}, page_content='dataset is perfectly usable without them (and if it is a private one, you probably do not care; after all, \\nyou know what is in it). However, community etiquette would suggest filling out the information \\nfor the datasets you make public: the more you specify, the more usable the data will be to others.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 68}, page_content='Organizing Data with Datasets 42\\nGathering the data\\nApart from legal aspects, there is no real limit on the kind of content you can store in the datasets: \\ntabular data, images, text; if it fits within the size requirements, you can store it. This includes \\ndata harvested from other sources; tweets by hashtag or topic are among the popular datasets \\nat the time of writing:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 68}, page_content='at the time of writing:\\nFigure 2.6: Tweets are among the most popular datasets\\nDiscussion of the different frameworks for harvesting data from social media (Twitter, Reddit, \\nand so on) is outside the scope of this book.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 69}, page_content='Chapter 2 43\\nAndrew Maranhão\\nhttps://www.kaggle.com/andrewmvd\\nWe spoke to Andrew Maranhão (aka Larxel), Datasets Grandmaster \\n(number 1 in Datasets at time of writing) and Senior Data Scientist at the \\nHospital Albert Einstein in São Paulo, about his rise to Datasets success, \\nhis tips for creating datasets, and his general experiences on Kaggle.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 69}, page_content='What’s your favourite kind of competition and why? In terms of \\ntechniques and solving approaches, what is your specialty on Kaggle?\\nMedical imaging is usually my favourite. It speaks to my purpose and job. Among medical competitions, \\nNLP is language-bound, tabular data varies widely among hospitals, but imaging is mostly the same, so'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 69}, page_content='any advancement in this context can bring about benefits for many countries across the world, and I love \\nthis impact potential. I also have a liking for NLP and tabular data, but I suppose this is pretty standard.\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 69}, page_content='In a tuberculosis detection in x-ray images competition, we had around 1,000 images, which is a pretty \\nsmall number for capturing all the manifestations of the disease. I came up with two ideas to offset this:\\n1.  Pre-train on external data of pneumonia detection (~20k images), as pneumonia can be mis -\\ntaken for tuberculosis.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 69}, page_content='taken for tuberculosis.\\n2.  Pre-train on multilabel classification of lung abnormalities (~600k images) and use grad-CAM \\nwith a simple SSD to generate bounding box annotations of classification labels.\\nIn the end, a simple blend of these two achieved 22% more compared to the result that the second-place \\nteam had. It happened at a medical convention, with about 100 teams participating.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='Organizing Data with Datasets 44\\nYou have become a Dataset Grandmaster and achieved the number \\n1 rank in Datasets. How do you choose topics and find, gather, and \\npublish data for your datasets on Kaggle?\\nThis is a big question; I’ll try to break it down piece by piece.\\n1. Set yourself a purpose\\nThe first thing that I have in mind when choosing a topic is the reason I am doing this in the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='first place.\\nWhen there is a deeper reason underneath, great datasets just come off as a result, not as a goal \\nin itself. Fei Fei Li, the head of the lab that created ImageNet, revealed in a TED talk that she \\nwanted to create a world where machines would be able to reason and appreciate the world with \\ntheir vision in the same way her children did.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='Having a purpose in mind will make it more likely that you’ll engage and improve over time, and \\nwill also differentiate you and your datasets. You can certainly live off tabular data on everyday \\ntopics, though I find that unlikely to leave a lasting impact.\\n2. A great dataset is the embodiment of a great question'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='If we look at the greatest datasets in current literature, such as ImageNet and others, we can see \\nsome common themes:\\n• It is a daring, relevant question with great potential for all of us (scientific or real-world \\napplication)\\n• The data was well collected, controlled for quality, and well documented\\n• There is an adequate amount of data and diversity for our current hardware'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='• It has an active community that continuously improves the data and/or builds upon \\nthat question\\nAs I mentioned before, I feel that asking questions is a primary role of a data scientist and is likely \\nto become even more prominent as automated machine and deep learning solutions advance. \\nThis is where datasets can certainly exercise something unique to your skillset.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 70}, page_content='3. Create your process for success, rather than only pursuing success for the sake of success\\nQuality far overshadows quantity; you only need 15 datasets to become a Grandmaster and the \\nflagship datasets of AI are few and well made.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='Chapter 2 45\\nI have thrown away as many datasets as I have published. It takes time, and it is not a one and \\ndone type of thing as many people treat it – datasets have a maintenance and continuous im-\\nprovement side to them.\\nOne thing that is very often overlooked is supporting the community that gathers around your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='data. Notebooks and datasets are mutual efforts, so supporting those who take the time to ana -\\nlyze your data goes a long way for your dataset too. Analyzing their bottlenecks and choices can \\ngive directions as to what pre-processing steps could be done and provided, and also the clarity \\nof your documentation.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='of your documentation.\\nAll in all, the process that I recommend starts with setting your purpose, breaking it down into \\nobjectives and topics, formulating questions to fulfil these topics, surveying possible sources of data, \\nselecting and gathering, pre-processing, documenting, publishing, maintaining and supporting, \\nand finally, improvement actions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='For instance, let’s say that you would like to increase social welfare; you break it down into an \\nobjective, say, racial equity. From there, you analyze topics related to the objective and find the \\nBlack Lives Matter movement. From here, you formulate the question: how can I make sense of \\nthe millions of voices talking about it?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='This narrows down your data type to NLP, which you can gather data for from news articles, \\nYouTube comments, and tweets (which you choose, as it seems more representative of your ques -\\ntion and feasible). You pre-process the data, removing identifiers, and document the collection \\nprocess and dataset purpose.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='process and dataset purpose.\\nWith that done, you publish it, and a few Kagglers attempt topic modeling but struggle to do so \\nbecause some tweets contain many foreign languages that create encoding problems. You support \\nthem by giving them advice and highlighting their work, and decide to go back and narrow the \\ntweets down to English, to fix this for good.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 71}, page_content='Their analysis reveals the demands, motivations, and fears relating to the movement. With their \\nefforts, it was possible to break down millions of tweets into a set of recommendations that may \\nimprove racial equity in society.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='Organizing Data with Datasets 46\\n4. Doing a good job is all that is in your control\\nUltimately, it is other people that turn you into a Grandmaster, and votes don’t always translate \\ninto effort or impact. In one of my datasets, about Cyberpunk 2077, I worked on it for about 40 \\nhours total and, to this day, it is still one of my least upvoted datasets.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='But it doesn’t matter. I put in the effort, I tried, and I learned what I could — that’s what is in \\nmy control, and next week I’ll do it again no matter what. Do your best and keep going.\\nAre there any particular tools or libraries that you would recommend \\nusing for data analysis/machine learning?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='Strangely enough, I both recommend and unrecommend libraries. LightGBM is a great tabular ML library \\nwith a fantastic ratio of performance to compute time, CatBoost can sometimes outperform it, but it comes \\nat the cost of increased compute time, during which you could be having and testing new ideas. Optuna'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='is great for hyperparameter tuning, Streamlit for frontends, Gradio for MVPs, Fast API for microservices, \\nPlotly and Plotly Express for charts, PyTorch and its derivatives for deep learning.\\nWhile libraries are great, I also suggest that at some point in your career you take the time to implement'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='it yourself. I first heard this advice from Andrew Ng and then from many others of equal calibre. Doing \\nthis creates very in-depth knowledge that sheds new light on what your model does and how it responds \\nto tuning, data, noise, and more.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='started?\\nOver the years, the things I wished I realized sooner the most were:\\n1.  Absorbing all the knowledge at the end of a competition\\n2.  Replication of winning solutions in finished competitions\\nIn the pressure of a competition drawing to a close, you can see the leaderboard shaking more than ever'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='before. This makes it less likely that you will take risks and take the time to see things in all their detail. \\nWhen a competition is over, you don’t have that rush and can take as long as you need; you can also \\nreplicate the rationale of the winners who made their solutions known.\\nIf you have the discipline, this will do wonders for your data science skills, so the bottom line is: stop when'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 72}, page_content='you are done, not when the competition ends. I have also heard this advice from an Andrew Ng keynote, \\nwhere he recommended replicating papers as one of his best ways to develop yourself as an AI practitioner.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='Chapter 2 47\\nAlso, at the end of a competition , you are likely to be exhausted and just want to call it a day. No problem \\nthere; just keep in mind that the discussion forum after the competition is done is one of the most knowl -\\nedge-rich places on Planet Earth, primarily because many rationales and code for winning solutions are'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='made public there. Take the time to read and study what the winners did; don’t give into the desire to \\nmove on to something else, as you might miss a great learning opportunity.\\nHas Kaggle helped you in your career? If so, how?\\nKaggle helped my career by providing a wealth of knowledge, experience and also building my portfolio.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='My first job as a data scientist was largely due to Kaggle and DrivenData competitions. All throughout my \\ncareer, I studied competition solutions and participated in a few more. Further engagement on Datasets \\nand Notebooks also proved very fruitful in learning new techniques and asking better questions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='In my opinion, asking great questions is the primary challenge faced by a data scientist. Answering them \\nis surely great as well, although I believe we are not far from a future where automated solutions will \\nbe more and more prevalent in modeling. There will always be room for modeling, but I suppose a lot of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='work will be streamlined in that regard. Asking great questions, however, is far harder to automate – if \\nthe question is not good, even the best solution could be meaningless.\\nHave you ever used something you have done in Kaggle competitions \\nin order to build your portfolio to show to potential employers?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='Absolutely. I landed my first job as a data scientist in 2017 using Kaggle as proof of knowledge. To this \\nday, it is still a fantastic CV component, as educational backgrounds and degrees are less representative \\nof data science knowledge and experience than a portfolio is.\\nA portfolio with projects with competitions shows not just added experience but also a willingness to going'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='above and beyond for development, which is arguably more important for long-term success.\\nDo you use other competition platforms? How do they compare to \\nKaggle?\\nI also use DrivenData and AICrowd. The great thing about them is that they allow organizations that \\ndon’t have the same access to financial resources, such as start-ups and research institutions, to create \\ncompetitions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 73}, page_content='competitions.\\nGreat competitions come from a combination of great questions and great data, and this can happen \\nregardless of company size. Kaggle has a bigger and more active community, and the hardware they \\nprovide, coupled with the data and Notebook capabilities, make it the best option; yet both DrivenData \\nand AICrowd introduce just as interesting challenges and allow for more diversity.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 74}, page_content='Organizing Data with Datasets 48\\nWorking with datasets\\nOnce you have created a dataset, you probably want to use it in your analysis. In this section, we \\ndiscuss different methods of going about this.\\nVery likely, the most important one is starting a Notebook where you use your dataset as a pri -\\nmary source. You can do this by going to the dataset page and then clicking on New Notebook :'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 74}, page_content='Figure 2.7: Creating a Notebook from the dataset page\\nOnce you have done this, you will be redirected to your Notebook  page:\\nFigure 2.8: Starting a Notebook using your datasetWhat’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nAssuming your primary goal is development, my recommendation is that you pick a competition on a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 74}, page_content='topic that interests you and a task that you haven’t done before. Critical sense and competence require \\ndepth and diversity. Focusing and giving your best will guarantee depth, and diversity is achieved by \\ndoing things you have not done before or have not done in the same way.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 75}, page_content='Chapter 2 49\\nHere are a few pointers around this:\\n• The alphanumeric title is generated automatically; you can edit it by clicking on it.\\n• On the right-hand side under Data, you see the list of data sources attached to your Note -\\nbook; the dataset I selected can be accessed under ../input/  or from /kaggle/input/ .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 75}, page_content='• The opening block (with the imported packages, descriptive comments, and printing the \\nlist of available files) is added automatically to a new Python Notebook.\\nWith this basic setup, you can start to write a Notebook for your analysis and utilize your dataset \\nas a data source. We will discuss Notebooks at greater length in Chapter 4,  Leveraging Discussion \\nForums.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 75}, page_content='Forums.\\nUsing Kaggle Datasets in Google Colab\\nKaggle Notebooks are free to use, but not without limits (more on that in Chapter 4 ), and the \\nfirst one you are likely to hit is the time limit. A popular alternative is to move to Google Colab, \\na free Jupyter Notebook environment that runs entirely in the cloud: https://colab.research.\\ngoogle.com .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 75}, page_content='google.com .\\nEven once we’ve moved the computations there, we might still want to have access to the Kaggle \\ndatasets, so importing them into Colab is a rather handy feature. The remainder of this section \\ndiscusses the steps necessary to use Kaggle Datasets through Colab.\\nThe first thing we do, assuming we are already registered on Kaggle, is head to the account page'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 75}, page_content='to generate the API token  (an access token containing security credentials for a login session, \\nuser identification, privileges, and so on):\\n1. Go to your account, which can be found at https://www.kaggle.com/USERNAME/account , \\nand click on Create New API Token:\\nFigure 2.9: Creating a new API token\\nA file named kaggle.json containing your username and token will be created.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 76}, page_content=\"Organizing Data with Datasets 50\\n2. The next step is to create a folder named Kaggle in your Google Drive and upload the \\n.json file there:\\nFigure 2.10: Uploading the .json file into Google Drive\\n3. Once done, you need to create a new Colab notebook and mount your drive by running \\nthe following code in the notebook:\\nfrom google.colab import drive\\ndrive.mount( '/content/gdrive' )\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 76}, page_content='4. Get the authorization code from the URL prompt and provide it in the empty box that \\nappears, and then execute the following code to provide the path to the .json config:\\nimport os\\n# content/gdrive/My Drive/Kaggle is the path where kaggle.json is \\n# present in the Google Drive\\nos.environ[ \\'KAGGLE_CONFIG_DIR\\' ] = \"/content/gdrive/My Drive/Kaggle\"\\n# change the working directory'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 76}, page_content='%cd /content/gdrive/My Drive/Kaggle\\n# check the present working directory using the pwd command\\n5. We can download the dataset now. Begin by going to the dataset’s page on Kaggle, clicking \\non the three dots next to New Notebook , and selecting Copy API command :'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 77}, page_content='Chapter 2 51\\nFigure 2.11: Copying the API command\\n6. Run the API command to download the Dataset (readers interested in details of the com-\\nmands used can consult the official documentation: https://www.kaggle.com/docs/api ) :\\n!kaggle datasets download -d ajaypalsinghlo/world-happiness-\\nreport-2021\\n7. The dataset will be downloaded to the Kaggle folder as a .zip archive – unpack it and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 77}, page_content='you are good to go.\\nAs you can see from the list above, using a Kaggle dataset in Colab is a straightforward process – \\nall you need is an API token, and making the switch gives you the possibility of using more GPU \\nhours than what is granted by Kaggle.\\nLegal caveats\\nJust because you can put some data on Kaggle does not necessarily mean that you should. An'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 77}, page_content='excellent example would be the People of Tinder  dataset. In 2017, a developer used the Tinder \\nAPI to scrape the website for semi-private profiles and uploaded the data on Kaggle. After the \\nissue became known, Kaggle ended up taking the dataset down. You can read the full story here: \\nhttps://www.forbes.com/sites/janetwburns/2017/05/02/tinder-profiles-have-been-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 77}, page_content='looted-again-this-time-for-teaching-ai-to-genderize-faces/?sh=1afb86b25454 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 78}, page_content='Organizing Data with Datasets 52\\nIn general, before you upload anything to Kaggle, ask yourself two questions:\\n1. Is it allowed from a copyright standpoint?  Remember to always check the licenses. When \\nin doubt, you can always consult https://opendefinition.org/guide/data/  or contact \\nKaggle.\\n2. Are there privacy risks associated with this dataset? Just because posting certain types'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 78}, page_content='of information is not, strictly speaking, illegal, doing so might be harmful to another \\nperson’s privacy. \\nThe limitations speak to common sense, so they are not too likely to hamper your efforts on Kaggle.\\nSummary\\nIn this chapter, we introduced Kaggle Datasets, the standardized manner of storing and using'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 78}, page_content='data in the platform. We discussed dataset creation, ways of working outside of Kaggle, and the \\nmost important functionality: using a dataset in your Notebook. This provides a good segue to \\nour next chapter, where we focus our attention on Kaggle Notebooks.\\nJoin our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 78}, page_content='https://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 79}, page_content='3\\nWorking and Learning with \\nKaggle Notebooks\\nKaggle Notebooks – which until recently were called Kernels – are Jupyter Notebooks in the \\nbrowser that can run free of charge. This means you can execute your experiments from any \\ndevice with an internet connection, although something bigger than a mobile phone is proba-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 79}, page_content='bly a good idea. The technical specification of the environment (as of the time of this writing) is \\nquoted below from the Kaggle website; the most recent version can be verified at https://www.\\nkaggle.com/docs/notebooks :\\n• 12 hours execution time for CPU/GPU, 9 hours for TPU\\n• 20 gigabytes of auto-saved disk space ( /kaggle/working )'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 79}, page_content='• Additional scratchpad disk space (outside /kaggle/working ) that will not be saved out -\\nside of the current session\\nCPU specifications:\\n• 4 CPU cores\\n• 16 gigabytes of RAM\\nGPU specifications:\\n• 2 CPU cores\\n• 13 gigabytes of RAM\\nTPU specifications:\\n• 4 CPU cores\\n• 16 gigabytes of RAM'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 80}, page_content='Working and Learning with Kaggle Notebooks 54\\nIn this chapter, we will cover the following topics:\\n• Setting up a Notebook\\n• Running your Notebook\\n• Saving Notebooks to GitHub\\n• Getting the most out of Notebooks\\n• Kaggle Learn courses\\nWithout further ado, let us jump into it. The first thing we need to do is figure out how to set up \\na Notebook.\\nSetting up a Notebook'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 80}, page_content='Setting up a Notebook\\nThere are two primary methods of creating a Notebook: from the front page or from a Dataset.\\nTo proceed with the first method, go to the Code section of the menu on the left-hand side of the  \\nlanding page at https://www.kaggle.com/  and click the + New Notebook  button. This is the \\npreferred method if you are planning an experiment that involves uploading your own dataset:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 80}, page_content='Figure 3.1: Creating a new Notebook from the Code page'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 81}, page_content='Chapter 3 55\\nAlternatively, you can go to the page of the Dataset you are interested in and click the  New Note -\\nbook button there, as we saw in the previous chapter:\\nFigure 3.2: Creating a new Notebook from a Dataset page\\nWhichever method you choose, after clicking New Notebook , you will be taken to your Notebook \\npage:\\nFigure 3.3: The Notebook page'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 82}, page_content='Working and Learning with Kaggle Notebooks 56\\nOn the right-hand side of the new Notebook  page shown above, we have a number of settings \\nthat can be adjusted:\\nFigure 3.4: Notebook options\\nWe will discuss the settings briefly. First, there is the coding Language. As of the time of this \\nwriting, the Kaggle environment only allows Python and R as available options for coding your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 82}, page_content='Notebooks. By default, a new Notebook is initialized with the language set to Python – if you \\nwant to use R instead, click on the dropdown and select R.\\nNext comes Environment : this toggle allows you to decide whether to always use the latest \\nDocker environment (the risky option; fast to get updates but dependencies might break with'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 82}, page_content='future updates) or pin the Notebook to the original version of the environment provided by Kaggle \\n(the safe choice). The latter option is the default one, and unless you are conducting very active \\ndevelopment work, there is no real reason to tinker with it.\\nAccelerator  allows a user to choose how to run the code: on CPU (no acceleration), GPU (nec-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 82}, page_content='essary for pretty much any serious application involving deep learning), or TPU. Keep in mind \\nthat moving from CPU to (a single) GPU requires only minimal changes to the code and can be \\nhandled via system device detection.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 83}, page_content='Chapter 3 57\\nMigrating your code to TPU requires more elaborate rewriting, starting with data processing. An \\nimportant point to keep in mind is that you can switch between CPU/GPU/TPU when you are \\nworking on your Notebook, but each time you do, the environment is restarted and you will need \\nto run all your code from the beginning.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 83}, page_content='Finally, we have the Internet toggle which enables or disables online access. If you are connected \\nand need to, for example, install an extra package, the download and installation of dependencies \\nwill take place automatically in the background. The most common situation in which you need \\nto explicitly disable internet access is for submission to a competition that explicitly prohibits'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 83}, page_content='online access at submission time.\\nAn important aspect of using Notebooks is that you can always take an existing one (created by \\nyourself or another Kaggler) and clone it to modify and adjust to your needs. This can be achieved \\nby clicking the Copy and Edit button at the top right of the Notebook page. In Kaggle parlance, \\nthe process is referred to as forking:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 83}, page_content='Figure 3.5: Forking an existing Notebook\\nA Notebook you create is private (only visible to you) by default. If you want to make it available \\nto others, you can choose between adding collaborators, so that only the users explicitly added \\nto the list will be able to view or edit the content, or making the Notebook public, in which case'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 83}, page_content='everybody can see it.A note on etiquette: If you have participated in a Kaggle competition before, you \\nwill probably have noticed that the leaderboard is flooded with forks of forks of \\nwell-scoring Notebooks. There is nothing wrong with building on somebody else’s \\nwork – but if you do, remember to upvote the original author and give explicit credit \\nto the creator of the reference work.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 84}, page_content='Working and Learning with Kaggle Notebooks 58\\nRunning your Notebook\\nAll the coding is finished, the Notebook seems to be working fine, and you are ready to execute. \\nTo do that, go to the upper-right corner of your Notebook page and click Save Version.\\nFigure 3.6: Saving your script\\nSave & Run All  is usually used to execute the script, but there is also a Quick Save option, which'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 84}, page_content='can be used to save an intermediate version of the script before it is ready for submission:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 85}, page_content='Chapter 3 59\\nFigure 3.7: Different options for Save Version\\nOnce you have launched your script(s), you can head to the lower-left corner and click on Active \\nEvents:\\nFigure 3.8: Monitoring active events'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 86}, page_content='Working and Learning with Kaggle Notebooks 60\\nIn this manner, you can monitor the behavior of your Notebooks. Normal execution is associated \\nwith the message Running; otherwise, it is displayed as Failed. Should you decide that you want \\nto kill a running session for whatever reason (for instance, you realize that you forgot to use the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 86}, page_content='most recent data), you can do it by clicking on the three dots on the right-hand side of your script \\nentry under Active Event s and you will receive a pop-up like the one shown in the figure below:\\nFigure 3.9: Canceling Notebook execution\\nSaving Notebooks to GitHub\\nA recently introduced feature (see https://www.kaggle.com/product-feedback/295170 ) allows'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 86}, page_content='you to store your code or your Notebook to the version control repository GitHub ( https://github.\\ncom/). You can store your work both to public and private repositories, and this will happen au-\\ntomatically as you save a version of your code. Such a feature could prove quite useful for sharing \\nyour work with your Kaggle teammates, as well as for showcasing your work to the wider public.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 86}, page_content='In order to enable this feature, you need to open your Notebook; in the File menu, choose the \\nLink to GitHub option.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 87}, page_content='Chapter 3 61\\nFigure 3.10: Enabling the GitHub feature\\nAfter choosing the option, you will have to link your GitHub account to the Notebook. You will \\nexplicitly be asked for linking permissions the first time you choose to link. For any subsequent \\nlinks to new Notebooks, the operation will be carried out automatically.\\nFigure 3.11: Linking to GitHub'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 88}, page_content='Working and Learning with Kaggle Notebooks 62\\nOnly after linking your Notebook will you be allowed to sync your work to a repository of your \\nchoice when you save it:\\nFigure 3.12: Committing your work to GitHub\\nAfter deciding on a repository and a branch (thus allowing you to store different development'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 88}, page_content='stages of your work), you can change the name of the file you are going to push to the repository \\nand modify the commit message.\\nIf you decide you no longer want to sync a particular Notebook on GitHub, all you have to do is \\nto go back to the File menu and select Unlink from GitHub . Finally, if you want Kaggle to stop'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 88}, page_content='connecting with your GitHub repository, you can unlink your accounts from either your Kaggle \\naccount page under My linked accounts or from GitHub’s settings pages ( https://github.com/\\nsettings/applications ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 89}, page_content='Chapter 3 63\\nGetting the most out of Notebooks\\nKaggle provides a certain amount of resources for free, with the quotas resetting weekly. You get \\na certain number of hours to use with both GPU and TPU; it is 30 hours for TPU, but for GPU the \\nnumbers can vary from week to week (you can find the official statement describing the “float -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 89}, page_content='ing” quotas policy here: https://www.kaggle.com/product-feedback/173129 ). You can always \\nmonitor your usage in your own profile:\\nFigure 3.13: Current status for accelerator quotas\\nWhile the amounts might seem large at first glance, this initial impression can be deceptive; it is \\nactually fairly easy to use your quota very quickly. Some practical suggestions that can help you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 89}, page_content='control the usage of the resources:\\n• The counter for the quota (measuring how long you have been using your chosen accel-\\nerator, GPU or TPU) starts running the moment you initialize  your Notebook.\\n• This means that you should always start by checking that GPU is disabled under settings \\n(see Figure 3.6 above). Write the boilerplate first, check your syntax, and enable/disable'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 89}, page_content='GPU for when you add the parts of the code that actually depend on GPU initialization. A \\nreminder: the Notebook will restart when you change the accelerator.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 90}, page_content='Working and Learning with Kaggle Notebooks 64\\n• It is usually a good idea to run the code end-to-end on a small subset of data to get a feel \\nfor the execution time. This way, you minimize the risk that your code will crash due to \\nexceeding this limit.\\nSometimes the resources provided freely by Kaggle are not sufficient for the task at hand, and you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 90}, page_content='need to move to a beefier machine. A good example is a recent tumor classification competition: \\nhttps://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/data .\\nIf your raw data is over 100GB, you need to either resize/downsample your images (which is \\nlikely to have an adverse impact on your model performance) or train a model in an environment'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 90}, page_content='capable of handling high-resolution images. You can set up the whole environment yourself (an \\nexample of this setup is the section Using Kaggle Datasets in Google Colab in Chapter 2 ), or you can \\nstay within the framework of Notebooks but swap the underlying machine. This is where Google \\nCloud AI Notebooks come in.\\nUpgrading to Google Cloud Platform (GCP)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 90}, page_content='The obvious benefit to upgrading to GCP is getting access to more powerful hardware: a Tesla \\nP100 GPU (provided free by Kaggle) is decent for many applications, but not top of the line in \\nterms of performance, and 16GB RAM can also be quite limiting, especially in resource-intensive \\napplications like large NLP models or high-resolution image processing. While the improvement in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 90}, page_content='execution time is obvious, leading to faster iteration through the development cycle, it comes at a \\ncost: you need to decide how much you are prepared to spend. For a powerful machine crunching \\nthe numbers, time is quite literally money.\\nIn order to migrate your Notebook to the GCP environment, go to the sideline menu on the right-\\nhand side and click on Upgrade to Google Cloud AI Notebooks:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 91}, page_content='Chapter 3 65\\nFigure 3.14: Upgrading to the Google Cloud AI Notebooks option'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 92}, page_content='Working and Learning with Kaggle Notebooks 66\\nYou will be  greeted by the following prompt:\\nFigure 3.15: Upgrade to Google Cloud AI Platform Notebooks prompt\\nWhen you click Continue , you will be redirected to the Google Cloud Platform console, where \\nyou need to configure your billing options. A reminder: GCP is not free. If it is your first time, you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 92}, page_content='will need to complete a tutorial guiding you through the necessary steps.\\nOne step beyond\\nAs mentioned earlier in this chapter, Kaggle Notebooks  are a fantastic tool for education and \\nparticipating in competitions; but they also serve another extremely useful purpose, namely as \\na component of a portfolio you can use to demonstrate your data science skills.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 92}, page_content='There are many potential criteria to consider when building your data science portfolio (branding, \\naudience reach, enabling a pitch to your potential employer, and so on) but none of them matter \\nif nobody can find it. Because Kaggle is part of Google, the Notebooks are indexed by the most \\npopular search engine in the world; so if someone is looking for a topic related to your code, it'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 92}, page_content='will show up in their search results.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 93}, page_content='Chapter 3 67\\nBelow, I show a personal example: a few years ago, I wrote a Notebook for a competition. The \\nproblem I wanted to tackle was adversarial validation (for those unfamiliar with the topic: a \\nfairly easy way to see if your training and test sets have a similar distribution is to build a binary'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 93}, page_content='classifier trained to tell them apart; the concept is covered in more detail in Chapter 6, Designing \\nGood Validation). When writing this chapter, I tried to search for the Notebook and, lo and behold, \\nit shows up high up in the search results (notice the fact that I did not mention Kaggle or any \\npersonal details like my name in my query):\\nFigure 3.16: Konrad’s Notebook showing up on Google'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 93}, page_content='Moving on to other benefits of using Notebooks to demonstrate your skillset: just like Compe -\\ntitions, Datasets, and Discussions, Notebooks can be awarded votes/medals and thus position \\nyou in the progression system and ranking. You can stay away from the competitions track and \\nbecome an Expert, Master, or Grandmaster purely by focusing on high-quality code the com -\\nmunity appreciates.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 94}, page_content='Working and Learning with Kaggle Notebooks 68\\nThe most up-to-date version of the progression requirements can be found at https://www.\\nkaggle.com/progression ; below we give a snapshot relevant to the Expert and Master tiers:\\nFigure 3.17: Tier progression requirements\\nProgressing in the  Notebooks category can be a challenging experience; while easier than Com-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 94}, page_content='petitions, it is definitely harder than Discussions. The most popular Notebooks are those linked \\nto a specific competition: exploratory data analysis, end-to-end proof of concept solutions, as \\nwell as leaderboard chasing; it is an unfortunately common practice that people clone the high-\\nest-scoring public Notebook, tweak some parameters to boost the score, and release it to wide'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 94}, page_content='acclaim (if upvotes can be considered a measure of sentiment). This is not meant to discourage the \\nreader from publishing quality work on Kaggle – a majority of Kagglers do appreciate novel work \\nand quality does prevail in the long term – but a realistic adjustment of expectations is in order.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 95}, page_content='Chapter 3 69\\nYour Kaggle profile comes with followers and gives you the possibility of linking other profes -\\nsional networks like LinkedIn or GitHub, so you can leverage the connections you gain inside \\nthe community:\\nFigure 3.18: Konrad’s Kaggle profile\\nIn this day and age, it is easy to be skeptical about claims of “community building”, but in the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 95}, page_content='case of Kaggle, it happens to actually be true. Their brand recognition in the data science universe \\nis second to none, both among practitioners and among recruiters who actually do their home -\\nwork. In practice, this means that a (decent enough) Kaggle profile can get you through the door \\nalready; which, as we all know, is frequently the hardest step.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='Working and Learning with Kaggle Notebooks 70\\nMartin Henze\\nhttps://www.kaggle.com/headsortails\\nWe had the  pleasure of speaking to Martin Henze, aka Heads or Tails, a \\nKaggle Grandmaster in Notebooks and Discussion and a Data Scientist \\nat Edison Software. Martin is also the author of Notebooks of the Week: \\nHidden Gems, a weekly collection of the very best Notebooks that have'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='escaped public notice. You can get notifications about new Hidden Gems posts by following his \\nKaggle profile or his accounts on Twitter and LinkedIn.\\nWhat’s your favourite kind of competition and why? In terms of \\ntechniques, solving approaches, what is your specialty on Kaggle?\\nFor a long time, my focus was on EDA (exploratory data analysis) notebooks rather than leaderboard'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='predictions themselves. Most of my experience prior to Kaggle had been with tabular data, and the major -\\nity of my EDA notebooks deal with extracting intricate insights from newly launched tabular challenges. \\nI still consider this my specialty on Kaggle, and I have spent a significant amount of time crafting the \\nstructure, data visualizations, and storytelling of my notebooks.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='How do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?\\nEven as Kaggle has shifted away from tabular competitions, I strongly believe that the data themselves \\nare the most important aspect of any challenge. It is easy to focus too early on model architectures and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='hyperparameter tuning. But in many competitions, the key to success remains a data-centric approach \\nthat is built on detailed knowledge of the dataset and its quirks and peculiarities. This is true for image \\ndata, NLP, time series, and any other data structures you can think of. Therefore, I always start with an'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='extensive EDA before building a simple baseline model, a CV framework, and then slowly iterating the \\ncomplexity of this pipeline. \\nThe main difference compared to my data science day job is probably that the kind of baseline models that \\nmost experienced people can build within the first week of a new challenge would be considered sufficient'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 96}, page_content='to put into production. In many cases, after those first few days we’re more than 80% on the way to the \\nultimate winner’s solution, in terms of scoring metric. Of course, the fun and the challenge of Kaggle are \\nto find creative ways to get those last few percent of, say, accuracy. But in an industry job, your time is \\noften more efficiently spent in tackling a new project instead.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='Chapter 3 71\\nHas Kaggle helped you in your career? If so, how?\\nKaggle has shaped and supported my career tremendously. The great experience in the Kaggle community \\nmotivated me to transition from academia to industry. Today, I’m working as a data scientist in a tech \\nstartup and I’m continuously growing and honing my skills through Kaggle challenges.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='In my case, my focus on constructing extensive Kaggle Notebooks helped me a lot, since I could easily use \\nthose as my portfolio. I don’t know how often a hiring manager would actually look at those resources, but \\nI frequently got the impression that my Grandmaster title might have opened more doors than my PhD'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='did. Or maybe it was a combination of the two. In any case, I can much recommend having a portfolio of \\npublic Notebooks. Moreover, during my job search, I used the strategies I learned on Kaggle for various \\ntake-home assignments and they served me well.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='started?\\nI think that we are all constantly growing in experience. And we’re all wiser now than we were ten years, \\nfive years, or even one year ago. With that out of the way, one crucial aspect that is often overlooked is \\nthat you want to have a plan for what you’re doing, and to execute and document that plan. And that’s'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='an entirely understandable mistake to make for new Kagglers, since everything is novel and complex and \\nat least somewhat confusing. I know that Kaggle was confusing for me when I first joined. So many things \\nyou can do: forums, datasets, challenges, courses. And the competitions can be downright intimidating:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='Neuronal Cell Instance Segmentation; Stock Market Volatility Prediction. What even are those \\nthings? But the competitions are also the best place to start.\\nBecause when a competition launches, nobody really has a clue about it. Yeah, maybe there is a person \\nwho has done their PhD on almost the same topic. But those are rare. Everyone else, we’re all pretty much'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='starting from zero. Digging into the data, playing with loss functions, running some simple starter models. \\nWhen you join a competition at the beginning, you go through all that learning curve in an accelerated \\nway, as a member of a community. And you learn alongside others who will provide you with tons of \\nideas. But you still need a plan.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='And that plan is important, because it’s easy to just blindly run some experiments and see all that GPU \\nRAM being used and feel good about it. But then you forget which version of your model was doing best, \\nand is there a correlation between local validation and leaderboard? Did I already test this combination'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 97}, page_content='of parameters? So write down what you are going to do and then log the results. There are more and more \\ntools that do the logging for you, but this is also easily done through a custom script.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='Working and Learning with Kaggle Notebooks 72\\nMachine learning is still mostly an experimental science, and the key to efficient experiments is to plan \\nthem well and to write down all of the results so you can compare and analyse them.\\nWhat mistakes have you made in competitions in the past?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='I have made lots of mistakes and I hope that I managed to learn from them. Not having a robust cross-vali -\\ndation framework was one of them. Not accounting for differences between train and test. Doing too much \\nEDA and neglecting the model building – that one was probably my signature mistake in my first few'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='competitions. Doing not enough EDA and missing something important – yep, done that too. Not selecting \\nmy final two submissions. (Ended up making not much of a difference, but I still won’t forget it again.)\\nThe point about mistakes, though, is similar to my earlier point about experiments and having a plan.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='Mistakes are fine if you learn from them and if they help you grow and evolve. You still want to avoid \\nmaking easy mistakes that could be avoided by foresight. But in machine learning (and science!) failure \\nis pretty much part of the process. Not everything will always work. And that’s fine. But you don’t want'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='to keep making the same mistakes over and over again. So the only real mistake is not to learn from your \\nmistakes. This is true for Kaggle competitions and in life.\\nAre there any particular tools or libraries that you would recommend \\nusing for data analysis or machine learning?\\nI know that we increasingly live in a Python world, but when it comes to tabular wrangling and data'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='visualization I still prefer R and its tidyverse: dplyr, ggplot2, lubridate, etc. The new tidymodels framework \\nis a serious contender to sklearn. Even if you’re a die-hard Python aficionado, it pays off to have a look \\nbeyond pandas and friends every once in a while. Different tools often lead to different viewpoints and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='more creativity. In terms of deep learning, I find PyTorch most intuitive alongside its FastAI interface. \\nAnd, of course, everyone loves huggingface nowadays; and for very good reasons.\\nWhat’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nThe most important thing is to remember to have fun and to learn something. So much valuable insight'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='and wisdom is being shared both during and after a competition that it would be a shame not to take \\nit in and grow from it. Even if the only thing you care for is winning, you can only accomplish that by \\nlearning and experimenting and standing on the shoulders of this community. But there is so much more'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 98}, page_content='to Kaggle than the leaderboards, and once you start contributing and giving back to the community you \\nwill grow in a much more holistic way. I guarantee it.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='Chapter 3 73\\nKaggle Learn courses\\nA great many things about Kaggle are about acquiring knowledge. Whether it be the things you \\nlearn in a competition, datasets you manage to find in the ever-growing repository, or demonstra -\\ntion of a hitherto unknown model class, there is always something new to find out. The newest'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='addition to that collection is the courses gathered under the Kaggle Learn  label: https://www.\\nkaggle.com/learn . These are micro-courses marketed by Kaggle as “the single fastest way to gain \\nthe skills you’ll need to do independent data science projects,” the core unifying theme being a \\ncrash course introduction across a variety of topics. Each course is divided into small chapters,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='followed by coding practice questions. The courses are delivered using Notebooks, where portions \\nof the necessary theory and exposition are intermingled with the bits you are expected to code \\nand implement yourself.\\nBelow, we provide a short overview of the most useful ones:\\n• Intro to ML/Intermediate ML : https://www.kaggle.com/learn/intro-to-machine-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='learning  and https://www.kaggle.com/learn/intermediate-machine-learning\\nThese two courses are best viewed as a two-parter: the first one introduces different class -\\nes of models used in machine learning, followed by a discussion of topics common to \\ndifferent models like under/overfitting or model validation. The second one goes deeper'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='into feature engineering, dealing with missing values and handling categorical variables. \\nUseful for people beginning their ML journey .\\n• pandas: https://www.kaggle.com/learn/pandas\\nThis course provides a crash-course introduction to one of the most fundamental tools \\nused in modern data science. You first learn how to create, read, and write data, and then'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='move on to data cleaning (indexing, selecting, combining, grouping, and so on). Useful \\nfor both beginners (pandas functionality can be overwhelming at times) and practitioners (as \\na refresher/reference) alike.\\n• Game AI: https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-\\nlearning\\nThis course is a great wrap-up of the tech-focused part of the curriculum introduced'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 99}, page_content='by Kaggle in the learning modules. You will write a game-playing agent, tinker with its \\nperformance, and use the minimax algorithm. This one is probably best viewed as a prac -\\ntice-oriented introduction to reinforcement learning .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='Working and Learning with Kaggle Notebooks 74\\n• Machine Learning Explainability: https://www.kaggle.com/learn/machine-learning-\\nexplainability\\nBuilding models is fun, but in the real world not everybody is a data scientist, so you might \\nfind yourself in a position where you need to explain what you have done to others. This'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='is where this mini-course on model explainability comes in: you will learn to assess how \\nrelevant your features are with three different methods: permutation importance, SHAP, \\nand partial dependence plots. Extremely useful to anybody working with ML in a commercial \\nsetting, where projects live or die on how well the message is conveyed .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='• AI Ethics: https://www.kaggle.com/learn/intro-to-ai-ethics\\nThis last course is a very interesting addition to the proposition: it discusses the practical \\ntools to guide the moral design of AI systems. You will learn how to identify the bias in \\nAI models, examine the concept of AI fairness, and find out how to increase transparency'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='by communicating ML model information. Very useful for practitioners, as “responsible AI” \\nis a phrase we will be hearing more and more of .\\nApart from the original content created by Kaggle, there are other learning opportunities avail -\\nable on the platform through user-created Notebooks; the reader is encouraged to explore them \\non their own.\\nAndrada Olteanu'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='on their own.\\nAndrada Olteanu\\nhttps://www.kaggle.com/andradaolteanu\\nAndrada Olteanu is one Kaggle Notebooks Grandmaster who very much \\nencourages learning from Notebooks. Andrada is a Z by HP Global \\nData Science Ambassador, Data Scientist at Endava, and Dev Expert \\nat Weights & Biases. We caught up with Andrada about Notebook com -\\npetitions, her career, and more.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='What’s your favourite kind of competition and why? In terms of \\ntechniques and solving approaches, what is your specialty on Kaggle?\\nI would say my specialty on Kaggle leans more towards Data Visualization, as it enables me to combine \\nart and creativity with data.\\nI would not say I have a favorite type of competition, but I would rather say I like to switch it up occa -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 100}, page_content='sionally and choose whatever I feel is interesting.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='Chapter 3 75\\nThe beauty of Kaggle is that one can learn multiple areas of Data Science (computer vision, NLP, explor -\\natory data analysis and statistics, time series, and so on) while also becoming familiar and comfortable \\nwith many topics (like sports, the medical field, finance and cryptocurrencies, worldwide events, etc.)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='Another great thing is that, for example, if one wants to become more proficient in working with text \\ndata, there is almost always a Kaggle Competition that requires NLP. Or, if one wants to learn how to \\npreprocess and model audio files, there are competitions that enable that skill as well.\\nTell us about a particularly challenging competition you entered, and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='what insights you used to tackle the task.\\nThe most challenging “competition” I have ever entered was the “Kaggle Data Science and Machine \\nLearning Annual Survey”. I know this is not a “real” competition – with a leaderboard and heavy-duty \\nmachine learning involved – however for me it was one of the competitions I have “sweated” during and \\nlearned the most.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='learned the most.\\nThis is a Notebook competition, where the users have to become creative in order to win one of the 5 prizes \\nKaggle puts on the table. I have participated in it 2 years in a row. In the first year (2020), it challenged \\nmy more “basic” visualization skills and forced me to think outside the box (I took 3rd place); in the sec -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='ond year (2021), I prepared for it for around 4 months by learning D3, in an attempt to get to a whole \\nother level on my Data Visualization skills (still in review; so far, I have won the “Early Notebook Award” \\nprize). The best insights I can give here are:\\n• First, do not get lost within the data and try to create graphs that are as accurate as possible; if'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='necessary, build double verification methods to be sure that what you are representing is clear \\nand concise. Nothing is worse than a beautiful graph that showcases inaccurate insights.\\n• Try to find inspiration around you: from nature, from movies, from your work. You can draw on \\namazing themes and interesting ways to spruce up your visualization.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='Has Kaggle helped you in your career? If so, how?\\nYes. Tremendously. I believe I owe a big part of where I am now in my career to Kaggle, and for this I am \\nforever grateful. Through Kaggle I have became a Z by HP Ambassador; I have also discovered Weights \\n& Biases, which is an amazing machine learning experiment platform and now I am a proud Dev Ex -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 101}, page_content='pert for them. Last but not least, through this platform I connected with my now Lead Data Scientist at \\nEndava, who recruited me, and I have been working with him since. In short, my position at Endava \\nand the connection I have with 2 huge companies (HP and Weights & Biases) are a direct result of my \\nactivity on the Kaggle platform.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='Working and Learning with Kaggle Notebooks 76\\nI believe the most overlooked aspect of Kaggle is the community. Kaggle has the biggest pool of people, all \\ngathered in one convenient place, from which one could connect, interact, and learn from.\\nThe best way to leverage this is to take, for example, the first 100 people from each Kaggle section (Com-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='petitions, Datasets, Notebooks – and if you want, Discussions), and follow on Twitter/LinkedIn everybody \\nthat has this information shared on their profile. This way, you can start interacting on a regular basis \\nwith these amazing people, who are so rich in insights and knowledge.\\nWhat mistakes have you made in competitions in the past?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='The biggest mistake I have made in competitions in the past is to not participate in them. I believe this is \\nthe biggest, most fundamental mistake beginners make when they enter onto the platform.\\nOut of fear (and I am talking from personal experience), they believe they are not ready, or they just'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='don’t know how to start. Fortunately, if you follow a simple system, it will become very easy to enter any \\ncompetition:\\n• Enter any competition you like or sounds interesting.\\n• Explore the description page and the data.\\n• If you have no idea how to start, no worries! Just enter the “Code” section and look around for'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='Notebooks that have a lot of upvotes, or are made by experienced people, like Grandmasters. \\nStart doing a “code along” Notebook, where you look at what others have done and “copy” it, \\nresearching and trying to improve it yourself. This is, in my opinion, the best way to learn – you \\nnever get stuck, and you learn by doing in a specific project.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='What’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nThey should keep in mind that it is OK to fail, as usually it is the best way to learn.\\nWhat they should also keep in mind is to always learn from the Competition Grandmasters, because \\nthey are usually the ones who share and explain machine learning techniques that one may never think'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='of. The best way of learning something is to look at others that “have already made it,” so your road to \\nsuccess will not be as bumpy, but rather much more painless, smooth, and quick. Take 2-3 Grandmasters \\nthat you really admire and make them your teachers; study their Notebooks, code along, and learn as \\nmuch as possible.\\nDo you use other competition platforms? How do they compare to \\nKaggle?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 102}, page_content='Kaggle?\\nI have never used any other competition platform – simply because I feel like Kaggle has it all.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 103}, page_content='Chapter 3 77\\nSummary\\nIn this chapter, we have discussed Kaggle Notebooks, multi-purpose, open coding environments \\nthat can be used for education and experimentation, as well as for promoting your data science \\nproject portfolio. You are now in a position to create your own Notebook, efficiently utilize the \\navailable resources, and use the results for competitions or your individual projects.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 103}, page_content='In the next chapter, we will introduce discussion forums, the primary form of exchanging ideas \\nand opinions on Kaggle.\\nJoin our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors: \\nhttps://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 105}, page_content='4\\nLeveraging Discussion Forums\\nDiscussion forums are the primary means of information exchange on Kaggle. Whether it’s dis-\\ncussing an ongoing competition, engaging in a conversation about a Dataset, or a Notebook \\npresenting a novel approach, Kagglers talk about things all the time.\\nIn this chapter, we present the discussion forums: how they are organized, and the code of con-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 105}, page_content='duct governing the wealth of information therein that can be used. We cover the following topics:\\n• How forums work\\n• Discussion approaches for example competitions\\n• Netiquette\\nHow forums work\\nYou can enter the discussion forum in several ways. The most direct way is by clicking on Dis-\\ncussions in the left-hand side panel:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 106}, page_content='Leveraging Discussion Forums 80\\nFigure 4.1: Entering the Discussions page from the main menu\\nThe top section contains Forums, which are aggregations of general topics. Perusing those is \\nuseful whether you are participating in your first competition, have a suggestion to make, or just \\nhave a general question because you feel lost.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 106}, page_content='Below the  forums, you can find the combined view of discussions across Kaggle: mostly conver -\\nsations related to competitions (which form the bulk of activity on Kaggle), but also Notebooks \\nor notable datasets. By default, they are sorted by Hotness; in other words, those with the highest \\nparticipation and most activity are shown closer to the top. This section is where you can find'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 106}, page_content='content more relevant to the dynamic nature of the field: a collection of discussions from different \\nsubsets of Kaggle, with the ability to filter on specific criteria:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 107}, page_content='Chapter 4 81\\nFigure 4.2: Discussions from across Kaggle\\nDepending on your interest, you can start personalizing the content by using the filters. Based \\non your preferences, you can filter by:\\n• RECENCY : Allows you to control the range of information you are catching up on\\n• MY ACTIVITY: If you need an overview of your comments/publications/views across all'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 107}, page_content='forums; useful if you are involved in multiple discussions simultaneously \\n• ADMIN: Provides a quick overview of announcements from Kaggle admins\\n• TYPES: Discussions can take place in the general forums, in specific competitions, or \\naround datasets'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 108}, page_content='Leveraging Discussion Forums 82\\n• TAGS: While not present everywhere, several discussions are tagged, and this functionality \\nallows a user to make use of that fact:\\nFigure 4.3: Available filters for discussions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 109}, page_content='Chapter 4 83\\nThe next figure shows a sample  output of filtering on discussions on the Beginner tag:\\nFigure 4.4: Filtering discussions to those tagged “Beginner”\\nAs an alternative, you can also focus on a specific topic; since topics like computer vision attract a \\nlot of interest, it is probably useful to sort the topics. You can sort by Hotness, Recent Comments,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 109}, page_content='Recently Posted, Most Votes, and Most Comments:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 110}, page_content='Leveraging Discussion Forums 84\\nFigure 4.5: The Computer Vision topics subset of the general discussion forum\\nPeople come to Kaggle for diverse reasons but, despite the growth in popularity of Notebooks, \\ncompetitions remain the primary attraction. Each Kaggle competition has its own dedicated dis-\\ncussion forum, which you can enter by going into the competition page and selecting Discussion :'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 110}, page_content='Figure 4.6: Discussion forum for a competition'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 111}, page_content='Chapter 4 85\\nIt was not always the case, but these days virtually all competitions have an FAQ topic pinned at \\nthe top of their dedicated discussion forum. Starting there is a good idea for two main reasons:\\n• It saves you time; the most popular queries are probably addressed there.\\n• You avoid asking redundant or duplicate questions in the remainder of the forum, making'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 111}, page_content='everyone’s experience better.\\nLike Notebooks, discussion forums have an option for you to bookmark particularly relevant \\ntopics for later reference:\\nFigure 4.7: Bookmarking a topic in a discussion forum'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 112}, page_content='Leveraging Discussion Forums 86\\nAn overview of all your bookmarked topics can be found on your profile page:\\nFigure 4.8: Bookmarking a topic in a discussion forum\\nExample discussion approaches\\nIt is a completely normal thing to feel lost in a competition at some point: you came in, tried a \\nfew ideas, got some traction on the leaderboard, and then you hit the Kaggle version of a runner’s'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 112}, page_content='wall. This is the moment when discussion forums are the place to consult.\\nAs an example, we will look at the Optiver Realized Volatility Prediction competition ( https://www.\\nkaggle.com/c/optiver-realized-volatility-prediction ), characterized by the organizers \\nlike this:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='Chapter 4 87\\nThere is quite a lot to unpack here, so we will walk over the main components of this challenge \\nand show how they can be approached via the discussion forums. First, participation in this com -\\npetition requires some level of financial knowledge; not quite experienced trader level maybe, but'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='understanding the different manners of calculating volatility is certainly not trivial for a layman \\n(which most Kagglers are in this specific matter). Luckily for the participants, the organizers were \\nvery active during the competition and provided guidance on resources intended to help new -\\ncomers to the field: https://www.kaggle.com/c/optiver-realized-volatility-prediction/'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='discussion/273923 .\\nIf the entry knowledge still proves insufficient to get started, do not hesitate to figure things out in \\npublic and ask for help, like here: https://www.kaggle.com/c/optiver-realized-volatility-\\nprediction/discussion/263039 .\\nOr here: https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/  \\n250612.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='250612.\\nAs the competition went on, people started developing increasingly sophisticated models to \\nhandle the problem. There is a balance to strike here: on the one hand, you might want to give \\nsomething back if you have learned from veterans sharing their findings before; on the other hand, \\nyou do not want to give away your (potential) advantage by publishing all your great code as a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='Notebook. A reasonable compromise is discussing, for example, your feature ideas in a post in the \\nforum competition, along the lines of this one: https://www.kaggle.com/c/optiver-realized-\\nvolatility-prediction/discussion/273915 .In the first three months of this competition, you’ll build models that predict short-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 113}, page_content='term volatility for hundreds of stocks across different sectors. You will have hundreds \\nof millions of rows of highly granular financial data at your fingertips, with which \\nyou’ll design your model forecasting volatility over 10-minute periods. Your models \\nwill be evaluated against real market data collected in the three-month evaluation \\nperiod after training.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='Leveraging Discussion Forums 88\\nIn recent years, more competitions are moving away from the fixed test dataset format and in-\\ntroduce some sort of variation: sometimes they enforce the usage of the Kaggle API (these com-\\npetitions require submission from a Notebook), others introduce a special timetable split into a \\ntraining phase and evaluation against live data. This was the case with Optiver:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='While straightforward to formulate, this setup generated a few challenges for re-training and \\nupdating the models. Should you encounter this kind of situation, feel free to inquire, as partic -\\nipants did in this competition: https://www.kaggle.com/c/optiver-realized-volatility-\\nprediction/discussion/249752 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='A validation scheme for your trained model is always an important topic in a Kaggle competition, \\nusually coupled with the perennial “CV vs LB” (cross-validation versus leaderboard) discussion. \\nThe Optiver competition was no exception to that rule: https://www.kaggle.com/c/optiver-\\nrealized-volatility-prediction/discussion/250650 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='Unless the thread is already present – and it’s always a good idea to check, so that redundancy can \\nbe minimized – you might want to consider a related type of thread: single-model performance. \\nSooner or later, everybody starts using ensembles of models, but they are not very efficient without \\ngood single-model components. The collaborative quest for knowledge does not stop there: if'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='you think you have found a better way of approaching the problem, it is probably a good idea to \\nshare it. Either you will have done something useful for others, or you will find out why you were \\nwrong (saving you time and effort); either way, a win, as shown, for instance, in this discussion: \\nhttps://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/260694 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='Apart from the obvious personal benefit (you get a peek into how other competitors are doing), \\nsuch threads allow for information exchange in the community, facilitating the collaborative el-\\nement and being helpful for beginners. An example of such a discussion can be found at https://'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 114}, page_content='www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/250695 .Starting after the final submission deadline there will be periodic updates to the \\nleaderboard to reflect market data updates that will be run against selected note -\\nbooks. Updates will take place roughly every two weeks, with an adjustment to \\navoid the winter holidays.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='Chapter 4 89\\nIf you have gone through the topics such as the ones listed above, there is a possibility you still \\nfind yourself wondering: am I missing anything important?  Kaggle is the kind of place where it is \\nperfectly fine to ask: https://www.kaggle.com/c/optiver-realized-volatility-prediction/\\ndiscussion/262203 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='discussion/262203 .\\nLet’s broaden our focus out to other competitions to wrap up this section. We mentioned val -\\nidation above, which always links – at least for a Kaggler – to the topic of information leakage \\nand overfitting. Leaks are discussed extensively in Chapter 6, which is dedicated to designing'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='validation schemes. Here, we touch briefly on how they are approached via discussions. With \\nKaggle being a community of inquisitive people, if there is suspicion of leakage, somebody is \\nlikely to raise the topic.\\nFor example, names of the files or IDs of records may contain timestamps, which means they'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='can be reverse engineered to effectively peek into the future and produce an unrealistically low \\nerror metric value. Such a situation took place in the Two Sigma Connect competition ( https://\\nwww.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/ ). You can read up on the \\ndetails in Kazanova’s post: https://www.kaggle.com/c/two-sigma-connect-rental-listing-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='inquiries/discussion/31870#176513 .\\nAnother example is the Airbus Ship Detection Challenge ( https://www.kaggle.com/c/airbus-\\nship-detection ), in which the participants were tasked with locating ships in satellite images. \\nIt turned out that a significant proportion of test images were (random) crops of the images in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='the training images and matching the two was relatively straightforward: https://www.kaggle.\\ncom/c/airbus-ship-detection/discussion/64355#377037 .\\nA rather infamous series of competitions were the ones sponsored by Santander. Of the three \\ninstances when the company organized a Kaggle contest, two involved data leakage: https://'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='www.kaggle.com/c/santander-value-prediction-challenge/discussion/61172 .\\nWhat happens next varies per competition: there have been instances when Kaggle decided to reset \\nthe competition with new or cleaned up data, but also when they allowed it to continue (because \\nthey perceived the impact as minimal). An example of handling such a situation can be found'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 115}, page_content='in the Predicting Red Hat Business Value competition: https://www.kaggle.com/c/predicting-\\nred-hat-business-value/discussion/23788 .\\nAlthough leaks in data can disturb a competition severely, the good news is that over the last 2-3 \\nyears, leakage has all but disappeared from Kaggle – so with any luck, this section will be read \\nonce but not become a staple of your experience on the platform.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='Leveraging Discussion Forums 90\\nThe topic of experience on the platform is an excellent segue into a Grandmaster interview.\\nYifan Xie\\nhttps://www.kaggle.com/yifanxie\\nYifan Xie is a Discussions and Competitions Master, as well as the \\nco-founder of Arion.ai. Here’s what he had to say about competing in \\ncompetitions and working with other Kagglers.\\nWhat’s your favorite kind of competition and why?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='In terms of techniques and solving approaches, what is your specialty \\non Kaggle?\\nI don’t really have a favorite type; I like tackling problems of all kinds. In terms of techniques, I have \\nbuilt up a solid pipeline of machine learning modules that allow me to quickly apply typical techniques \\nand algorithms on most data problems. I would say this is a kind of competitive advantage for me: a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='focus on standardizing, both in terms of work routine and technical artifacts over time. This allows for \\nquicker iteration and in turn helps improve efficiency when conducting data experiments, which is a \\ncore component of Kaggle.\\nHow do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='Over time, I have developed a specific way of managing and gathering information for most of my ma-\\njor data endeavors. This is applicable to work, Kaggle competitions, and other side projects. Typically, \\nI capture useful information such as bookmarks, data dictionaries, to-do lists, useful commands, and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='experiment results in a standardized format dedicated to each competition, and when competing in a \\nteam, I will share this info with my teammates.\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task?\\nFor me, it has always been useful to understand the wider context of the competition; for instance, what'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='are the social/engineering/financial processes that underpin and bring about the data we are working on? \\nFor competitions in which one can meaningfully observe individual data points, such as the Deepfake \\nDetection Challenge,  I would build a specific dashboard (usually using Streamlit) that allows me to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 116}, page_content='check individual data points (in this case, it was pair of true and fake videos), as well as building simple \\nstat gathering into the dashboard to allow me a better feel of the data.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='Chapter 4 91\\nHas Kaggle helped you in your career? If so, how?\\nI would say Kaggle is the platform that contributed the most to my current career path as a co-owner of \\na data science consultancy firm. It allowed me to build over several years the skillset and methodology to \\ntackle data problems in different domains. I have both customers and colleagues who I got to know from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='forming teams on Kaggle competitions, and it has always served me very well as a source of knowledge, \\neven though I am less active on it these days.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?\\nFor newcomers on Kaggle, the one error I can see is overlooking critical non-technical matters: rules on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='teaming, data usage, sharing of private information, usage of multiple accounts for innocuous reasons, etc. \\nThese are the types of error that could completely invalidate one’s often multi-month competition efforts.\\nThe one thing I wish I knew at the beginning would be not to worry about the day-to-day position on the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='public leaderboard – it brings unnecessary pressure on oneself, and causes overfitting.\\nAre there any particular tools or libraries that you would recommend \\nusing for data analysis or machine learning?\\nThe usual: Scikit-learn, XGB/LGB, PyTorch, etc. The one tool I would recommend that people learn to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='master beyond basic usage would be NumPy, especially for more advanced ways to sort and subset in -\\nformation; stuff that a lazy approach via pandas makes easy, but for which a more elaborate equivalent \\nversion in NumPy would bring much better efficiency.\\nWhat’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='There are four reasons to do any data science-related stuff, in my book: for profit, for knowledge, for fun, \\nand for good. Kaggle for me is always a great source of knowledge and very often a great memory to \\ndraw upon, so my recommendation would always be to remind oneself that ranking is temporary, but \\nknowledge/memory are permanent :)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='Do you use other competition platforms? How do they compare to \\nKaggle?\\nI am a very active participant on Numerai. For me, based on my four reasons to do data science, it is \\nmore for profit, as they provide a payout via their cryptocurrency. It is more of a solitary effort, as there \\nis not really an advantage to teaming; they don’t encourage or forbid it, but it is just that more human'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 117}, page_content='resources don’t always equate to better profit on a trading competition platform like Numerai.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='Leveraging Discussion Forums 92\\nNetiquette\\nAnybody who has been online for longer than 15 minutes knows this: during a discussion, no \\nmatter how innocent the topic, there is always a possibility that people will become emotion-\\nal, and a conversation will leave the civilized parts of the spectrum. Kaggle is no exception to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='the rule, so the community has guidelines for appropriate conduct: https://www.kaggle.com/\\ncommunity-guidelines .\\nThose apply not just to discussions, but also to Notebooks and other forms of communication. \\nThe main points you should keep in mind when interacting on Kaggle are:\\n• Don’t slip into what Scott Adams calls the mind-reading illusion: Kaggle is an extremely'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='diverse community of people from all over the world (for many of them, English is not their \\nfirst language), so maintaining nuance is a massive challenge. Don’t make assumptions \\nand try to clarify whenever possible.\\n• Do not make things personal; Godwin’s law is there for a reason. In particular, references \\nto protected immutable characteristics are an absolute no-go area.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='• Your mileage might vary, but the fact remains: this is not the Internet Wild West of the \\n1990s, when telling somebody online to RTFM was completely normal; putdowns tend \\nto alienate people.\\n• Do not attempt to manipulate the progression system (the basis for awarding Kaggle \\nmedals): this aspect covers an entire spectrum of platform abuse, from explicitly asking'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='for upvotes, to collusion, to outright cheating.\\nIn short, do toward others as you would have them do to you, and things should work out fine.Numerai for me is a more sustainable activity than Kaggle during busy periods of my working calendar, \\nbecause the training data is usually unchanged at each round, and I can productionize to a high degree'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 118}, page_content='to automate the prediction and submission once the initial models are built.\\nThe continuity feature of Numerai also makes it better suited for people who want to build dedicated \\nmachine learning pipelines for tabular datasets.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 119}, page_content='Chapter 4 93\\nSummary\\nIn this chapter, we have talked about discussion forums, the primary manner of communication \\non the Kaggle platform. We demonstrated the forum mechanics, showed you examples of how \\ndiscussions can be leveraged in more advanced competitions, and briefly summarized discussion \\nnetiquette.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 119}, page_content='netiquette.\\nThis concludes the first, introductory part of this book. The next chapter marks the start of a more \\nin-depth exploration of how to maximize what you get out of Kaggle, and looks at getting to \\ngrips with the huge variety of different tasks and metrics you must wrestle with in competitions.\\nJoin our book’s Discord space'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 119}, page_content='Join our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors: \\nhttps://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 121}, page_content='Part II\\nSharpening Your Skills for \\nCompetitions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='5\\nCompetition Tasks and Metrics\\nIn a competition, you start by examining the target metric. Understanding how your model’s \\nerrors are evaluated is key for scoring highly in every competition. When your predictions are \\nsubmitted to the Kaggle platform, they are compared to a ground truth based on the target metric.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='For instance, in the Titanic competition ( https://www.kaggle.com/c/titanic/ ), all your sub -\\nmissions are evaluated based on accuracy, the percentage of surviving passengers you correctly \\npredict. The organizers decided upon this metric because the aim of the competition is to find \\na model that estimates the probability of survival of a passenger under similar circumstances.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='In another knowledge competition, House Prices - Advanced Regression Techniques  (https://www.\\nkaggle.com/c/house-prices-advanced-regression-techniques ), your work will be evaluated \\nbased on an average difference between your prediction and the ground truth. This involves com-\\nputing the logarithm, squaring, and taking the square root, because the model is expected to be'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='able to quantify as correctly as possible the order of the price of a house on sale.\\nIn real-world data science, target metrics are also key for the success of your project, though \\nthere are certainly differences between the real world and a Kaggle competition. We could easily \\nsummarize by saying that there are more complexities in the real world. In real-world projects,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='you will often have not just one but multiple metrics that your model will be evaluated against. \\nFrequently, some of the evaluation metrics won’t even be related to how your predictions perform \\nagainst the ground truth you are using for testing. For instance, the domain of knowledge you are \\nworking in, the scope of the project, the number of features considered by your model, the overall'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 123}, page_content='memory usage, any requirement for special hardware (such as a GPU, for instance), the latency \\nof the prediction process, the complexity of the predicting model, and many other aspects may \\nend up counting more than the mere predictive performance.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='Competition Tasks and Metrics 98\\nReal-world problems are indeed dominated by business and tech infrastructure concerns much \\nmore than you may imagine before being involved in any of them.\\nYet you cannot escape the fact that the basic principle at the core of both real-world projects and \\nKaggle competitions is the same. Your work will be evaluated according to some criteria, and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='understanding the details of such criteria, optimizing the fit of your model in a smart way, or se -\\nlecting its parameters according to the criteria will bring you success. If you can learn more about \\nhow model evaluation occurs in Kaggle, your real-world data science job will also benefit from it.\\nIn this chapter, we are going to detail how evaluation metrics for certain kinds of problems'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='strongly influence the way you can operate when building your model solution in a data science \\ncompetition. We also address the variety of metrics available in Kaggle competitions to give you \\nan idea of what matters most and, in the margins, we discuss the different effects of metrics on \\npredictive performance and how to correctly translate them into your projects. We will cover the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='following topics:\\n• Evaluation metrics and objective functions\\n• Basic types of tasks: regression, classification, and ordinal\\n• The Meta Kaggle dataset\\n• Handling never-before-seen metrics\\n• Metrics for regression (standard and ordinal)\\n• Metrics for binary classification (label prediction and probability)\\n• Metrics for multi-class classification\\n• Metrics for object detection problems'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='• Metrics for multi-label classification and recommendation problems\\n• Optimizing evaluation metrics\\nEvaluation metrics and objective functions\\nIn a Kaggle competition, you can find the evaluation metric in the left menu on the Overview \\npage of the competition. By selecting the Evaluation  tab, you will get details about the evaluation'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 124}, page_content='metric. Sometimes you will find the metric formula, the code to reproduce it, and some discus -\\nsion of the metric. On the same page, you will also get an explanation about the submission file \\nformat, providing you with the header of the file and a few example rows.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='Chapter 5 99\\nThe association between the evaluation metric and the submission file is important, because you \\nhave to consider that the metric works essentially after you have trained your model and pro -\\nduced some predictions. Consequently, as a first step, you will have to think about the difference \\nbetween an evaluation metric  and an objective function .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='Boiling everything down to the basics, an objective function serves your model during training \\nbecause it is involved in the process of error minimization (or score maximization, depending \\non the problem). In contrast, an evaluation metric serves your model after it has been trained by \\nproviding a score. Therefore, it cannot influence how the model fits the data, but does influence'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='it in an indirect way: by helping you to select the most well-performing hyperparameter settings \\nwithin a model, and the best models among competing ones. Before proceeding with the rest of \\nthe chapter, which will show you how this can affect a Kaggle competition and why the analysis \\nof the Kaggle evaluation metric should be your first act in a competition, let’s first discuss some'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='terminology that you may encounter in the discussion forums.\\nYou will often hear talk about objective functions, cost functions, and loss functions, sometimes \\ninterchangeably. They are not exactly the same thing, however, and we explain the distinction here:\\n• A loss function  is a function that is defined on a single data point, and, considering the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='prediction of the model and the ground truth for the data point, computes a penalty.\\n• A cost function  takes into account the whole dataset used for training (or a batch from \\nit), computing a sum or average over the loss penalties of its data points. It can comprise \\nfurther constraints, such as the L1 or L2 penalties, for instance. The cost function directly \\naffects how the training happens.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='• An objective function is the most general (and safe-to-use) term related to the scope \\nof optimization during machine learning training: it comprises cost functions, but it is \\nnot limited to them. An objective function, in fact, can also take into account goals that \\nare not related to the target: for instance, requiring sparse coefficients of the estimated'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='model or a minimization of the coefficients’ values, such as in L1 and L2 regularizations. \\nMoreover, whereas loss and cost functions imply an optimization based on minimization, \\nan objective function is neutral and can imply either a maximization or a minimization \\nactivity performed by the learning algorithm.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 125}, page_content='Likewise, when it comes to evaluation metrics, you’ll hear about scoring functions and error \\nfunctions. Distinguishing between them is easy: a scoring function  suggests better prediction \\nresults if scores from the function are higher, implying a maximization process.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='Competition Tasks and Metrics 100\\nAn error function  instead suggests better predictions if smaller error quantities are reported by \\nthe function, implying a minimization process.\\nBasic types of tasks\\nNot all objective functions are suitable for all problems. From a general point of view, you’ll find'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='two kinds of problems in Kaggle competitions: regression tasks and classification tasks. Recently, \\nthere have also been reinforcement learning ( RL) tasks, but RL doesn’t use metrics for evaluation; \\ninstead, it relies on a ranking derived from direct match-ups against other competitors whose \\nsolutions are assumed to be as well-performing as yours (performing better in this match-up than'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='your peers will raise your ranking, performing worse will lower it). Since RL doesn’t use metrics, \\nwe will keep on referring to the regression-classification dichotomy, though ordinal tasks, where \\nyou predict ordered labels represented by integer numbers, may elude such categorization and \\ncan be dealt with successfully either using a regression or classification approach.\\nRegression'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='Regression\\nRegression  requires you to build a model that can predict a real number; often a positive number, \\nbut there have been examples of negative number prediction too. A classic example of a regression \\nproblem is House Prices - Advanced Regression Techniques , because you have to guess the value of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='a house. The evaluation of a regression task involves computing a distance between your pre -\\ndictions and the values of the ground truth. This difference can be evaluated in different ways, \\nfor instance by squaring it in order to punish larger errors, or by applying a log to it in order to \\npenalize predictions of the wrong scale.\\nClassification'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='Classification\\nWhen facing a classification task on Kaggle, there are more nuances to take into account. The \\nclassification, in fact, could be binary , multi-class , or multi-label.\\nIn binary problems, you have to guess if an example should be classified or not into a specific \\nclass (usually called the positive class and compared to the negative one). Here, the evaluation'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='could involve the straightforward prediction of the class ownership itself, or an estimation of \\nthe probability of such ownership. A typical example is the Titanic competition, where you have \\nto guess a binary outcome: survival or not-survival. In this case, the requirement of the compe -\\ntition is just the prediction, but in many cases, it is necessary to provide a probability because'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 126}, page_content='in certain fields, especially for medical applications, it is necessary to rank positive predictions \\nacross different options and situations in order to make the best decision.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='Chapter 5 101\\nThough counting the exact number of correct matches in a binary classification may seem a valid \\napproach, this won’t actually work well when there is an imbalance, that is, a different number \\nof examples, between the positive and the negative class. Classification based on an imbalanced'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='distribution of classes requires evaluation metrics that take the imbalance into account, if you \\nwant to correctly track improvements on your model.\\nWhen you have more than two classes, you have a multi-class prediction problem. This also \\nrequires the use of suitable functions for evaluation, since it is necessary to keep track of the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='overall performance of the model, but also to ensure that the performance across the classes is \\ncomparable (for instance, your model could underperform with respect to certain classes). Here, \\neach case can be in one class exclusively, and not in any others. A good example is Leaf Classifica -\\ntion (https://www.kaggle.com/c/leaf-classification ), where each image of a leaf specimen'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='has to be associated with the correct plant species.\\nFinally, when your class predictions are not exclusive and you can predict multiple class ownership \\nfor each example, you have a multi-label problem that requires further evaluations in order to \\ncontrol whether your model is predicting the correct classes, as well as the correct number and mix'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='of classes. For instance, in Greek Media Monitoring Multilabel Classification (WISE 2014) ( https://\\nwww.kaggle.com/c/wise-2014 ), you had to associate each article with all the topics it deals with.\\nOrdinal\\nIn a problem involving a prediction on an ordinal scale, you have to guess integer numeric labels,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='which are naturally ordered. As an example, the magnitude of an earthquake is on an ordinal scale. \\nIn addition, data from marketing research questionnaires is often recorded on ordinal scales (for \\ninstance, consumers’ preferences or opinion agreement). Since an ordinal scale is made of ordered \\nvalues, ordinal tasks can be considered somewhat halfway between regression and classification,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='and you can solve them in both ways.\\nThe most common way is to treat your ordinal task as a multi-class problem. In this case, you will \\nget a prediction of an integer value (the class label) but the prediction will not take into account \\nthat the classes have a certain order. You can get a feeling that there is something wrong with'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 127}, page_content='approaching the problem as a multi-class problem if you look at the prediction probability for \\nthe classes. Often, probabilities will be distributed across the entire range of possible values, de -\\npicting a multi-modal and often asymmetric distribution (whereas you should expect a Gaussian \\ndistribution around the maximum probability class).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='Competition Tasks and Metrics 102\\nThe other way to solve the ordinal prediction problem is to treat it as a regression problem and \\nthen post-process your result. In this way, the order among classes will be taken into consider -\\nation, though the prediction output won’t be immediately useful for scoring on the evaluation'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='metric. In fact, in a regression you get a float number as an output, not an integer representing \\nan ordinal class; moreover, the result will include the full range of values between the integers of \\nyour ordinal distribution and possibly also values outside of it. Cropping the output values and \\ncasting them into integers by unit rounding may do the trick, but this might lead to inaccuracies'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='requiring some more sophisticated post-processing (we’ll discuss more on this later in the chapter).\\nNow, you may be wondering what kind of evaluation you should master in order to succeed in \\nKaggle. Clearly, you always have to master the evaluation metric of the competition you have \\ntaken on. However, some metrics are more common than others, which is information you can'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='use to your advantage. What are the most common metrics? How can we figure out where to look \\nfor insights in competitions that have used similar evaluation metrics? The answer is to consult \\nthe Meta Kaggle dataset.\\nThe Meta Kaggle dataset\\nThe Meta Kaggle dataset ( https://www.kaggle.com/kaggle/meta-kaggle ) is a collection of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='rich data about Kaggle’s community and activity, published by Kaggle itself as a public dataset. \\nIt contains  CSV tables filled with public activity from Competitions, Datasets, Notebooks, and \\nDiscussions. All you have to do is to start a Kaggle Notebook (as you saw in Chapters 2  and 3), add \\nto it the Meta Kaggle dataset, and start analyzing the data. The CSV tables are updated daily, so'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='you’ll have to refresh your analysis often, but that’s worth it given the insights you can extract.\\nWe will sometimes refer to the Meta Kaggle dataset in this book, both as inspiration for many \\ninteresting examples of the dynamics in a competition and as a way to pick up useful examples \\nfor your learning and competition strategies. Here, we are going to use it in order to figure out'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 128}, page_content='what evaluation metrics have been used most frequently for competitions in the last seven years. \\nBy looking at the most common ones in this chapter, you’ll be able to start any competition from \\nsolid ground and then refine your knowledge of the metric, picking up competition-specific nu-\\nances using the discussion you find in the forums.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 129}, page_content='Chapter 5 103\\nBelow, we introduce the code necessary to produce a data table of metrics and their counts per \\nyear. It is designed to run directly on the Kaggle platform:\\nimport numpy as np\\nimport pandas as pd\\ncomps = pd.read_csv( \"/kaggle/input/meta-kaggle/Competitions.csv\" )\\nevaluation = [ \\'EvaluationAlgorithmAbbreviation\\' ,\\n              \\'EvaluationAlgorithmName\\' ,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 129}, page_content=\"'EvaluationAlgorithmDescription' ,]\\ncompt = [ 'Title', 'EnabledDate' , 'HostSegmentTitle' ]\\ndf = comps[compt + evaluation].copy()\\ndf['year'] = pd.to_datetime(df.EnabledDate).dt.year.values\\ndf['comps'] = 1\\ntime_select = df.year >= 2015\\ncompetition_type_select = df.HostSegmentTitle.isin(\\n      ['Featured' , 'Research' ])\\npd.pivot_table(df[time_select&competition_type_select],\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 129}, page_content=\"values= 'comps',\\n               index=[ 'EvaluationAlgorithmAbbreviation' ],\\n               columns=[ 'year'],\\n               fill_value= 0.0,\\n               aggfunc=np. sum,\\n               margins= True\\n              ).sort_values(\\n                by=( 'All'), ascending= False).iloc[1:,:].head( 20)\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 129}, page_content='In this code, we read in the CSV table containing the data relating to the competitions. We focus \\non the columns representing the evaluation and the columns informing us about the competition \\nname, start date, and type. We limit the rows to those competitions held since 2015 and that are \\nof the Featured or Research type (which are the most common ones). We complete the analysis'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 129}, page_content='by creating a pandas pivot table, combining the evaluation algorithm with the year, and counting \\nthe number of competitions using it. We just display the top 20 algorithms.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 130}, page_content='Competition Tasks and Metrics 104\\nHere is the resulting table (at the time of writing):\\nyear2015 2016 2017 2018 2019 2020 2021 TotEvaluation Algorithm\\nAUC 441 3 32017\\nLogLoss 22 52 32016\\nMAP@{K} 1 30410110\\nCategorizationAccuracy 10401208\\nMulticlassLoss 2 3201008\\nRMSLE 21 311008\\nQuadraticWeightedKappa 30012107\\nMeanFScoreBeta 10121207\\nMeanBestErrorAtK 00221106\\nMCRMSLE 00100 506\\nMCAUC 10100 30 5'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 130}, page_content='MCAUC 10100 30 5\\nRMSE 110 3000 5\\nDice 0110210 5\\nGoogleGlobalAP 0012110 5\\nMacroFScore 00010214\\nScore 00 30000 3\\nCRPS 2000100 3\\nOpenImagesObjectDetectionAP 0001110 3\\nMeanFScore 0010002 3\\nRSNAObjectDetectionAP 00010102\\nUsing the same variables we just instantiated in order to generate the table, you can also check \\nthe data to find the competitions where the metric of your choice has been adopted:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 130}, page_content=\"metric = 'AUC'\\nmetric_select = df[ 'EvaluationAlgorithmAbbreviation' ]==metric\\nprint(df[time_select&competition_type_select&metric_select]\\n        [[ 'Title', 'year' ]])\\nIn the above snippet, we decided to represent the competitions that have been using the AUC \\nmetric. You just have to change the string representing the chosen metric and the resulting list \\nwill be updated accordingly.\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='Chapter 5 105\\nComing back to the table generated, we can examine the most popular evaluation metrics used \\nin competitions hosted on Kaggle:\\n• The two top metrics are closely related to each other and to binary probability classifica -\\ntion problems. The AUC metric helps to measure if your model’s predicted probabilities'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='tend to predict positive cases with high probabilities, and the Log Loss helps to measure \\nhow far your predicted probabilities are from the ground truth (and as you optimize for \\nLog Loss, you also optimize for the AUC metric).\\n• In 3rd position, we find MAP@{K}, which is a common metric in recommender systems and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='search engines. In Kaggle competitions, this metric has been used mostly for information \\nretrieval evaluations, such as in the Humpback Whale Identification competition ( https://\\nwww.kaggle.com/c/humpback-whale-identification ), where you have to precisely iden -\\ntify a whale and you have five possible guesses. Another example of MAP@{K} usage is in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='the Quick, Draw! Doodle Recognition Challenge ( https://www.kaggle.com/c/quickdraw-\\ndoodle-recognition/ ), where your goal is to guess the content of a drawn sketch and you \\nare allowed three attempts. In essence, when MAP@{K} is the evaluation metric, you can \\nscore not just if you can guess correctly, but also if your correct guess is among a certain'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='number (the “K” in the name of the function) of other incorrect predictions.\\n• Only in 6th position can we find a regression metric, the RMSLE, or Root Mean Squared \\nLogarithmic Error, and in 7th place the  Quadratic Weighted Kappa, a metric particularly \\nuseful for estimating model performance on problems that involve guessing a progressive \\ninteger number (an ordinal scale problem).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='As you skim through the list of top metrics, you will  keep on finding metrics that are commonly \\ndiscussed in machine learning textbooks. In the next few sections, after first discussing what to \\ndo when you meet a never-before-seen metric (something that happens in Kaggle competitions \\nmore frequently than you may expect), we will revise some of the most common metrics found'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 131}, page_content='in regression and classification competitions.\\nHandling never-before-seen metrics\\nBefore proceeding, we have to consider that the top 20 table doesn’t cover all the metrics used \\nin competitions. We should be aware that there are metrics that have only been used once in \\nrecent years.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 132}, page_content=\"Competition Tasks and Metrics 106\\nLet’s keep on using the results from the previous code to find out what they are:\\ncounts = (df[time_select&competition_type_select]\\n          .groupby( 'EvaluationAlgorithmAbbreviation' ))\\ntotal_comps_per_year = (df[time_select&competition_type_select]\\n                        .groupby( 'year').sum())\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 132}, page_content=\"single_metrics_per_year = (counts. sum()[counts. sum().comps== 1]\\n                           .groupby( 'year').sum())\\nsingle_metrics_per_year\\ntable = (total_comps_per_year.rename(columns={ 'comps': 'n_comps' })\\n         .join(single_metrics_per_year / total_comps_per_year)\\n         .rename(columns={ 'comps': 'pct_comps' }))\\nprint(table)\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 132}, page_content='print(table)\\nAs a result, we get the following table showing, for each year, how many competitions used a \\nmetric that has never been used afterward ( n_comps), and the proportion of these competitions \\nper year ( pct_comps ):\\n      n_comps pct_comps\\nyear                   \\n2015       28  0.179\\n2016       19  0.158\\n2017       34  0.177\\n2018       35  0.229\\n2019       36  0.278'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 132}, page_content='2019       36  0.278\\n2020       43  0.302\\n2021        8  0.250\\nObserving the relative share of competitions with a never-to-be-seen-afterward metric, we im -\\nmediately notice how it is growing year by year and that it reached the 25%-30% level in recent \\nyears, implying that typically one competition out of every three or four requires you to study \\nand understand a metric from scratch.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 132}, page_content='You can get the list of such metrics that have occurred in the past with a simple code snippet:\\nprint(counts. sum()[counts. sum().comps== 1].index.values)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content=\"Chapter 5 107\\nBy executing the code, you will get a list similar to this one:\\n['AHD@{Type}' 'CVPRAutoDrivingAveragePrecision' 'CernWeightedAuc'\\n'FScore_1' 'GroupMeanLogMAE' 'ImageNetObjectLocalization'\\n'IndoorLocalization'  'IntersectionOverUnionObjectSegmentationBeta'\\n'IntersectionOverUnionObjectSegmentationWithClassification'\\n'IntersectionOverUnionObjectSegmentationWithF1' 'Jaccard'\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content=\"'JaccardDSTLParallel' 'JigsawBiasAUC' 'LaplaceLogLikelihood'\\n'LevenshteinMean' 'Lyft3DObjectDetectionAP' 'M5_WRMSSE' 'MASpearmanR' \\n'MCRMSE' 'MCSpearmanR' 'MWCRMSE' 'MeanColumnwiseLogLoss' \\n'MulticlassLossOld' 'NDCG@{K}' 'NQMicroF1' 'NWRMSLE' 'PKUAutoDrivingAP' \\n'R2Score' 'RValue' 'RootMeanSquarePercentageError' 'SIIMDice' 'SMAPE'\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content=\"'SantaResident' 'SantaRideShare' 'SantaWorkshopSchedule2019' 'TrackML'\\n 'TravelingSanta2' 'TwoSigmaNews' 'WeightedAUC' 'WeightedMulticlassLoss' \\n'WeightedPinballLoss' 'WeightedRowwisePinballLoss' 'YT8M_\\nMeanAveragePrecisionAtK' 'ZillowMAE' 'football' 'halite' 'mab']\\nBy close inspection, you can find many metrics relating to deep learning and reinforcement learn -\\ning competitions.\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content='ing competitions.\\nWhat do you do when you meet a metric that has never been used before? Of course, you can rely \\non the discussions in the Kaggle discussion forums, where you can always find good inspiration \\nand many Kagglers who will help you. However, if you want to build up your own knowledge \\nabout the metric, aside from Googling it, we advise that you try to experiment with it by coding'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content='the evaluation function by yourself, even in an imperfect way, and trying to simulate how the \\nmetric reacts to different types of error produced by the model. You could also directly test how it \\nfunctions on a sample from the competition training data or synthetic data that you have prepared.\\nWe can quote a few examples of this approach as used by Kagglers:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content='• Carlo Lepelaars  with Spearman’s Rho: https://www.kaggle.com/carlolepelaars/\\nunderstanding-the-metric-spearman-s-rho\\n• Carlo Lepelaars with Quadratic Weighted Kappa: https://www.kaggle.com/\\ncarlolepelaars/understanding-the-metric-quadratic-weighted-kappa\\n• Rohan Rao  with Laplace Log Likelihood: https://www.kaggle.com/rohanrao/osic-\\nunderstanding-laplace-log-likelihood'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 133}, page_content='This can give you increased insight into the evaluation and an advantage over other competitors \\nrelying only on answers from Googling and Kaggle forums.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='Competition Tasks and Metrics 108\\nRohan Rao\\nhttps://www.kaggle.com/rohanrao\\nBefore we start exploring different metrics, let’s catch up with Rohan \\nRao (aka Vopani) himself, Quadruple Grandmaster and Senior Data \\nScientist at H2O.ai, about his successes on Kaggle and the wisdom he \\nhas to share with us.\\nWhat’s your favourite kind of competition and why? In terms of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='techniques and solving approaches, what is your speciality on Kaggle?\\nI like to dabble with different types of competitions, but my favorite would certainly be time series ones. I \\ndon’t quite like the typical approaches to and concepts of time series in the industry, so I tend to innovate \\nand think out of the box by building solutions in an unorthodox way, which has ended up being very'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='successful for me.\\nHow do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?\\nFor any Kaggle competition, my typical workflow would look like this:\\n• Understand the problem statement and read all the information related to rules, format, timelines, \\ndatasets, metrics, and deliverables.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='• Dive deep into the data. Slice and dice it in every way possible and explore/visualize it to be able \\nto answer any question about it.\\n• Build a simple pipeline with a baseline model and make a submission to confirm the process works.\\n• Engineer features, tune hyperparameters, and experiment with multiple models to get a sense \\nof what’s generally working and what’s not.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='• Constantly go back to analyzing the data, reading discussions on the forum, and tweaking the \\nfeatures and models to the fullest. Maybe team up at some point.\\n• Ensemble multiple models and decide which submissions to make as final.\\nIn my day-to-day work in data science, most of this happens too. But there are two crucial elements that \\nare additionally required:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 134}, page_content='are additionally required:\\n• Curating and preparing datasets for the problem statement.\\n• Deploying the final model or solution into production.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='Chapter 5 109\\nMetrics for regression (standard and ordinal)\\nWhen working with regression problems, that is, problems that involve estimating a continuous \\nvalue (that could range from minus infinity to infinity), the most commonly used error measures \\nare RMSE (root mean squared error ) and MAE (mean absolute error ), but you can also find'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='slightly different error measures useful, such as RMSLE or MCRMSLE.\\nMean squared error (MSE) and R squared\\nThe root mean squared error is the  root of the mean squared error  (MSE), which is nothing else \\nbut the mean of the good old sum of squared errors  (SSE) that you learned about when you'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='studied how a regression works.The majority of my time has been spent in these two activities for most of the projects I’ve worked on in \\nthe past.\\nHas Kaggle helped you in your career? If so, how?\\nThe vast majority of everything I’ve learned in machine learning has come from Kaggle. The community,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='the platform, and the content are pure gold and there is an incredible amount of stuff you can learn.\\nWhat has benefitted me the most is the experience of competing in Kaggle competitions; it has given me \\nimmense confidence in understanding, structuring, and solving problems across domains, which I have'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='been able to apply successfully in many of the companies and projects I worked on outside Kaggle.\\nMany recruiters have contacted me for opportunities looking at my Kaggle achievements, primarily in \\nCompetitions. It gives a fairly good indication of a candidate’s ability in solving data science problems \\nand hence it is a great platform to showcase your skills and build a portfolio.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='What mistakes have you made in competitions in the past?\\nI’ve made some mistake in every competition! That’s how you learn and improve. Sometimes it’s a coding \\nbug, sometimes a flawed validation setup, sometimes an incorrect submission selection!\\nWhat’s important is to learn from these and ensure you don’t repeat them. Iterating over this process'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 135}, page_content='automatically helps to improve your overall performance on Kaggle.\\nAre there any particular tools or libraries that you would recommend \\nusing for data analysis/machine learning?\\nI strongly believe in never marrying a technology. Use whatever works best, whatever is most comfortable \\nand effective, but constantly be open to learning new tools and libraries.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='Competition Tasks and Metrics 110\\nHere is the formula for the MSE:\\n𝑀𝑀𝑀𝑀𝑀𝑀 𝑀1\\n𝑛𝑛𝑀𝑀𝑀𝑀𝑀𝑀 𝑀 1\\n𝑛𝑛∑(𝑦𝑦 𝑖𝑖̂ − 𝑦𝑦 𝑖𝑖)2𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nLet’s start by explaining how the formula works. First of all, n indicates the number of cases,  \\n𝑦𝑦𝑖𝑖  is the ground truth, and 𝑦𝑦𝑖𝑖̂  the prediction. You first get the difference between your predictions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='and your real values. You square the differences (so they become positive or simply zero), then \\nyou sum them all, resulting in your SSE. Then you just have to divide this measure by the number \\nof predictions to obtain the average value, the MSE. Usually, all regression models minimize the \\nSSE, so you won’t have great problems trying to minimize MSE or its direct derivatives such as R'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='squared (also called  the coefficient of determination ), which is given by:\\n𝑅𝑅2=𝑆𝑆𝑆𝑆𝑆𝑆\\n𝑆𝑆𝑆𝑆𝑆𝑆= ∑ (𝑦𝑦𝑖𝑖̂ − 𝑦𝑦 𝑖𝑖)2\\n(𝑦𝑦𝑖𝑖− 𝑦𝑦𝑦)2𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nHere, SSE (the sum of squared errors) is compared to the sum of squares total ( SST), which is \\njust the variance of the response. In statistics, in fact, SST is defined as the squared difference \\nbetween your target values and their mean:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='𝑆𝑆𝑆𝑆𝑆𝑆 =  ∑(𝑦𝑦 𝑖𝑖− 𝑦𝑦𝑦 𝑦2𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nTo put it another way, R squared compares the squared errors of the model against the squared \\nerrors from the simplest model possible, the average of the response. Since both SSE and SST \\nhave the same scale, R squared can help you to determine whether transforming your target is \\nhelping to obtain better predictions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='Please remember that linear transformations, such as minmax ( https://\\nscikit-learn.org/stable/modules/generated/sklearn.preprocessing.\\nMinMaxScaler.html ) or standardization ( https://scikit-learn.org/stable/  \\nmodules/generated/sklearn.preprocessing.StandardScaler.html ), do not \\nchange the performance of any regressor, since they are linear transformations of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 136}, page_content='the target. Non-linear  transformations, such as the square root, the cubic root, the \\nlogarithm, the exponentiation, and their combinations, should instead definitely \\nmodify the performance of your regression model on the evaluation metric (hopefully \\nfor the better, if you decide on the right transformation).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='Chapter 5 111\\nMSE is a great instrument for  comparing regression models applied to the same problem.  The \\nbad news is that the MSE is seldom used in Kaggle competitions, since RMSE is preferred. In fact, \\nby taking the root of MSE, its value will resemble the original scale of your target and it will be'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='easier at a glance to figure out if your model is doing a good job or not. In addition, if you are con-\\nsidering the same regression model across different data problems (for instance, across various \\ndatasets or data competitions), R squared is better because it is perfectly correlated with MSE \\nand its values range between 0 and 1, making all comparisons easier.\\nRoot mean squared error (RMSE)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='RMSE is just the square root of MSE, but this implies some subtle change. Here is its formula:\\n𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 𝑅𝑅 √∑( 𝑦𝑦𝑦𝑖𝑖−𝑅𝑦𝑦𝑖𝑖)2\\n𝑛𝑛𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nIn the above formula, n indicates the number of cases, 𝑦𝑦𝑖𝑖  is the ground truth, and 𝑦𝑦𝑖𝑖̂  the prediction. \\nIn MSE, large prediction errors are greatly penalized because of the squaring activity. In RMSE,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='this dominance is lessened because of the root effect (however, you should always pay attention \\nto outliers; they can affect your model performance a lot, no matter whether you are evaluating \\nbased on MSE or RMSE). \\nConsequently, depending on the problem, you can get a better fit with an algorithm using MSE as'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='an objective function by first applying the square root to your target (if possible, because it requires \\npositive values), then squaring the results. Functions such as the TransformedTargetRegressor  \\nin Scikit-learn help you to appropriately transform your regression target in order to get better-fit -\\nting results with respect to your evaluation metric.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 137}, page_content='Recent competitions where RMSE has been used include:\\n• Avito Demand Prediction Challenge : https://www.kaggle.com/c/avito-\\ndemand-prediction\\n• Google Analytics Customer Revenue Prediction: https://www.kaggle.com/c/\\nga-customer-revenue-prediction\\n• Elo Merchant Category Recommendation  https://www.kaggle.com/c/elo-\\nmerchant-category-recommendation'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 138}, page_content='Competition Tasks and Metrics 112\\nRoot mean squared log error (RMSLE)\\nAnother common transformation of  MSE is  root mean squared log error  (RMSLE). MCRMSLE is \\njust a variant made popular by the COVID-19 forecasting competitions, and it is the column-wise \\naverage of the RMSLE values of each single target when there are multiple ones. Here is the for -\\nmula for RMSLE:\\n𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 𝑅𝑅 √1'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 138}, page_content='𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 𝑅𝑅 √1\\n𝑛𝑛∑(log(𝑦𝑦𝑖𝑖̂+1)−𝑅log𝑅(𝑦𝑦𝑖𝑖+1))2𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nIn the formula, n indicates the number of cases, 𝑦𝑦𝑖𝑖  is the ground truth, and 𝑦𝑦𝑖𝑖̂  the prediction. Since \\nyou are applying a logarithmic transformation to your predictions and your ground truth before \\nall the other squaring, averaging, and rooting operations, you don’t penalize huge differences'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 138}, page_content='between the predicted and the actual values, especially when both are large numbers. In other \\nwords, what you care the most about when using RMSLE is the scale of your predictions with respect \\nto the scale of the ground truth. As with RMSE, machine learning algorithms for regression can better \\noptimize for RMSLE if you apply a logarithmic transformation to the target before fitting it (and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 138}, page_content='then reverse the effect using the exponential function).\\nBy far, at the moment, RMSLE is the most used evaluation metric for regression in Kaggle com-\\npetitions.Recent competitions using RMSLE as an evaluation metric are:\\n• ASHRAE - Great Energy Predictor III : https://www.kaggle.com/c/ashrae-\\nenergy-prediction\\n• Santander Value Prediction Challenge: https://www.kaggle.com/c/'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 138}, page_content='santander-value-prediction-challenge\\n• Mercari Price Suggestion Challenge: https://www.kaggle.com/c/mercari-  \\nprice-suggestion-challenge\\n• Sberbank Russian Housing Market : https://www.kaggle.com/  \\nolgabelitskaya/sberbank-russian-housing-market\\n• Recruit Restaurant Visitor Forecasting: https://www.kaggle.com/c/  \\nrecruit-restaurant-visitor-forecasting'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='Chapter 5 113\\nMean absolute error (MAE)\\nThe MAE ( mean absolute error) evaluation metric is the absolute  value of the difference between \\nthe predictions and the targets. Here is the formulation of MAE:\\n𝑀𝑀𝑀𝑀𝑀𝑀 𝑀𝑀1\\n𝑛𝑛∑|𝑦𝑦𝑖𝑖̂−𝑀𝑦𝑦𝑖𝑖|𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nIn the formula, n stands for the number of cases, 𝑦𝑦𝑖𝑖  is the ground truth, and 𝑦𝑦𝑖𝑖̂  the prediction. MAE'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='is not particularly sensitive to outliers (unlike MSE, where errors are squared), hence you may \\nfind it is an evaluation metric in many competitions whose datasets present outliers. Moreover, \\nyou can easily work with it since many algorithms can directly use it as an objective function; \\notherwise, you can optimize for it indirectly by just training on the square root of your target and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='then squaring the predictions.\\nIn terms of downside, using MAE as an objective function results in much slower convergence, \\nsince you are actually optimizing for predicting the median of the target (also called the L1 norm), \\ninstead of the  mean (also called the L2 norm), as occurs by MSE minimization. This results in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='more complex computations for the optimizer, so the training time can even grow exponentially \\nbased on your number of training cases (see, for instance, this Stack Overflow question: https://\\nstackoverflow.com/questions/57243267/why-is-training-a-random-forest-regressor-\\nwith-mae-criterion-so-slow-compared-to ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='Having mentioned the ASHRAE competition earlier, we should also mention that regression eval -\\nuation measures are quite relevant to forecasting competitions. For instance, the M5 forecasting \\ncompetition was held recently ( https://mofc.unic.ac.cy/m5-competition/ ) and data from \\nall the other M competitions is available too. If you are interested in forecasting competitions, of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='which there are a few on Kaggle, please see https://robjhyndman.com/hyndsight/forecasting-\\ncompetitions/  for an overview about M competitions and how valuable Kaggle is for obtaining \\nbetter practical and theoretical results from such competitions. Notable recent competitions that used MAE as an evaluation metric are:\\n• LANL Earthquake Prediction: https://www.kaggle.com/c/LANL-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 139}, page_content='Earthquake-Prediction\\n• How Much Did It Rain? II: https://www.kaggle.com/c/how-much-did-\\nit-rain-ii'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='Competition Tasks and Metrics 114\\nEssentially, forecasting competitions do not require a very different evaluation to regression \\ncompetitions. When dealing with forecasting tasks, it is true that you can get some unusual eval-\\nuation metrics such as the Weighted Root Mean Squared Scaled Error  (https://www.kaggle.\\ncom/c/m5-forecasting-accuracy/overview/evaluation ) or the symmetric mean absolute'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='percentage error, better known as sMAPE (https://www.kaggle.com/c/demand-forecasting-\\nkernels-only/overview/evaluation ). However, in the end they are just variations of the usual \\nRMSE or MAE that you can handle using the right target transformations.\\nMetrics for classification (label prediction and \\nprobability)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='probability)\\nHaving discussed the  metrics for regression problems, we are going now to illustrate the metrics \\nfor classification problems, starting from the binary classification problems (when you have to \\npredict between two classes), moving to the multi-class (when you have more than two classes), \\nand then to the multi-label (when the classes overlap).\\nAccuracy'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='Accuracy\\nWhen analyzing the performance of a binary classifier, the most common and accessible metric \\nthat is used is accuracy. A misclassification error is when your model predicts the wrong class \\nfor an example. The accuracy is just the complement of the misclassification error and it can be \\ncalculated as the ratio between the number of correct numbers divided by the number of answers:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴 𝐴𝐴𝐴𝑐𝑐𝐴𝐴𝐴𝐴𝑐𝑐𝐴𝐴𝑐𝑐𝑐𝐴𝐴𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝐴𝐴𝑐𝑐\\n𝑐𝑐𝑐𝑐𝑐𝑐𝐴𝐴𝑡𝑡𝑐𝐴𝐴𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝐴𝐴𝑐𝑐𝑐 \\nAs a metric, the accuracy is  focused strongly on the effective performance of the model in a real \\nsetting: it tells you if the model works as expected. However, if your purpose is to evaluate and \\ncompare and have a clear picture of how effective your approach really is, you have to be cautious'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='when using the accuracy because it can lead to wrong conclusions when the classes are imbalanced \\n(when they have different  frequencies). For instance, if a certain class makes up just 10% of the \\ndata, a predictor that predicts nothing but the majority class will be 90% accurate, proving itself'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='quite useless in spite of the high accuracy.This metric has been used, for instance, in Cassava Leaf Disease Classification  \\n(https://www.kaggle.com/c/cassava-leaf-disease-classification ) and \\nText Normalization Challenge - English Language ( https://www.kaggle.com/c/\\ntext-normalization-challenge-english-language ), where you scored a cor -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 140}, page_content='rect prediction only if your predicted text matched the actual string.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 141}, page_content='Chapter 5 115\\nHow can you spot such a problem? You can do this  easily by using a confusion matrix. In a con-\\nfusion matrix, you create a two-way table comparing the actual classes on the rows against the \\npredicted classes on the columns. You can create a straightforward one using the Scikit-learn \\nconfusion_matrix  function:\\nsklearn.metrics.confusion_matrix('),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 141}, page_content='y_true, y_pred, *, labels= None, sample_weight= None,\\n    normalize= None\\n)\\nProviding the y_true  and y_pred vectors will suffice to return you a meaningful table, but you \\ncan also provide row/column labels and sample weights for the examples in consideration, and \\nnormalize (set the marginals to sum to 1) over the true examples (the rows), the predicted exam-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 141}, page_content='ples (the columns), or all the examples. A perfect classifier will have all the cases on the principal \\ndiagonal of the matrix. Serious problems with the validity of the predictor are highlighted if there \\nare few or no cases on one of the cells of the diagonal.\\nIn order to give you a better idea of how it works, you can try the graphical example offered'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 141}, page_content='by Scikit-learn at https://scikit-learn.org/stable/auto_examples/model_selection/\\nplot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-\\nmatrix-py :\\nFigure 5.1: Confusion matrix, with each cell normalized to 1.00, to represent the share of \\nmatches'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 142}, page_content='Competition Tasks and Metrics 116\\nYou can attempt to improve the  usability of the accuracy by considering the accuracy relative to \\neach of the classes and averaging them, but you will find it more useful to rely on other metrics \\nsuch as precision, recall, and the F1-score.\\nPrecision and recall\\nTo obtain the precision and recall metrics, we again start from the confusion matrix. First, we'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 142}, page_content='have to name each of the cells:\\nPredicted\\nNegative Positive\\nActualNegative True Negative False Positive\\nPositive False Negative True Positive\\nTable 5.1: Confusion matrix with cell names\\nHere is how we define the cells:\\n• TP ( true positives): These are located in the upper-left cell, containing examples that \\nhave correctly been predicted as positive ones.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 142}, page_content='• FP ( false positives): These are located in the upper-right cell, containing examples that \\nhave been predicted as positive but are actually negative.\\n• FN ( false negatives): These are located in the lower-left cell, containing examples that \\nhave been predicted as negative but are actually positive.\\n• TN ( true negatives): These are located in the lower-right cell, containing examples that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 142}, page_content='have been correctly predicted as negative ones.\\nUsing these cells, you can actually get more precise information about how your  classifier works \\nand how you can tune your model better. First, we can easily revise the accuracy formula:\\n𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴𝐴 𝐴𝐴(𝑇𝑇𝑇𝑇+𝑇𝑇𝑇𝑇)\\n(𝑇𝑇𝑇𝑇+𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇+𝐹𝐹𝑇𝑇)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 143}, page_content='Chapter 5 117\\nThen, the first informative metric is called precision  (or specificity ) and it is actually the accuracy \\nof the positive cases:\\n𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝑃𝑃𝑇𝑇𝑃𝑃\\n𝑇𝑇𝑃𝑃+𝐹𝐹𝑃𝑃 \\nIn the computation, only the number of true positives and the number of false positives are \\ninvolved. In essence, the metric tells you how often you are correct when you predict a positive.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 143}, page_content='Clearly, your model could get high scores by predicting positives for only the examples it has high \\nconfidence in. That is actually the purpose of the measure: to force models to predict a positive \\nclass only when they are sure and it is safe to do so.\\nHowever, if it is in your interest also to predict as many positives as possible, then you’ll also need'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 143}, page_content='to watch over the recall (or coverage  or sensitivity  or even true positive rate) metric:\\n𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅𝑅 𝑅𝑅𝑇𝑇𝑇𝑇\\n𝑇𝑇𝑇𝑇+𝐹𝐹𝐹𝐹 \\nHere, you will also need to know about false negatives. The interesting thing about these two met -\\nrics is that, since they are based on examples classification, and a classification is actually based'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 143}, page_content='on probability (which is usually set between the positive and negative class at the 0.5 threshold), \\nyou can change the threshold and have one of the two metrics  improved at the expense of the other. \\nFor instance, if you increase the threshold, you will get more precision (the classifier is more'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 143}, page_content='confident of the prediction) but less recall. If you decrease the threshold, you get less precision \\nbut more recall. This is also called the precision/recall trade-off .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 144}, page_content='Competition Tasks and Metrics 118\\nThe Scikit-learn website offers a simple and practical overview of this trade-off ( https://scikit-\\nlearn.org/stable/auto_examples/model_selection/plot_precision_recall.html ), helping \\nyou to trace a precision/recall curve and thus understand how these two measures can be ex -\\nchanged to obtain a result that better fits your needs:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 144}, page_content='Figure 5.2: A two-class precision-recall curve with its characteristic steps\\nOne metric associated with the precision/recall trade-off is  the average precision. Average preci-\\nsion computes the mean precision for recall values from 0 to 1 (basically, as you vary the threshold \\nfrom 1 to 0). Average precision is very popular for tasks related to object detection, which we'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 144}, page_content='will discuss a bit later on, but it is also very useful for classification in tabular data. In practice, \\nit proves valuable when you want to monitor model performance on a very rare class (when the \\ndata is extremely imbalanced) in a more precise and exact way, which is often the case with fraud \\ndetection problems.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 144}, page_content='detection problems. \\nFor more specific insights on this, read Gael Varoquaux’s  discussion: http://gael-varoquaux.\\ninfo/interpreting_ml_tuto/content/01_how_well/01_metrics.html#average-precision .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='Chapter 5 119\\nThe F1 score\\nAt this point, you have probably already figured out that using precision or recall as an evaluation \\nmetric is not an ideal choice because you can only optimize one at the expense of the other. For \\nthis reason, there are no Kaggle competitions that use only one of the two metrics. You should'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='combine them (as in the average precision). A single metric, the F1 score, which is the harmonic \\nmean of precision and recall, is commonly considered to be the best solution:\\n𝐹𝐹𝐹 𝐹 𝐹𝐹𝐹𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 𝐹𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝\\n𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 \\nIf you get a high F1 score, it is  because your model has improved in precision or recall or in both.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='You can find a fine example of the usage of this metric in the Quora Insincere Questions Classifica -\\ntion competition ( https://www.kaggle.com/c/quora-insincere-questions-classification ).\\nIn some competitions, you also get the F-beta score. This is simply the weighted harmonic mean \\nbetween precision and recall, and beta decides the weight of the recall in the combined score:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='𝐹𝐹𝛽𝛽=(1 + 𝛽𝛽2)∗ (𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 ∗𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 )\\n(𝛽𝛽2∗ 𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 +𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 ) \\nSince we have already introduced the concept of threshold and classification probability, we can \\nnow discuss the log loss and ROC-AUC, both quite common classification metrics.\\nLog loss and ROC-AUC\\nLet’s start with the log loss , which is also known as cross-entropy  in deep learning models. The'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='log loss is the difference between the  predicted probability and the ground truth probability:\\n𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿𝐿 𝐿𝐿𝐿1\\n𝑛𝑛∑[𝑦𝑦𝑖𝑖log(𝑦𝑦𝑖𝑖̂)+𝐿(1𝐿𝐿𝑦𝑦𝑖𝑖)log𝐿(1𝐿𝐿𝑦𝑦𝑖𝑖̂)]𝑛𝑛\\n𝑖𝑖𝑖𝑖 \\nIn the above formula, n stands for the number of examples, 𝑦𝑦𝑖𝑖  is the ground truth for the ith case, \\nand 𝑦𝑦𝑖𝑖̂  the prediction.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='and 𝑦𝑦𝑖𝑖̂  the prediction.\\nIf a competition uses the log loss, it is implied that the objective is to estimate as correctly as \\npossible the probability of an example being of a positive class. You can actually find the log loss \\nin quite a lot of competitions. \\nWe suggest you have a look, for instance, at the recent Deepfake Detection Challenge ( https://www.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 145}, page_content='kaggle.com/c/deepfake-detection-challenge ) or at the older Quora Question Pairs ( https://\\nwww.kaggle.com/c/quora-question-pairs ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 146}, page_content='Competition Tasks and Metrics 120\\nThe ROC curve, or receiver operating characteristic curve, is a graphical chart used to evaluate \\nthe performance of a binary classifier and to compare multiple classifiers. It is the building block \\nof the ROC-AUC metric, because the metric is simply the area delimited under the ROC curve.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 146}, page_content='The ROC curve consists of the true positive rate (the recall) plotted against the false positive rate \\n(the ratio of negative instances that are incorrectly classified as positive ones). It is equivalent \\nto one minus the true negative rate (the ratio of negative examples that are correctly classified). \\nHere are a few examples:\\nFigure 5.3: Different ROC curves and their AUCs'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 146}, page_content='Ideally, a ROC curve of a  well-performing classifier should quickly climb up the true positive \\nrate (recall) at low values of the false positive rate. A ROC-AUC between 0.9 to 1.0 is considered \\nvery good.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='Chapter 5 121\\nA bad classifier can be spotted by the ROC curve appearing very similar, if not identical, to the \\ndiagonal of the chart, which represents the performance of a purely random classifier, as in the \\ntop left of the figure above; ROC-AUC scores near 0.5 are considered to be almost random results.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='If you are comparing different classifiers, and you are using the area under the curve ( AUC), the \\nclassifier with the higher area is the more performant one.\\nIf the classes are balanced, or not too imbalanced, increases in the AUC are proportional to the \\neffectiveness of the trained model and they can be intuitively thought of as the ability of the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='model to output higher probabilities for true positives. We also think of it as the ability to order \\nthe examples more properly from positive to negative. However, when the positive class is rare, \\nthe AUC starts high and its increments may mean very little in terms of predicting the  rare class \\nbetter. As we mentioned before, in such a case, average precision is a more helpful metric.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='You can read a detailed treatise in the following paper: Su, W., Yuan, Y., and Zhu, M. A relationship \\nbetween the average precision and the area under the ROC curve.  Proceedings of the 2015 International \\nConference on The Theory of Information Retrieval. 2015.\\nMatthews correlation coefficient (MCC)\\nWe complete our overview of binary classification metrics with the Matthews correlation coeffi-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='cient (MCC), which made its appearance in VSB Power Line Fault Detection ( https://www.kaggle.\\ncom/c/vsb-power-line-fault-detection ) and Bosch Production Line Performance ( https://www.\\nkaggle.com/c/bosch-production-line-performance ).\\nThe formula for the MCC is:\\n𝑀𝑀𝑀𝑀𝑀𝑀 = (𝑇𝑇𝑇𝑇∗𝑇𝑇𝑇𝑇)− (𝐹𝐹𝑇𝑇∗𝐹𝐹𝑇𝑇)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='√(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇)∗(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇)∗(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇)∗(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇) AUC has recently been used for quite a lot of different competitions. We suggest you \\nhave a look at these three:\\n• IEEE-CIS Fraud Detection: https://www.kaggle.com/c/ieee-fraud-  \\ndetection\\n• Riiid Answer Correctness Prediction: https://www.kaggle.com/c/riiid-\\ntest-answer-prediction'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 147}, page_content='test-answer-prediction\\n• Jigsaw Multilingual Toxic Comment Classification : https://www.kaggle.\\ncom/c/jigsaw-multilingual-toxic-comment-classification/'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='Competition Tasks and Metrics 122\\nIn the above formula, TP stands for true positives, TN for true negatives, FP for false positives, and \\nFN for false negatives. It is the same nomenclature as we met when discussing precision and recall.\\nBehaving as a correlation coefficient, in other words, ranging from +1 (perfect prediction) to -1'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='(inverse prediction), this metric can be considered a measure of the quality of the classification \\neven when the classes are quite imbalanced.\\nIn spite of its complexity, the formula can be reformulated and simplified, as demonstrated by \\nNeuron Engineer ( https://www.kaggle.com/ratthachat ) in his Notebook: www.kaggle.com/\\nratthachat/demythifying-matthew-correlation-coefficients-mcc .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='The work done by Neuron Engineer in understanding the ratio of the evaluation metric is indeed \\nexemplary. In fact, his reformulated MCC becomes:\\n𝑀𝑀𝑀𝑀𝑀𝑀 =(𝑃𝑃𝑃𝑃𝑠𝑠𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 + 𝑁𝑁𝑁𝑁𝑔𝑔𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 − 1) ∗  𝑃𝑃𝑃𝑃𝑠𝑠𝑁𝑁𝑁𝑁𝑔𝑔 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃  \\nWhere each element of the formula is:\\n𝑃𝑃𝑃𝑃𝑠𝑠𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 = 𝑇𝑇𝑃𝑃\\n𝑇𝑇𝑃𝑃+𝐹𝐹𝑃𝑃 \\n𝑁𝑁𝑁𝑁𝑔𝑔𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝𝑝 = 𝑇𝑇𝑁𝑁\\n𝑇𝑇𝑁𝑁+𝐹𝐹𝑁𝑁'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='𝑇𝑇𝑁𝑁+𝐹𝐹𝑁𝑁 \\n𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝑃𝑃 √𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃\\n𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 \\n𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃 𝑃 𝑇𝑇𝑃𝑃+𝐹𝐹𝑃𝑃 \\n𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁𝑁 𝑁 𝑇𝑇𝑁𝑁+𝐹𝐹𝑁𝑁 \\nThe reformulation helps to clarify, in a more intelligible form than the original, that you can get'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='higher performance from improving both positive and negative class precision, but that’s not \\nenough: you also have to have positive and negative predictions in proportion to the ground truth, \\nor your submission will be greatly penalized.\\nMetrics for multi-class classification\\nWhen moving to multi-class  classification, you simply use the binary classification metrics that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 148}, page_content='we have just seen, applied to each class, and then you summarize them using some of the aver -\\naging strategies that are commonly used for multi-class situations.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='Chapter 5 123\\nFor instance, if you want to evaluate your solution based on the F1 score, you have three possible \\naveraging choices:\\n• Macro averaging: Simply  calculate the F1 score for each class and then average all the \\nresults. In this way, each class will count as much the others, no matter how frequent its'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='positive cases are or how important they are for your problem, resulting therefore in equal \\npenalizations when the model doesn’t perform well with any class:\\n𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 𝑚𝑚𝐹𝐹𝐹𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐+𝐹𝐹𝐹𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐+⋯+𝐹𝐹𝐹 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐\\n𝑁𝑁 \\n• Micro averaging: This approach will sum all the contributions from each class to com-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='pute an aggregated F1 score. It results in no particular favor to or penalization of any class, \\nsince all the computations are made regardless of each class, so it can more accurately \\naccount for class imbalances:\\n𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚𝑚 𝑚 𝑚𝑚𝑚 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐  \\n• Weighting: As with macro averaging, you first calculate the F1 score for each class, but'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='then you make a weighted average mean of all of them using a weight that depends on \\nthe number of true labels of each class. By using such a set of weights, you can take into \\naccount the frequency of positive cases from each class or the relevance of that class for \\nyour problem. This approach clearly favors the majority classes, which will be weighted \\nmore in the computations:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='more in the computations:\\n𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤𝑤 𝑡𝑡𝑤𝑤𝑡𝑡 =  𝐹𝐹𝐹 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 ∗ 𝑊𝑊 𝑐+ 𝐹𝐹𝐹 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 ∗𝑊𝑊 𝑐+⋯+𝐹𝐹 𝐹 𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐𝑐 ∗𝑊𝑊 𝑛𝑛 \\n𝑊𝑊1+𝑊𝑊2+⋯+𝑊𝑊 𝑁𝑁= 1.0  \\nCommon multi-class metrics that you may encounter in Kaggle competitions are:\\n• Multiclass accuracy (weighted): Bengali.AI Handwritten Grapheme Classification ( https://\\nwww.kaggle.com/c/bengaliai-cv19 )'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='• Multiclass log loss (MeanColumnwiseLogLoss): Mechanisms of Action (MoA) Prediction \\n(https://www.kaggle.com/c/lish-moa/ )\\n• Macro-F1  and Micro-F1 (NQMicroF1): University of Liverpool - Ion Switching  (https://\\nwww.kaggle.com/c/liverpool-ion-switching ), Human Protein Atlas Image Classification  \\n(https://www.kaggle.com/c/human-protein-atlas-image-classification/ ), Ten-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 149}, page_content='sorFlow 2.0 Question Answering ( https://www.kaggle.com/c/tensorflow2-question-\\nanswering )'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='Competition Tasks and Metrics 124\\n• Mean-F1: Shopee - Price Match Guarantee ( https://www.kaggle.com/c/shopee-product-\\nmatching/ ). Here, the F1 score is calculated for every predicted  row, then averaged, whereas \\nthe Macro-F1 score is defined as the mean of class-wise/label-wise F1 scores.\\nThen there is also Quadratic Weighted Kappa , which we will explore later on as a smart eval-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='uation metric for ordinal prediction problems. In its simplest form, the  Cohen Kappa  score, it \\njust measures the agreement between your predictions and the ground truth. The metric was \\nactually created for measuring inter-annotation agreement, but it is really versatile and has \\nfound even better uses.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='found even better uses.\\nWhat is inter-annotation agreement? Let’s imagine that you have a labeling task: classifying \\nsome photos based on whether they contain an image of a cat, a dog, or neither. If you ask a set \\nof people to do the task for you, you may incur some erroneous labels because someone (called'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='the judge in this kind of task) may misinterpret a dog as a cat or vice versa. The smart way to do \\nthis job correctly is to divide the work among multiple judges labeling the same photos, and then \\nmeasure their level of agreement based on the Cohen Kappa score.\\nTherefore, the Cohen Kappa is devised as a score expressing the level of agreement between two'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='annotators on a labeling (classification) problem:\\n𝑘𝑘 𝑘 𝑘𝑘𝑘 0−𝑘𝑘 𝑒𝑒)/𝑘1 −  𝑘𝑘𝑒𝑒) \\nIn the formula, p0 is the relative observed agreement among raters, and pe is the hypothetical prob -\\nability of chance agreement. Using the confusion matrix nomenclature, this can be rewritten as:\\n𝑘𝑘𝑘𝑘2∗(𝑇𝑇𝑇𝑇∗𝑇𝑇𝑇𝑇−𝐹𝐹𝑇𝑇∗𝐹𝐹𝑇𝑇)\\n(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇)∗(𝐹𝐹𝑇𝑇+𝑇𝑇𝑇𝑇)+(𝑇𝑇𝑇𝑇+𝐹𝐹𝑇𝑇)∗(𝐹𝐹𝑇𝑇+𝑇𝑇𝑇𝑇)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='The interesting aspect of this formula is that the score takes into account the empirical probability \\nthat the agreement has happened just by chance, so the measure has a correction for all the most \\nprobable classifications. The metric ranges from 1, meaning complete agreement, to -1, meaning \\nthe judges completely oppose each other (total disagreement).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 150}, page_content='Values around 0 signify that agreement and disagreement among the judges is happening by \\nmere chance. This helps you figure out if the model is really performing better than chance in \\nmost situations.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='Chapter 5 125\\nAndrey Lukyanenko\\nhttps://www.kaggle.com/artgor\\nOur second interview of the chapter is with Andrey Lukyanenko, a Note -\\nbooks and Discussions Grandmaster and Competitions Master. In his \\nday job, he is a Machine Learning Engineer and TechLead at MTS Group. \\nHe had many interesting things to say about his Kaggle experiences!'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='What’s your favourite kind of competition and why? In terms of \\ntechniques, solving approaches, what is your specialty on Kaggle?\\nI prefer competitions where solutions can be general enough to be transferable to other datasets/domains. \\nI’m interested in trying various neural net architectures, state-of-the-art approaches, and post-processing'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='tricks. I don’t favor those competitions that require reverse engineering or creating some “golden features,” \\nas these approaches won’t be applicable in other datasets.\\nWhile you were competing on Kaggle, you also became a \\nGrandmaster in Notebooks (and ranked number one) and Discussions. \\nHave you invested in these two objectives?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='I have invested a lot of time and effort into writing Notebooks, but the Discussion Grandmaster rank \\nhappened kind of on its own.\\nLet’s start with the Notebook ranking.\\nThere was a special competition in 2018 called DonorsChoose.org Application Screening. DonorsChoose \\nis a fund that empowers public school teachers from across the country to request much-needed materials'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='and experiences for their students. It organized a competition, where the winning solutions were based \\nnot on the score on the leaderboard, but on the number of the upvotes on the Notebook. This looked inter -\\nesting and I wrote a Notebook for the competition. Many participants advertised their analysis on social'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='media and I did the same. As a result, I reached second place and won a Pixelbook (I’m still using it!).\\nI was very motivated by this success and continued writing Notebooks. At first, I simply wanted to share \\nmy analysis and get feedback, because I wanted to try to compare my analytics and visualization skills'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='with other people to see what I could do and what people thought of it. People started liking my kernels \\nand I wanted to improve my skills even further. Another motivation was a desire to improve my skill at \\nmaking a quick MVP (minimum viable product). When a new competition starts, many people begin'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 151}, page_content='writing Notebooks, and if you want to be one of the first, you have to be able to do it fast without sacrificing \\nquality. This is challenging, but fun and rewarding.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='Competition Tasks and Metrics 126\\nI was able to get the Notebook Grandmaster rank in the February of 2019; after some time, I reached first \\nplace and held it for more than a year. Now I write Notebooks less frequently, but I still enjoy doing it.\\nAs for discussions, I think it kind of happened on its own. I answered the comments on my Notebooks, and'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='shared and discussed ideas about competitions in which I took part, and my discussion ranking steadily \\nincreased.\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.\\nIt was the Predicting Molecular Properties competition. I have written a blog post about it in more detail'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='here (https://towardsdatascience.com/a-story-of-my-first-gold-medal-in-one-kaggle-\\ncompetition-things-done-and-lessons-learned-c269d9c233d1 ). It was a domain-specific com-\\npetition aimed at predicting interactions between atoms in molecules. Nuclear Magnetic Resonance \\n(NMR) is a technology that uses principles similar to MRI to understand the structure and dynamics of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='proteins and molecules. Researchers around the world conduct NMR experiments to further understand \\nthe structure and dynamics of molecules, across areas like environmental science, pharmaceutical science, \\nand materials science. In this competition, we tried to predict the magnetic interaction between two atoms'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='in a molecule (the scalar coupling constant). State-of-the-art methods from quantum mechanics can \\ncalculate these coupling constants given only a 3D molecular structure as input. But these calculations \\nare very resource-intensive, so can’t be always used. If machine learning approaches could predict these'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='values, it would really help medicinal chemists to gain structural insights faster and more cheaply.\\nI usually write EDA kernels for new Kaggle competitions, and this one was no exception. A common \\napproach for tabular data in Kaggle competitions is extensive feature engineering and using gradient'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='boosting models. I used LGBM too in my early attempts, but knew that there should be better ways to \\nwork with graphs. I realized that domain expertise would provide a serious advantage, so I hunted for \\nevery piece of such information. Of course, I noticed that there were several active experts, who wrote on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='the forum and created kernels, so I read everything from them. And one day I received an e-mail from an \\nexpert in this domain who thought that our skills could complement each other. Usually, I prefer to work \\non competitions by myself for some time, but in this case, combining forces seemed to be a good idea to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='me. And this decision turned out to be a great one! With time we were able to gather an amazing team.\\nAfter some time, we noticed a potential for neural nets in the competition: a well-known Kaggler, Heng, \\nposted an example of an MPNN (Message Passing Neural Network) model. After some time, I was even'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 152}, page_content='able to run it, but the results were worse compared to our models. Nevertheless, our team knew that we \\nwould need to work with these Neural Nets if we wanted to aim high. It was amazing to see how Christof \\nwas able to build new neural nets extremely fast. Soon, we focused only on developing those models.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='Chapter 5 127\\nAfter that, my role switched to a support one. I did a lot of experiments with our neural nets: trying various \\nhyperparameters, different architectures, various little tweaks to training schedules, and so on. Sometimes \\nI did EDA on our predictions to find our interesting or wrong cases, and later we used this information \\nto improve our models even further.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='We got the 8th place and I learned a lot during this competition.\\nHas Kaggle helped you in your career? If so, how?\\nKaggle definitely helped me a lot, especially with my skills and my personal brand. Writing and publishing \\nKaggle Notebooks taught me not only EDA and ML skills, but it forced me to become adaptable, to be able'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='to understand new topics and tasks quickly, to iterate more efficiently between approaches. At the same \\ntime, it provided a measure of visibility for me, because people appreciated my work.\\nMy first portfolio ( https://erlemar.github.io/ ) had a lot of different Notebooks, and half of them \\nwere based on old Kaggle competitions. It was definitely helpful in getting my first jobs. My Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='achievements also helped me attract recruiters from good companies, sometimes even to skip steps of the \\ninterview process, and even led me to several consulting gigs.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='started?\\nI think we need to separate inexperienced Kagglers into two groups: those who are inexperienced in data \\nscience in general and those who are inexperienced on Kaggle.\\nThose who are inexperienced in general make a number of different mistakes (and it is okay, everyone \\nstarted somewhere):\\n• One of the most serious problems: lack of critical thinking and not knowing how to do their own'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='research;\\n• Not knowing when and what tools/approaches to use;\\n• Blindly taking public Notebooks and using them without understanding how they work;\\n• Fixating on a certain idea and spending too much time pursuing it, even when it doesn’t work;\\n• Despairing and losing motivation when their experiments fail.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='As for those people who have experience in data science but don’t have experience with Kaggle, I’d say \\nthat the most serious thing they overlook is that they underestimate Kaggle’s difficulty. They don’t expect \\nKaggle to be very competitive, that you need to try many different things to succeed, that there are a lot of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 153}, page_content='tricks that work only in competitions, that there are people who professionally participate in competitions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='Competition Tasks and Metrics 128\\nAlso, people often overestimate domain expertise. I admit that there were a number of competitions \\nwhen the teams with domain experts in them won gold medals and prizes, but in most cases experienced \\nKagglers triumph.\\nAlso, I have seen the following situation many times: some person proclaims that winning Kaggle is easy,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='and that he (or his group of people) will get a gold medal or many gold medals in the recent future. In \\nmost cases, they silently fail.\\nWhat mistakes have you made in competitions in the past?\\n• Not enough looking in the data. Sometimes I wasn’t able to generate better features or apply \\nbetter postprocessing due to this. And reserve engineering and “golden features” is a whole ad -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='ditional topic.\\n• Spending too much time on a single idea because I hoped it would work. This is called sunk-cost \\nfallacy.\\n• Not enough experiments. The effort pays off – if you don’t spend enough time and resources on \\nthe competition, you won’t get a high place on a leaderboard.\\n• Entering “wrong” competitions. There were competitions with leaks, reverse engineering, etc.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='There were competitions with an unreasonable split between public and private test data and a \\nshake-up ensured. There were competitions that weren’t interesting enough for me and I shouldn’t \\nhave started participating in them.\\n• Teaming up with the wrong people. There were cases when my teammates weren’t as active as I \\nexpected a nd it led to a worse team score.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='What’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nI think it is important to remember your goal, know what are you ready to invest into this competition, \\nand think about the possible outcomes. There are many possible goals that people have while entering \\na competition:\\n• Winning money or getting a medal;'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 154}, page_content='• Getting new skills or improving existing ones;\\n• Working with a new task/domain;\\n• Networking;\\n• PR;\\n• etc;\\nOf course, it is possible to have multiple motivations.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='Chapter 5 129\\nMetrics for object detection problems\\nIn recent years, deep learning competitions have become more and more common on Kaggle. Most \\nof these competitions, focused on image recognition or on natural language processing tasks, have \\nnot required the use of evaluation metrics much different from the ones we have explored up to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='now. However, a couple of specific problems have required some special metric to be evaluated \\ncorrectly: those relating to object detection and segmentation .\\nFigure 5.4: Computer vision tasks. (Source: https://cocodataset.org/#explore?id=38282, https://\\ncocodataset.org/#explore?id=68717)\\nIn object detection, you don’t have to classify an image, but instead find relevant portions of a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='picture and label them accordingly. For instance, in Figure 5.4, an object detection classifier has \\nbeen entrusted to locate within a photo the portions of the  picture where either dogs or cats are \\npresent and classify each of them with a proper label. The example on the left shows the localiza -\\ntion of a cat using a rectangular box (called a bounding box). The example on the right presents'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='how multiple cats and dogs are detected in the picture by bounding boxes and then correctly \\nclassified (the blue bounding boxes are for dogs, the red ones for cats).As for what are you ready to invest, it is usually about the amount of time and effort you are ready to \\nspend as well as the hardware that you have.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='When I speak about the outcomes, I mean what will happen when the competition ends. It is possible that \\nyou will invest a lot in this competition and win, but you could also lose. Are you ready for this reality? Is \\nwinning a particular competition critical to you? Maybe you need to be prepared to invest more effort;'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 155}, page_content='on the other hand, maybe you have long-term goals and one failed competition won’t hurt much.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='Competition Tasks and Metrics 130\\nIn segmentation, you instead have a classification at the pixel level, so if you have  a 320x200 \\nimage, you actually have to make 64,000 pixel classifications. Depending on the task, you can \\nhave a semantic segmentation  where you have to classify every pixel in a photo, or an instance'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='segmentation  where you only have to classify the pixels representing objects of a certain type of \\ninterest (for instance, a cat as in our example in Figure 5.5 below):\\nFigure 5.5: Semantic segmentation and instance segmentation on the same image. (Source: \\nhttps://cocodataset.org/#explore?id=338091)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='Let’s start with an overview of the specific metrics for these tasks, metrics that can work well for \\nboth problems, since, in both cases, you are predicting entire areas (rectangular ones in object \\ndetection, polygonal ones in segmentation) of a picture and you have to compare your predictions'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='against a ground truth, which is, again, expressed as areas. On the side of segmentation, the easiest \\nmetric is the pixel accuracy , which, as the name suggests, is the accuracy on the pixel classification. In order to describe the spatial location of an object, in object detection we use \\nbounding boxes, which define a rectangular area in which the object lies. A bounding'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='box is usually specified using two (x, y) coordinates: the upper-left and lower-right \\ncorners. In terms of a machine learning algorithm, finding the coordinates of bound -\\ning boxes corresponds to applying a regression problem to multiple targets. However, \\nyou probably won’t frame the problem from scratch but rely on pre-built and often'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 156}, page_content='pre-trained models such as Mask R-CNN ( https://arxiv.org/abs/1703.06870 ), \\nRetinaNet ( https://arxiv.org/abs/2106.05624v1 ), FPN (https://arxiv.org/\\nabs/1612.03144v2 ), YOLO ( https://arxiv.org/abs/1506.02640v1 ), Faster \\nR-CNN (https://arxiv.org/abs/1506.01497v1 ), or SDD ( https://arxiv.org/\\nabs/1512.02325 ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='Chapter 5 131\\nIt is not a great metric because, as happens with accuracy on binary and multi-class problems, \\nyour score may look great if the relevant pixels do not take up very much of the image (you just \\npredict the majority claim, thus you don’t segment).\\nTherefore, there are two metrics that are used much more, especially in competitions: the inter-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='section over union  and the dice coefficient.\\nIntersection over union (IoU)\\nThe intersection over union  (IoU) is also known as the Jaccard index. When used in segmentation  \\nproblems, using IoU implies that you have two images to compare: one is your prediction and the \\nother is the mask revealing the ground truth, which is usually a binary matrix where the value 1'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='stands for the ground truth and 0 otherwise. In the case of multiple objects, you have multiple \\nmasks, each one labeled with the class of the object.\\nWhen used in object detection problems, you have the boundaries of two rectangular areas (those \\nof the prediction and the ground truth), expressed by the coordinates of their vertices. For each'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='classified class, you compute the area of overlap between your prediction and the ground truth \\nmask, and then you divide this by the area of the union between your prediction and the ground \\ntruth, a sum that takes into account any overlap. In this way, you are proportionally penalized \\nboth if you predict a larger area than what it should be (the denominator will be larger) or a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='smaller one (the numerator will be smaller):\\nFigure 5.6: Visual representation of the IoU calculation\\nIn Figure 5.6 you can see a visual representation of the areas involved in the computation. By \\nimagining the squares overlapping more, you can figure out how the metric efficiently penalizes \\nyour solution when your prediction, even if covering the ground truth, exceeds it (the area of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 157}, page_content='union becomes larger).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 158}, page_content='Competition Tasks and Metrics 132\\nDice\\nThe other useful metric is the Dice coefficient , which is the area of overlap between the prediction \\nand ground  truth doubled and then divided by the sum of the prediction and ground truth areas:\\nFigure 5.7: Visual representation of the Dice calculation\\nIn this case, with respect to the Jaccard index, you do not take into account the overlap of the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 158}, page_content='prediction with the ground truth in the denominator. Here, the expectation is that, as you maxi -\\nmize the area of overlap, you predict the correct area size. Again, you are penalized if you predict \\nareas larger than you should be predicting. In fact, the two metrics are positively correlated and \\nthey produce almost the same results for a single classification problem.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 158}, page_content='The differences actually arise when you are working with multiple classes. In fact, both with IoU \\nand the Dice coefficient, when you have multiple classes you average the result of all of them. \\nHowever, in doing so, the IoU metric tends to penalize the overall average more if a single class \\nprediction is wrong, whereas the Dice coefficient is more lenient and tends to represent the av -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 158}, page_content='erage performance.Here are some examples of competitions where IoU has been used:\\n• TGS Salt Identification Challenge ( https://www.kaggle.com/c/tgs-salt-\\nidentification-challenge/ ) with Intersection Over Union Object Seg -\\nmentation\\n• iMaterialist (Fashion) 2019 at FGVC6 ( https://www.kaggle.com/c/\\nimaterialist-fashion-2019-FGVC6 ) with Intersection Over Union Ob -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 158}, page_content='ject Segmentation With Classification\\n• Airbus Ship Detection Challenge ( https://www.kaggle.com/c/airbus-\\nship-detection ) with Intersection Over Union Object Segmentation Beta'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='Chapter 5 133\\nIoU and Dice constitute the basis for all the more complex metrics in segmentation and object \\ndetection. By choosing an appropriate threshold level for IoU or Dice (usually 0.5), you can de -\\ncide whether  or not to confirm a detection, therefore a classification. At this point, you can use'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='previously discussed metrics for classification, such as precision, recall, and F1, such as is done in \\npopular object detection and segmentation challenges such as Pascal VOC ( http://host.robots.\\nox.ac.uk/pascal/VOC/voc2012 ) or COCO ( https://cocodataset.org ).\\nMetrics for multi-label classification and \\nrecommendation problems'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='recommendation problems\\nRecommender systems are one of the most popular applications of data analysis and machine \\nlearning, and there are quite a few competitions on Kaggle that have used the recommendation \\napproach. For instance, the Quick, Draw! Doodle Recognition Challenge was a prediction evaluated \\nas a recommender system. Some other competitions on Kaggle, however, truly strived to build'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='effective recommender systems (such as Expedia Hotel Recommendations : https://www.kaggle.\\ncom/c/expedia-hotel-recommendations ) and RecSYS, the conference on recommender sys -\\ntems (https://recsys.acm.org/ ), even hosted one of its yearly contests on Kaggle (RecSYS 2013: \\nhttps://www.kaggle.com/c/yelp-recsys-2013 ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='Mean Average Precision at K (MAP@{K}) is typically  the metric of choice for evaluating the \\nperformance of recommender systems, and it is the most common metric you will encounter on \\nKaggle in all the competitions that try to build or approach a problem as a recommender system. Examples of Kaggle competitions using the Dice coefficient (it is often encountered'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='in competitions with medical purposes, but not necessarily only there, because it \\ncan also be used for clouds and cars):\\n• HuBMAP - Hacking the Kidney: https://www.kaggle.com/c/hubmap-\\nkidney-segmentation\\n• Ultrasound Nerve Segmentation: https://www.kaggle.com/c/ultrasound-\\nnerve-segmentation\\n• Understanding Clouds from Satellite Images : https://www.kaggle.com/c/'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 159}, page_content='understanding_cloud_organization\\n• Carvana Image Masking Challenge: https://www.kaggle.com/c/carvana-\\nimage-masking-challenge'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='Competition Tasks and Metrics 134\\nThere are also some other metrics, such as the precision at k , or P@K, and the average precision \\nat k, or AP@K, which are loss functions, in other words, computed at the level of each single \\nprediction. Understanding how they work can help you better understand the MAP@K and how \\nit can perform both in recommendations and in multi-label classification.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='In fact, analogous to recommender systems, multi-label classifications imply that your model \\noutputs a series of class predictions. Such results could be evaluated using some average of some \\nbinary classification metrics (such as in Greek Media Monitoring Multilabel Classification (WISE \\n2014), which used the mean F1 score: https://www.kaggle.com/c/wise-2014 ) as well as metrics'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='that are more typical of recommender systems, such as MAP@K. In the end, you can deal with \\nboth recommendations and multi-label predictions as ranking tasks , which translates into a set \\nof ranked suggestions in a recommender system and into a set of labels (without a precise order) \\nin multi-label classification.\\nMAP@{K}'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='MAP@{K}\\nMAP@K is a complex metric and it derives from many computations. In order to understand \\nthe MAP@K metric fully, let’s start with its simplest component, the precision at k (P@K). In \\nthis case, since the prediction for an example is a ranked sequence of predictions (from the most \\nprobable to the least), the function takes into account only the top k predictions, then it computes'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='how many matches it got with respect to the ground truth and divides that number by k. In a few \\nwords, it is quite similar to an accuracy measure averaged over k  predictions.\\nA bit more complex in terms of computation, but conceptually simple, the average precision at k \\n(AP@K) is the average of P@K computed over all the values ranging from 1 to k. In this way, the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='metric evaluates how well the prediction works overall, using the top prediction, then the top \\ntwo predictions, and so on until the top k  predictions.\\nFinally, MAP@K is the mean of the AP@K for the entire predicted sample, and it is a metric be -\\ncause it comprises all the predictions in its evaluation. Here is the MAP@5 formulation you can'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 160}, page_content='find in the Expedia Hotel Recommendations  competition ( https://www.kaggle.com/c/expedia-\\nhotel-recommendations ):\\n𝑀𝑀𝑀𝑀𝑀𝑀@5=1\\n|𝑈𝑈|∑∑𝑀𝑀 (𝑘𝑘)min\\u2061(5,𝑛𝑛 )\\n𝑘𝑘𝑘𝑘|𝑈𝑈|\\n𝑢𝑢𝑘𝑘 \\nIn the formula, |𝑈𝑈|  is the number of user recommendations, P(k) is the precision at cutoff k, and n is \\nthe number of predicted hotel clusters (you could predict up to 5 hotels for each recommendation).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='Chapter 5 135\\nIt is clearly a bit more daunting than our explanation, but the formula just expresses that the \\nMAP@K is the mean of all the AP@K evaluations over all the predictions.\\nHaving completed this overview of specific metrics for different regression and classification \\nmetrics, let’s discuss how to deal with evaluation metrics in a Kaggle competition.\\nOptimizing evaluation metrics'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='Optimizing evaluation metrics\\nSumming up what we have discussed so far, an objective function is a function inside your learning \\nalgorithm that measures how well the algorithm’s internal model is fitting the provided data. The \\nobjective function also provides feedback to the algorithm in order for it to improve its fit across'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='successive iterations. Clearly, since the entire algorithm’s efforts are recruited to perform well \\nbased on the objective function, if the Kaggle evaluation metric perfectly matches the objective \\nfunction of your algorithm, you will get the best results.\\nUnfortunately, this is not frequently the case. Often, the evaluation metric provided can only be'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='approximated by existing objective functions. Getting a good approximation, or striving to get \\nyour predictions performing better with respect to the evaluation criteria, is the secret to perform -\\ning well in Kaggle competitions. When your objective function does not match your evaluation \\nmetric, you have a few alternatives:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='1. Modify your learning algorithm and have it incorporate an objective function that matches \\nyour evaluation metric, though this is not possible for all algorithms (for instance, algo -\\nrithms such as LightGBM and XGBoost allow you to set custom objective functions, but \\nmost Scikit-learn models don’t allow this).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='2. Tune your model’s hyperparameters, choosing the ones that make the result shine the \\nmost when using the evaluation metric.\\n3. Post-process your results so they match the evaluation criteria more closely. For instance, \\nyou could code an optimizer that performs transformations on your predictions (proba-\\nbility calibration algorithms are an example, and we will discuss them at the end of the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='chapter).\\nHaving the competition metric incorporated into your machine learning algorithm is really the \\nmost effective method to achieve better predictions, though only a few algorithms can be hacked \\ninto using the competition metric as your objective function. The second approach is therefore the \\nmore common one, and many competitions end up in a struggle to get the best hyperparameters'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 161}, page_content='for your models to perform on the evaluation metric.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='Competition Tasks and Metrics 136\\nIf you already have your evaluation function coded, then doing the right cross-validation or choos -\\ning the appropriate test set plays the lion share. If you don’t have the coded function at hand, you \\nhave to first code it in a suitable way, following the formulas provided by Kaggle.\\nInvariably, doing the following will make the difference:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='• Looking for all the relevant information about the evaluation metric and its coded func-\\ntion on a search engine\\n• Browsing through the most common packages (such as Scikit-learn: https://scikit-\\nlearn.org/stable/modules/model_evaluation.html#model-evaluation  or TensorFlow: \\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses )'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='• Browsing GitHub projects (for instance, Ben Hammer’s Metrics project: https://github.\\ncom/benhamner/Metrics )\\n• Asking or looking around in the forums and available Kaggle Notebooks (both for the \\ncurrent competition and for similar competitions)\\n• In addition, as we mentioned before, querying the Meta Kaggle dataset ( https://www.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='kaggle.com/kaggle/meta-kaggle ) and looking in the Competitions  table will help you \\nfind out which other Kaggle competitions used that same evaluation metric, and imme -\\ndiately provides you with useful code and ideas to try out\\nLet’s discuss in greater detail the alternatives you have when your evaluation metric doesn’t'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='match your algorithm’s objective function. We’ll start by exploring custom metrics.\\nCustom metrics and custom objective functions\\nAs a first option when your objective function does not match your evaluation metric, we learned \\nabove that you can solve this by creating your own custom objective function, but that only a few'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='algorithms can easily be modified to incorporate a specific objective function.\\nThe good news is that the  few algorithms that allow this are among the most effective ones in \\nKaggle competitions and data science projects. Of course, creating your own custom objective \\nfunction may sound a little bit tricky, but it is an incredibly rewarding approach to increasing your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 162}, page_content='score in a competition. For instance, there are options to do this when using gradient boosting \\nalgorithms such as XGBoost, CatBoost, and LightGBM, as well as with all deep learning models \\nbased on TensorFlow or PyTorch.\\nYou can find great tutorials for custom metrics and objective functions in TensorFlow and Py -\\nTorch here:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='Chapter 5 137\\n• https://towardsdatascience.com/custom-metrics-in-keras-and-how-simple-they-\\nare-to-use-in-tensorflow2-2-6d079c2ca279\\n• https://petamind.com/advanced-keras-custom-loss-functions/\\n• https://kevinmusgrave.github.io/pytorch-metric-learning/extend/losses/\\nThese will provide you with the basic function templates and some useful suggestions about how'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='to code a custom objective or evaluation function.\\nIf you need to create a custom loss in LightGBM, XGBoost, or CatBoost, as indicated in their re -\\nspective documentation, you have to code a function that takes as inputs the prediction and the \\nground truth, and that returns as outputs the gradient and the hessian.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='From a code implementation perspective, all you have to do is to create a function, using closures \\nif you need to pass more parameters beyond just the vector of predicted labels and true labels. \\nHere is a simple example of a focal loss  (a loss that aims to heavily weight the minority class in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='the loss computations as described in Lin, T-Y. et al. Focal loss for dense object detection : https://\\narxiv.org/abs/1708.02002 ) function that you can use as a model for your own custom functions:\\nfrom scipy.misc import derivative\\nimport xgboost as xgb\\ndef focal_loss (alpha, gamma):\\n    def loss_func (y_pred, y_true):\\n        a, g = alpha, gamma\\n        def get_loss (y_pred, y_true):'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='p = 1 / (1 + np.exp(-y_pred))If you want just to get straight to the custom objective function you need, you can \\ntry this Notebook by RNA ( https://www.kaggle.com/bigironsphere ): https://\\nwww.kaggle.com/bigironsphere/loss-function-library-keras-pytorch/  \\nnotebook . It contains a large range of custom loss functions for both TensorFlow'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 163}, page_content='and PyTorch that have appeared in different competitions.\\nYou can consult this post on Stack Overflow for a better understanding of what a gra -\\ndient and a hessian are: https://stats.stackexchange.com/questions/231220/\\nhow-to-compute-the-gradient-and-hessian-of-logarithmic-loss-  \\nquestion-is-based .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='Competition Tasks and Metrics 138\\n            loss = (-(a * y_true + ( 1 - a)*( 1 - y_true)) * \\n                    (( 1 - (y_true * p + ( 1 - y_true) * \\n                     ( 1 - p)))**g) * (y_true * np.log(p) + \\n                    ( 1 - y_true) * np.log( 1 - p)))\\n            return loss\\n        partial_focal = lambda y_pred: get_loss(y_pred, y_true)'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='grad = derivative(partial_focal, y_pred, n= 1, dx=1e-6)\\n        hess = derivative(partial_focal, y_pred, n= 2, dx=1e-6)\\n        return grad, hess\\n    return loss_func\\nxgb = xgb.XGBClassifier(objective=focal_loss(alpha= 0.25, gamma= 1))\\nIn the above code snippet, we have defined a new cost function, focal_loss , which is then fed'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='into an XGBoost instance’s object parameters. The example is worth showing because the focal \\nloss requires the specification of some parameters in order to work properly on your problem \\n(alpha and gamma). The more simplistic solution of having their values directly coded into the \\nfunction is not ideal, since you may have to change them systematically as you are tuning your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='model. Instead, in the proposed function, when you input the parameters into the focal_loss  \\nfunction, they reside in memory and they are referenced by the loss_func  function that is returned \\nto XGBoost. The returned cost function, therefore, will work, referring to the alpha and gamma \\nvalues that you have initially instantiated.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='Another interesting aspect of the example is that it really makes it easy to compute the gradient \\nand the hessian of the cost function by means of the derivative function from SciPy. If your cost \\nfunction is differentiable, you don’t have to worry about doing any calculations by hand. How -\\never, creating a custom objective function requires some mathematical knowledge and quite a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='lot of effort to make sure it works properly for your purposes. You can read about the difficulties \\nthat Max Halford experienced while implementing a focal loss for the LightGBM algorithm, and \\nhow he overcame them, here: https://maxhalford.github.io/blog/lightgbm-focal-loss/ . \\nDespite the difficulty, being able to conjure up a custom loss can really determine your success in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='a Kaggle competition where you have to extract the maximum possible result from your model.\\nIf building your own objective function isn’t working out, you can simply lower your ambitions, \\ngive up building your function as an objective function used by the optimizer, and instead code it \\nas a custom evaluation metric. Though your model won’t be directly optimized to perform against'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 164}, page_content='this function, you can still improve its predictive performance with hyperparameter optimization \\nbased on it. This is the second option we talked about in the previous section.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='Chapter 5 139\\nJust remember, if you are writing a metric from scratch, sometimes you may need to abide by \\ncertain code conventions for your function to work properly. For instance, if you use Scikit-learn, \\nyou have to convert your functions using the make_scorer  function. The make_scorer  function is'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='actually a wrapper that makes your evaluation function suitable for working with the Scikit-learn \\nAPI. It will wrap your function while considering some meta-information, such as whether to use \\nprobability estimates or predictions, whether you need to specify a threshold for prediction, and, \\nlast but not least, the directionality of the optimization, that is, whether you want to maximize'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content=\"or minimize the score it returns:\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn.metrics import average_precision_score\\nscorer = make_scorer(average_precision_score, \\naverage= 'weighted' , greater_is_better= True, needs_proba= False)\\nIn the above example, you prepare a scorer based on the average precision metric, specifying that\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='it should use a  weighted computation when dealing with multi-class classification problems.\\nPost-processing your predictions\\nPost-processing tuning  implies that your predictions are transformed, by means of a function, \\ninto something else in order to present a better evaluation. After building your custom loss or'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='optimizing for your evaluation metric, you can also improve your results by leveraging the char-\\nacteristics of your evaluation metric using a specific function applied to your predictions. Let’s \\ntake the Quadratic Weighted Kappa, for instance. We mentioned previously that this metric is \\nuseful when you have to deal with the prediction of an ordinal value. To recap, the original Kappa'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='coefficient is a chance-adjusted index of agreement between the algorithm and the ground truth. \\nIt is a kind of accuracy measurement corrected by the probability that the match between the \\nprediction and the ground truth is due to a fortunate chance.If you are optimizing for your evaluation metric, you can apply grid search, random'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='search, or some more sophisticated optimization such as Bayesian optimization and \\nfind the set of parameters that makes your algorithm perform optimally for your \\nevaluation metric, even if it works with a different cost function. We will explore \\nhow to best arrange parameter optimization and obtain the best results on Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 165}, page_content='competitions after having discussed model validation, specifically in the chapter \\ndealing with tabular data problems.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='Competition Tasks and Metrics 140\\nHere is the original version of the Kappa coefficient, as seen before:\\n𝑘𝑘 𝑘 𝑘𝑘𝑘 0−𝑘𝑘 𝑒𝑒)/𝑘1 −  𝑘𝑘𝑒𝑒) \\nIn the formula, p0 is the relative observed agreement among raters, and pe is the hypothetical prob -\\nability of chance agreement. Here, you need just two matrices, the one with the observed scores'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='and the one with the expected scores based on chance agreement. When the Kappa coefficient \\nis weighted, you also consider a weight matrix and the formula turns into this:\\n𝑘𝑘 𝑘 𝑘𝑘𝑘 0−𝑘𝑘 𝑒𝑒)/𝑘1 − 𝑘𝑘𝑝𝑝) \\nThe matrix pp contains the penalizations to weight errors differently, which is very useful for or -\\ndinal predictions since this matrix can penalize much more when the predictions deviate further'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='from the ground truths. Using the quadratic form, that is, squaring the resulting k, makes the \\npenalization even more severe. However, optimizing for such a metric is really not easy, since it \\nis very difficult to implement it as a cost function. Post-processing can help you.\\nAn example can be found in the PetFinder.my Adoption Prediction competition ( https://www.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='kaggle.com/c/petfinder-adoption-prediction ). In this competition, given that the results \\ncould have 5 possible ratings (0, 1, 2, 3, or 4), you could deal with them either using a classification \\nor a regression. If you used a regression, a post-processing transformation of the regression output \\ncould improve the model’s performance against the Quadratic Weighted Kappa metric, outper -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='forming the results you could get from a classification directly outputting discrete predictions.\\nIn the case of the PetFinder competition, the post-processing consisted of an optimization process \\nthat started by transforming the regression results into integers, first using the boundaries [0.5,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='1.5, 2.5, 3.5] as thresholds and, by an iterative fine-tuning, finding a better set of boundaries that \\nmaximized the performance. The fine-tuning of the boundaries required the computations of an \\noptimizer such as SciPy’s optimize.minimize , which is based on the Nelder-Mead algorithm. The \\nboundaries found by the optimizer were validated by a cross-validation scheme. You can read'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 166}, page_content='more details about this post-processing directly from the post made by Abhishek Thakur during the \\ncompetition: https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/76107 .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='Chapter 5 141\\nUnfortunately, post-processing is often very dependent on the metric you are using (understand -\\ning the metric is imperative for devising any good post-processing) and often also data-specific, \\nfor instance, in the case of time series data and leakages. Hence, it is very difficult to generalize any'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='procedure for figuring out the right post-processing for any competition. Nevertheless, always be \\naware of this possibility and be on the lookout in a competition for any hint that post-processing \\nresults is favorable. You can always get hints about post-processing from previous competitions \\nthat have been similar, and by forum discussion – eventually, someone will raise the topic.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='Predicted probability and its adjustment\\nTo complete the above discussion on metrics optimization (post-processing of predictions), we \\nwill discuss situations where it is paramount to predict correct probabilities, but you are not \\nsure if the algorithm you are using is doing a good job. As we detailed previously, classification'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='probabilities concern both binary and multiclass classification problems, and they are commonly \\nevaluated using the logarithmic loss (aka log loss or logistic loss or cross-entropy loss) in its bi -\\nnary or multi-class version (for more details, see the previous sections on Metrics for classification \\n(label prediction and probability) and Metrics for multi-class classification ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='However, evaluating or optimizing for the log loss may not prove enough. The main problems to be \\non the lookout for when striving to achieve correct probabilistic predictions with your model are:\\n• Models that do not return a truly probabilistic estimate\\n• Unbalanced distribution of classes in your problem'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='• Different class distribution between your training data and your test data (on both public \\nand private leaderboards)Aside from the PetFinder competition, many other competitions have demonstrated \\nthat smart post-processing can lead to improved results and rankings. We’ll point \\nout a few examples here:\\n• https://www.kaggle.com/khoongweihao/post-processing-\\ntechnique-c-f-1st-place-jigsaw'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 167}, page_content='• https://www.kaggle.com/tomooinubushi/postprocessing-based-\\non-leakage\\n• https://www.kaggle.com/saitodevel01/indoor-post-processing-\\nby-cost-minimization'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='Competition Tasks and Metrics 142\\nThe first point alone provides reason to check and verify the quality of classification predictions \\nin terms of modeled uncertainty. In fact, even if many algorithms are provided in the Scikit-learn \\npackage together with a predict_proba  method, this is a very weak assurance that they will \\nreturn a true probability.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='return a true probability.\\nLet’s take, for instance, decision trees, which are the basis of many effective methods to model \\ntabular data. The probability outputted by a classification decision tree ( https://scikit-learn.\\norg/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html ) is based on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='terminal leaves; that is, it depends on the distribution of classes on the leaf that contains the case \\nto be predicted. If the tree is fully grown, it is highly likely that the case is in a small leaf with very \\nfew other cases, so the predicted probability will be very high. If you change parameters such'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='as max_depth , max_leaf_nodes , or min_samples_leaf , the resulting probability will drastically \\nchange from higher values to lower ones depending on the growth of the tree.\\nDecision trees are the most common base model for ensembles such as bagging models and \\nrandom forests, as well as boosted models such as gradient boosting (with its high-performing'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='implementations XGBoost, LightGBM, and CatBoost). But, for the same reasons – probability \\nestimates that are not truly based on solid probabilistic estimations – the problem affects many \\nother commonly used models, such as support-vector machines and k-nearest neighbors. Such \\naspects were mostly unknown to Kagglers until the Otto Group Product Classification Challenge'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='(https://www.kaggle.com/c/otto-group-product-classification-challenge/overview/ ), \\nwhen it was raised by Christophe Bourguignat and others during the competition (see https://\\nwww.kaggle.com/cbourguignat/why-calibration-works ), and easily solved at the time using \\nthe calibration functions that had recently been added to Scikit-learn.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='Aside from the model you will be using, the presence of imbalance between classes in your problem \\nmay also result in models that are not at all reliable. Hence, a good approach in the case of unbal-\\nanced classification problems is to rebalance the classes using undersampling or oversampling \\nstrategies, or different custom weights for each class to be applied when the loss is computed by'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='the algorithm. All these strategies may render your model more performant; however, they will \\nsurely distort the probability estimates and you may have to adjust them in order to obtain an \\neven better model score on the leaderboard.\\nFinally, a third point of concern is related to how the test set is distributed. This kind of informa-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 168}, page_content='tion is usually concealed, but there are often ways to estimate it and figure it out (for instance, by \\ntrial and error based on the public leaderboard results, as we mentioned in Chapter 1, Introducing \\nKaggle and Other Data Science Competitions ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='Chapter 5 143\\nFor instance, this happened in the iMaterialist Furniture Challenge ( https://www.kaggle.com/c/\\nimaterialist-challenge-furniture-2018/ ) and the more popular Quora Question Pairs  \\n(https://www.kaggle.com/c/quora-question-pairs ). Both competitions gave rise to various \\ndiscussions on how to post-process in order to adjust probabilities to test expectations (see'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='https://swarbrickjones.wordpress.com/2017/03/28/cross-entropy-and-training-test-\\nclass-imbalance/  and https://www.kaggle.com/dowakin/probability-calibration-0-005-\\nto-lb for more details on the method used). From a general point of view, assuming that you \\ndo not have an idea of the test distribution of classes to be predicted, it is still very beneficial to'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='correctly predict probability based on the priors you get from the training data (and until you \\nget evidence to the contrary, that is the probability distribution that your model should mimic). \\nIn fact, it will be much easier to correct your predicted probabilities if your predicted probability \\ndistribution matches those in the training set.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content=\"The solution, when your predicted probabilities are misaligned with the training distribution of \\nthe target, is to use the calibration function  provided by Scikit-learn, CalibratedClassifierCV :\\nsklearn.calibration.CalibratedClassifierCV(base_estimator= None, *,\\n    method= 'sigmoid' , cv=None, n_jobs= None, ensemble= True)\"),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='The purpose of the calibration function is to apply a post-processing function to your predicted \\nprobabilities in order to make them adhere more closely to the empirical probabilities seen in the \\nground truth. Provided that your model is a Scikit-learn model or behaves similarly to one, the \\nfunction will act as a wrapper for your model and directly pipe its predictions into a post-pro -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='cessing function. You have the choice between using two methods for post-processing. The first is \\nthe sigmoid method (also called Plat’s scaling), which is nothing more than a logistic regression. \\nThe second is the isotonic regression, which is a non-parametric regression; beware that it tends \\nto overfit if there are few examples.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='You also have to choose how to fit this calibrator. Remember that it is a model that is applied \\nto the results of your model, so you have to avoid overfitting by systematically reworking pre -\\ndictions. You could use a cross-validation  (more on this in the following chapter on Designing \\nGood Validation) and then produce a number of models that, once averaged, will provide your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 169}, page_content='predictions ( ensemble=True ). Otherwise, and this is our usual choice, resort to an out-of-fold \\nprediction  (more on this in the following chapters) and calibrate on that using all the data avail -\\nable (ensemble=False ).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='Competition Tasks and Metrics 144\\nEven if CalibratedClassifierCV  can handle most situations, you can also figure out some empir -\\nical way to fix probability estimates for the best performance at test time. You can use any trans -\\nformation function, from a handmade one to a sophisticated one derived by genetic algorithms,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='for instance. Your only limit is simply that you should cross-validate it and possibly have a good \\nfinal result from the public leaderboard (but not necessarily, because you should trust your local \\ncross-validation score more, as we are going to discuss in the next chapter). A good example of such \\na strategy is provided by Silogram ( https://www.kaggle.com/psilogram ), who, in the Microsoft'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='Malware Classification Challenge, found out a way to tune the unreliable probabilistic outputs of \\nrandom forests into probabilistic ones simply by raising the output to a power determined by \\ngrid search (see https://www.kaggle.com/c/malware-classification/discussion/13509 ).\\nSudalai Rajkumar\\nhttps://www.kaggle.com/sudalairajkumar'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='In our final interview of the chapter, we speak to Sudalai Rajkumar, \\nSRK, a Grandmaster in Competitions, Datasets, and Notebooks, and a \\nDiscussion Master. He is ranked #1 in the Analytics Vidhya data science \\nplatform, and works as an AI/ML advisor for start-ups.\\nWhat’s your favourite kind of competition and why? In terms of \\ntechniques and solving approaches, what is your specialty on Kaggle?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='My favorite kinds of competition are ones that involve a good amount of feature engineering. I think \\nthat is my strength as well. I am generally interested in data exploration to get a deep understanding of \\nthe data (which you can infer from my series of simple exploration Notebooks ( https://www.kaggle.\\ncom/sudalairajkumar/code )) and then creating features based on it.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='How do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?\\nThe framework for a competition involves data exploration, finding the right validation method, feature \\nengineering, model building, and ensembling/stacking. All these are involved in my day job as well. But'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 170}, page_content='in addition to this, there is a good amount of stakeholder discussion, data collection, data tagging, model \\ndeployment, model monitoring, and data storytelling that is involved in my daily job.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='Chapter 5 145\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.\\nSantander Product Recommendation is a memorable competition that we entered. Rohan & I did a \\nlot of feature engineering and built multiple models. When we did final ensembling, we used different'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='weights for different products and some of them did not add up to 1. From the data exploration and \\nunderstanding, we hand-picked these weights, which helped us. This made us realise the domain/data \\nimportance in solving problems and how data science is an art as much as science.\\nHas Kaggle helped you in your career? If so, how?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='Kaggle played a very important role in my career. I was able to secure my last two jobs mainly because \\nof Kaggle. Also, the success from Kaggle helps to connect with other stalwarts in the data science field \\neasily and learn from them. It also helps a lot in my current role as AI / ML advisor for start-ups, as it \\ngives credibility.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='gives credibility.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?\\nUnderstanding the data in depth. Often this is overlooked, and people get into model-building right \\naway. Exploring the data plays a very important role in the success of any Kaggle competition. This helps'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='to create proper cross validation and to create better features and to extract more value from the data.\\nWhat mistakes have you made in competitions in the past?\\nIt is a very big list, and I would say that they are learning opportunities. In every competition, out of 20-30 \\nideas that I try, only 1 may work. These mistakes/failures give much more learning than the actual success'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='or things that worked. For example, I learnt about overfitting the very hard way by falling from top deciles \\nto bottom deciles in one of my very first competitions. But that learning stayed with me forever thereafter.\\nAre there any particular tools or libraries that you would recommend using for data analysis/machine \\nlearning?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 171}, page_content='learning?\\nI primarily use XGBoost/LightGBM in the case of tabular data. I also use open source AutoML libraries \\nand Driverless AI to get early benchmarks these days. I use Keras, Transformers, and PyTorch for deep \\nlearning models.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='Competition Tasks and Metrics 146\\nSummary\\nIn this chapter, we have discussed evaluation metrics in Kaggle competitions. First, we explained \\nhow an evaluation metric can differ from an objective function. We also remarked on the differ -\\nences between regression and classification problems. For each type of problem, we analyzed the \\nmost common metrics that you can find in a Kaggle competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='After that, we discussed the metrics that have never previously been seen in a competition and \\nthat you won’t likely see again. Finally, we explored and studied different common metrics, giving \\nexamples of where they have been used in previous Kaggle competitions. We then proposed a few \\nstrategies for optimizing an evaluation metric. In particular, we recommended trying to code your'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='own custom cost functions and provided suggestions on possible useful post-processing steps.\\nYou should now have grasped the role of an evaluation metric in a Kaggle competition. You should \\nalso have a strategy to deal with every common or uncommon metric, by retracing past competi-\\ntions and by gaining a full understanding of the way a metric works. In the next chapter, we are'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='going to discuss how to use evaluation metrics and properly estimate the performance of your \\nKaggle solution by means of a validation strategy.What’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nConsistency is the key. Each competition will have its own ups and downs. There will be multiple days'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='without any progress, but we should not give up and keep trying. I think this is applicable for anything \\nand not just Kaggle competitions.\\nDo you use other competition platforms? How do they compare to \\nKaggle?\\nI have also taken part on other platforms like the Analytics Vidhya DataHack platform, Driven Data,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 172}, page_content='CrowdAnalytix etc. They are good too, but Kaggle is more widely adopted and global in nature, so the \\namount of competition on Kaggle is much higher compared to other platforms.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 173}, page_content='Chapter 5 147\\nJoin our book’s Discord space\\nJoin the book’s Discord workspace for a monthly Ask me Anything session with the authors: \\nhttps://packt.link/KaggleDiscord'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 175}, page_content='6\\nDesigning Good Validation\\nIn a Kaggle competition, in the heat of modeling and submitting results, it may seem enough to \\ntake at face value the results you get back from the leaderboard. In the end, you may think that \\nwhat counts in a competition is your ranking. This is a common error that is made repeatedly'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 175}, page_content='in competitions. In actual fact, you won’t know what the actual leaderboard (the private one) \\nlooks like until after the competition has closed, and trusting the public part of it is not advisable \\nbecause it is quite often misleading.\\nIn this chapter, we will introduce you to the importance of validation  in data competitions. You \\nwill learn about:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 175}, page_content='will learn about:\\n• What overfitting is and how a public leaderboard can be misleading\\n• The dreadful shake-ups\\n• The different kinds of validation strategies\\n• Adversarial validation\\n• How to spot and leverage leakages\\n• What your strategies should be when choosing your final submissions\\nMonitoring your performances when modeling and distinguishing when overfitting happens is a'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 175}, page_content='key competency not only in data science competitions but in all data science projects. Validating \\nyour models properly is one of the most important skills that you can learn from a Kaggle com-\\npetition and that you can resell in the professional world.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='Designing Good Validation 150\\nSnooping on the leaderboard\\nAs we previously described, in each competition, Kaggle divides the test set into a public part , \\nwhich is visualized on the ongoing leaderboard, and a private part , which will be used to calcu-\\nlate the final scores. These test parts are usually randomly determined (although in time series'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='competitions, they are determined based on time) and the entire test set is released without any \\ndistinction made between public and private.\\nTherefore, a submission derived from a model will cover the entire test set, but only the public \\npart will immediately be scored, leaving the scoring of the private part until after the competition \\nhas closed.\\nGiven this, three considerations arise:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='• In order for a competition to work properly, training data and test data should be from \\nthe same distribution. Moreover, the private and public parts of the test data should \\nresemble each other in terms of distribution.\\n• Even if the training and test data are apparently from the same distribution, the lack of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='sufficient examples in either set could make it difficult to obtain aligned results between \\nthe training data and the public and private test data.\\n• The public test data should be regarded as a holdout test in a data science project: to be \\nused only for final validation. Hence, it should not be queried much in order to avoid what'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='is called adaptive overfitting , which implies a model that works well on a specific test \\nset but underperforms on others.\\nKeeping in mind these three considerations is paramount to understanding the dynamics of \\na competition. In most competitions, there are always quite a few questions in the discussion'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='forums about how the training, public, and private test data relate to each other, and it is quite \\ncommon to see submissions of hundreds of solutions that have only been evaluated based on \\ntheir efficacy on the public leaderboard.Recently, in order to avoid the scrutinizing of test data in certain competitions, Kaggle'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 176}, page_content='has even held back the test data, providing only some examples of it and replacing \\nthem with the real test set when the submission is made. These are called Code  \\ncompetitions because you are not actually providing the predictions themselves, \\nbut a Notebook containing the code to generate them.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='Chapter 6 151\\nIt is also common to hear discussions about shake-ups  that revolutionize the rankings. They \\nare, in fact, a rearranging of the final rankings that can disappoint many who previously held \\nbetter positions on the public leaderboard. Anecdotally, shake-ups are commonly attributed to \\ndifferences between the training and test set or between the private and public parts of the test'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='data. They are measured ex ante based on how competitors have seen their expected local scores \\ncorrelate with the leaderboard feedback and ex post by a series of analyses based on two figures:\\n• A general shake-up figure based on mean(abs(private_rank-public_rank)/number_\\nof_teams)\\n• A top leaderboard shake-up figure, taking into account only the top 10% of public ranks'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='However, aside from an ex post evaluation, there are quite a few lessons that we can get from \\nprevious shake-ups that can help you in your Kaggle competitions. A few researchers from UC \\nBerkeley think so too. In their paper presented at NIPS 2019, Roelofs, Fridovich-Keil et al. study \\nin detail a few thousand Kaggle competitions to gain insight into the public-private leaderboard'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='dynamics in Kaggle competitions. Although they focus on a limited subset of competitions (120, \\nabove a certain number of participants, focused on binary classification), they obtained some \\ninteresting findings:\\n• There is little adaptive overfitting; in other words, public standings usually do hold in the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='unveiled private leaderboard.These ex post figures were first devised by Steve Donoho  (https://www.kaggle.com/\\nbreakfastpirate ) who compiled a ranking of the worst Kaggle shake-ups (see \\nhttps://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/\\ndiscussion/49106#278831 ). They are nowadays easily available, recreated by many'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 177}, page_content='Notebooks based on the Meta Kaggle dataset we discussed in Chapter 5, Competi-\\ntion Tasks and Metrics  (see https://www.kaggle.com/jtrotman/meta-kaggle-\\ncompetition-shake-up ). For instance, by consulting these figures, you may find \\nout how dreadful the RSNA Intracranial Hemorrhage Detection  competition was for  \\nmany because of its shake-ups, especially in the top positions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='Designing Good Validation 152\\n• Most shake-ups are due to random fluctuations and overcrowded rankings where com-\\npetitors are too near to each other, and any slight change in the performance in the private \\ntest sets causes major changes in the rankings.\\n• Shake-ups happen when the training set is very small or the training data is not indepen-\\ndent and identically distributed (i.i.d.).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='In our long experience of Kaggle competitions, however, we have seen quite a lot of problems with \\nadaptive overfitting since the beginning. For instance, you can read Greg Park’s analysis of one of \\nthe first competitions we ever took part in: http://gregpark.io/blog/Kaggle-Psychopathy-\\nPostmortem/ . Since this is quite a common and persistent problem for many Kagglers, we suggest'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='a strategy that is a bit more sophisticated than simply following what happens on the public \\nleaderboard:\\n• Always build reliable cross-validation systems for local scoring.\\n• Always try to control non-i.i.d distributions using the best validation scheme dictated \\nby the situation. Unless clearly stated in the description of the competition, it is not an'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='easy task to spot non-i.i.d. distributions, but you can get hints from discussion or by ex -\\nperimenting using stratified validation schemes (when stratifying according to a certain \\nfeature, the results improve decisively, for instance).\\n• Correlate local scoring with the public leaderboard in order to figure out whether or not \\nthey go in the same direction.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='• Test using adversarial validation, revealing whether or not the test distribution is similar \\nto the training data.\\n• Make your solutions more robust using ensembling, especially if you are working with \\nsmall datasets.\\nIn the following sections, we are going to explore each of these ideas (except for ensembling,'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='which is the topic of a future chapter) and provide you with all the best tools and strategies to \\nobtain the best results, especially on the private dataset.The full paper, Roelofs, R., Fridovich-Keil, S. et al. A meta-analysis of overfitting in \\nmachine learning. Proceedings of the 33rd International Conference on Neural In -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 178}, page_content='formation Processing Systems. 2019, can be found at this link: https://papers.\\nnips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf .'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='Chapter 6 153\\nThe importance of validation in competitions\\nIf you think about a competition carefully, you can imagine it as a huge system of experiments. \\nWhoever can create the most systematic and efficient way to run these experiments wins.\\nIn fact, in spite of all your theoretical knowledge, you will be in competition with the hundreds'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='or thousands of data professionals who have more or less the same competencies as you. \\nIn addition, they will be using exactly the same data as you and roughly the same tools for learn -\\ning from the data (TensorFlow, PyTorch, Scikit-learn, and so on). Some will surely have better \\naccess to computational resources, although the availability of Kaggle Notebooks and generally'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='decreasing cloud computing prices mean the gap is no longer so wide. Consequently, if you look \\nat differences in knowledge, data, models, and available computers, you won’t find many dis -\\ncriminating factors between you and the other competitors that could explain huge performance \\ndifferences in a competition. Yet, some participants consistently outperform others, implying'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='there is some underlying success factor.\\nIn interviews and meet-ups, some Kagglers describe this success factor as “grit,” some others as \\n“trying everything,” some others again as a “willingness to put everything you have into a com-\\npetition.” These may sound a bit obscure and magic. Instead, we call it systematic experimen-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='tation. In our opinion, the key to successful participation resides in the number of experiments \\nyou conduct and the way you run all of them. The more experiments you undertake, the more \\nchances you will have to crack the problem better than other participants. This number certainly \\ndepends on a few factors, such as the time you have available, your computing resources (the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='faster the better, but as we previously mentioned, this is not such a strong differentiator per se), \\nyour team size, and their involvement in the task. This aligns with the commonly reported grit \\nand engagement as keys for success.\\nHowever, these are not the only factors affecting the result. You have to take into account that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='the way you run your experiments also has an impact. Fail fast and learn from it  is an important \\nfactor in a competition. Of course, you need to reflect carefully both when you fail and when you \\nsucceed in order to learn something from your experiences, or your competition will just turn \\ninto a random sequence of attempts in the hope of picking the right solution.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 179}, page_content='Therefore, ceteris paribus , having a proper validation strategy is the great discriminator between \\nsuccessful Kaggle competitors and those who just overfit the leaderboard and end up in low -\\ner-than-expected rankings after a competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='Designing Good Validation 154\\nGenerally, the impact of choosing proper validation is too often overlooked in favor of more \\nquantitative factors, such as having the latest, most powerful GPU or a larger team producing \\nsubmissions. \\nNevertheless, if you count only on the firepower of experiments and their results on the leader-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='board, it will be like “throwing mud at the wall and hoping something will stick” (see http://\\ngregpark.io/blog/Kaggle-Psychopathy-Postmortem/ ). Sometimes such a strategy will work, \\nbut most often it won’t, because you will miss important opportunities to experiment in the \\nright direction, and you won’t even be able to see the shining gem you managed to produce in'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='the middle of all that mud. For instance, if you concentrate too much on trying your luck on the \\npublic leaderboard using a random, unsystematic strategy, even if you produce great solutions, \\nyou may end up not choosing your final submission correctly and missing the best scoring one \\non the private leaderboard.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='on the private leaderboard.\\nHaving a proper validation strategy can help you decide which of your models should be submitted \\nfor ranking on the private test set. Though the temptation to submit your top public leaderboard \\nmodels may be high, always consider your own validation scores. For your final submissions, de -'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='pending on the situation and whether or not you trust the leaderboard, choose your best model \\nbased on the leaderboard and your best based on your local validation results. If you don’t trust \\nthe leaderboard (especially when the training sample is small or the examples are non-i.i.d.), \\nsubmit models that have two of the best validation scores, picking two very different models or'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='ensembles. In this way, you will reduce the risk of choosing solutions that won’t perform on the \\nprivate test set.\\nHaving pointed out the importance of having a method of experimenting, what is left is all a \\nmatter of the practicalities of validation. In fact, when you model a solution, you take a series of \\ninterrelated decisions:\\n1. How to process your data\\n2. What model to apply'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 180}, page_content='2. What model to apply\\n3. How to change the model’s architecture (especially true for deep learning models)\\n4. How to set the model’s hyperparameters\\n5. How to post-process the predictionsValidation  is the method you use to correctly evaluate the errors that your model \\nproduces and to measure how its performance improves or decreases based on your \\nexperiments.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='Chapter 6 155\\nEven if the public leaderboard is perfectly correlated with the private one, the limited number of \\ndaily submissions (a limitation present in all competitions) prevents you from even scratching \\nthe surface of possible tests that you could do in all the aforementioned areas. Having a proper \\nvalidation system tells you beforehand if what you are doing could work on the leaderboard.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='Dmitry Larko\\nhttps://www.kaggle.com/dmitrylarko\\nDmitry Larko is a Kaggle Competition Grandmaster and the chief data \\nscientist at H2O.ai. He has over a decade of experience in ML and data \\nscience. He discovered Kaggle in December 2012 and participated in \\nhis first competition a few months later. He is a strong advocate of \\nvalidation in Kaggle competitions, as he told us in his interview.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='What’s your favorite kind of competition and why? In terms of \\ntechniques and solving approaches, what is your specialty on Kaggle?\\nI have mostly participated in competitions for tabular datasets but also enjoy competitions for computer \\nvision.\\nHow do you approach a Kaggle competition? How different is this \\napproach to what you do in your day-to-day work?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='I always try to start simple and build a submission pipeline for smaller/simpler models first. A major \\nstep here is to create a proper validation scheme so you can validate your ideas in a robust way. Also, it is \\nalways a good idea to spend as much time as you can looking at the data and analyzing it.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='In my day-to-day work, I am building an AutoML platform, so a lot of things I try on Kaggle end up \\nbeing implemented as a part of this platform.\\nTell us about a particularly challenging competition you entered, and \\nwhat insights you used to tackle the task.\\nNothing comes to my mind, and it doesn’t matter, because what is technically challenging for me could'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='be a piece of cake for somebody else. Technical challenges are not that important; what’s important is to \\nremember that a competition is somewhat like a marathon, not a sprint. Or you can see it as a marathon \\nof sprints if you like. So, it is important not to get exhausted, sleep well, exercise, and take a walk in a park'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 181}, page_content='to regenerate your brain for new ideas. To win a Kaggle competition, you will need all your creativity \\nand expertise and sometimes even a bit of luck.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 182}, page_content='Designing Good Validation 156\\nBias and variance\\nA good validation system helps you with metrics that are more reliable than the error measures \\nyou get from your training set. In fact, metrics obtained on the training set are affected by the \\ncapacity and complexity of each model. You can think of the capacity of a model as its memory'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 182}, page_content='that it can use to learn from data. Has Kaggle helped you in your career? If so, how?\\nI got my current job thanks to the fact I was a Kaggle Competition Grandmaster. For my current employer, \\nthis fact was evidence enough of my expertise in the field.\\nIn your experience, what do inexperienced Kagglers often overlook? \\nWhat do you know now that you wish you’d known when you first \\nstarted?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 182}, page_content='started?\\nMostly they overlook the right validation scheme and follow the feedback from the public leaderboard. \\nThat ends badly in most cases, leading to something known as a “shake-up” on Kaggle.\\nAlso, they rush to skip exploratory data analysis and build models right away, which leads to simplistic \\nsolutions and mediocre leaderboard scores.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 182}, page_content='What mistakes have you made in competitions in the past?\\nMy main mistake is really the same that an inexperienced person will make – following the leaderboard \\nscore and not my internal validation. Every time I decided to do so, it cost me several places on the lea-\\nderboard.\\nAre there any particular tools or libraries that you would recommend \\nusing for data analysis or machine learning?'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 182}, page_content='That would be the usual suspects. For tabular data: LightGBM, XGBoost, CatBoost; for deep learning: \\nPyTorch, PyTorch-Lightning, timm; and Scikit-learn for everyone.\\nWhat’s the most important thing someone should keep in mind or do \\nwhen they’re entering a competition?\\nStart simple, always validate; believe in your validation score and not the leaderboard score.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='Chapter 6 157\\nEach model has a set of internal parameters that help the model to record the patterns taken \\nfrom the data. Every model has its own skills for acquiring patterns, and some models will spot \\ncertain rules or associations whereas others will spot others. As a model extracts patterns from \\ndata, it records them in its “memory.”'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='You also hear about the capacity or expressiveness of a model as a matter of bias and variance. \\nIn this case, the bias and variance of a model refer to the predictions, but the underlying princi-\\nple is strictly related to the expressiveness of a model. Models can be reduced to mathematical \\nfunctions that map an input (the observed data) to a result (the predictions). Some mathematical'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='functions are more complex than others, in the number of internal parameters they have and in \\nthe ways they use them:\\n• If the mathematical function of a model is not complex or expressive enough to capture \\nthe complexity of the problem you are trying to solve, we talk of bias, because your pre -\\ndictions will be limited (“biased”) by the limits of the model itself.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='• If the mathematical function at the core of a model is too complex for the problem at \\nhand, we have a variance problem, because the model will record more details and noise \\nin the training data than needed and its predictions will be deeply influenced by them \\nand become erratic.\\nNowadays, given the advances in machine learning and the available computation resources, the'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='problem is always due to variance, since deep neural networks and gradient boosting, the most \\ncommonly used solutions, often have a mathematical expressiveness that exceeds what most of \\nthe problems you will face need in order to be solved.\\nWhen all the useful patterns that a certain model can extract have been captured, if the model has'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='not exhausted its capacity, it will then start memorizing data characteristics and signals that are \\nunrelated to the prediction (usually referred to as noise). While the initially extracted patterns \\nwill help the model to generalize to a test dataset and predict more correctly, not everything that'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='it learns specifically about the training set will help; instead, it may damage its performance. The \\nprocess of learning elements of the training set that have no generalization value is commonly \\ncalled overfitting .\\nThe core purpose of validation is to explicitly define a score or loss value that separates the gen-'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 183}, page_content='eralizable part of that value from that due to overfitting the training set characteristics.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 184}, page_content='Designing Good Validation 158\\nThis is the validation loss . You can see the situation visualized in the following figure of learning \\ncurves:\\nFigure 6.1: Learning more from the training data does not always mean learning to predict\\nIf you graph the loss measure on the y-axis against some measure of learning effort of the model'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 184}, page_content='(this could be epochs for neural networks, or rounds for gradient boosting) on the x-axis, you \\nwill notice that learning always seems to happen on the training dataset, but this is not always \\ntrue on other data.\\nThe same thing happens even if you change the hyperparameters, process the data, or decide'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 184}, page_content='on a different model altogether. The curves will change shape, but you’ll always have a sweet \\npoint where overfitting starts. That point can be different across models and between the various \\nchoices that you make in your modeling efforts. If you have properly computed the point when \\noverfitting starts thanks to a correct validation strategy, your model’s performance will surely'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 184}, page_content='correlate with the leaderboard results (both public and private), and your validation metrics will \\nprovide you with a proxy to evaluate your work without making any submissions.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='Chapter 6 159\\nYou can hear about overfitting at various levels:\\n• At the level of the training data, when you use a model that is too complex for the problem\\n• At the level of the validation set itself, when you tune your model too much with respect \\nto a specific validation set\\n• At the level of the public leaderboard, when your results are far from what you would \\nexpect from your training'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='expect from your training\\n• At the level of the private leaderboard, when in spite of the good results on the public \\nleaderboard, your private scores will be disappointing\\nThough slightly different in meaning, they all equally imply that your model is not generalizable, \\nas we have described in this section.\\nTrying different splitting strategies'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='As previously discussed, the validation loss is based on a data sample that is not part of the training \\nset. It is an empirical measure that tells you how good your model is at predicting, and a more \\ncorrect one than the score you get from your training, which will tell you mostly how much your \\nmodel has memorized the training data patterns. Correctly choosing the data sample you use for'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='validation constitutes your validation strategy.\\nTo summarize the strategies for validating your model and measuring its performance correctly, \\nyou have a couple of choices:\\n• The first choice is to work with a holdout system, incurring the risk of not properly \\nchoosing a representative sample of the data or overfitting to your validation holdout.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='• The second option is to use a probabilistic approach and rely on a series of samples to \\ndraw your conclusions on your models. Among the probabilistic approaches, you have \\ncross-validation, leave-one-out  (LOO), and bootstrap. Among the cross-validation strat -\\negies, there are different nuances depending on the sampling strategies you take based on'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='the characteristic of your data (simple random sampling, stratified sampling, sampling \\nby groups, time sampling).\\nWhat all these strategies have in common is that they are sampling strategies. It means that they \\nhelp you to infer a general measure (the performance of your model) based on a small part of'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 185}, page_content='your data, randomly selected. Sampling is at the root of statistics and it is not an exact procedure \\nbecause, based on your sampling method, your available data, and the randomness of picking up \\ncertain cases as part of your sample, you will experience a certain degree of error.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='Designing Good Validation 160\\nFor instance, if you rely on a biased sample, your evaluation metric may be estimated incorrectly \\n(over- or under-estimated). However, if properly designed and implemented, sampling strategies \\ngenerally provide you with a good estimate of your general measure.\\nThe other aspect that all these strategies have in common is that they are partitions , which divide'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='cases in an exclusive way as either part of the training or part of the validation. In fact, as we dis-\\ncussed, since most models have a certain memorization capability, using the same cases in both \\ntraining and validation leads to inflated estimates because it allows the model to demonstrate \\nits memorization abilities; instead, we want it to be evaluated on its ability to derive patterns'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='and functions that work on unseen examples.\\nThe basic train-test split\\nThe first strategy that we will analyze is the train-test split. In this strategy, you sample a portion \\nof your training set (also known as the holdout) and you use it as a test set for all the models that \\nyou train using the remaining part of the data.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='The great advantage of this strategy is that it is very simple: you pick up a part of your data and \\nyou check your work on that part. You usually split the data 80/20 in favor of the training partition. \\nIn Scikit-learn, it is implemented in the train_test_split  function. We’ll draw your attention \\nto a couple of aspects of the method:'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='• When you have large amounts of data, you can expect that the test data you extract is \\nsimilar to (representative of) the original distribution on the entire dataset. However, since \\nthe extraction process is based on randomness, you always have the chance of extracting \\na non-representative sample. In particular, the chance increases if the training sample'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='you start from is small. Comparing the extracted holdout partition using adversarial val-\\nidation (more about this in a few sections) can help you to make sure you are evaluating \\nyour efforts in a correct way.\\n• In addition, to ensure that your test sampling is representative, especially with regard \\nto how the training data relates to the target variable, you can use stratification , which'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='ensures that the proportions of certain features are respected in the sampled data. You \\ncan use the stratify  parameter in the train_test_split  function and provide an array \\ncontaining the class distribution to preserve.\\nWe have to remark that, even if you have a representative holdout available, sometimes a simple'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 186}, page_content='train-test split is not enough for ensuring a correct tracking of your efforts in a competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='Chapter 6 161\\nIn fact, as you keep checking on this test set, you may drive your choices to some kind of adapta-\\ntion overfitting (in other words, erroneously picking up the noise of the training set as signals), as \\nhappens when you frequently evaluate on the public leaderboard. For this reason, a probabilistic \\nevaluation, though more computationally expensive, is more suited for a competition.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='Probabilistic evaluation methods\\nProbabilistic evaluation of the performance of a machine learning model is based on the statistical \\nproperties of a sample from a distribution. By sampling, you create a smaller set of your original \\ndata that is expected to have the same characteristics. In addition, what is left untouched from'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='the sampling constitutes a sample in itself, and it is also expected to have the same characteristics \\nas the original data. By training and testing your model on this sampled data and repeating this \\nprocedure a large number of times, you are basically creating a statistical estimator measuring \\nthe performance of your model. Every sample may have some “error” in it; that is, it may not be'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='fully representative of the true distribution of the original data. However, as you sample more, \\nthe mean of your estimators on these multiple samples will converge to the true mean of the \\nmeasure you are estimating (this is an observed outcome that, in probability, is explained by a \\ntheorem called the Law of Large Numbers).'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='Probabilistic estimators naturally require more computations than a simple train-test split, but \\nthey offer more confidence that you are correctly estimating the right measure: the general per -\\nformance of your model.\\nk-fold cross-validation\\nThe most used probabilistic validation method is k-fold cross-validation , which is recognized'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='as having the ability to correctly estimate the performance of your model on unseen test data \\ndrawn from the same distribution. \\nk-fold cross-validation can be successfully used to compare predictive models, as well as when \\nselecting the hyperparameters for your model that will perform the best on the test set.'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='There are quite a few different variations of k-fold cross-validation, but the simplest one, which \\nis implemented in the KFold function in Scikit-learn, is based on the splitting of your available \\ntraining data into k partitions. After that, for k iterations, one of the k partitions is taken as a test'),\n",
       " Document(metadata={'source': 'D:\\\\RAG NVIDIA\\\\RAG BOOK\\\\The Kaggle Book Data analysis and machine learning for competitive data science (Konrad Banachewicz, Luca Massaron) (Z-Library).pdf', 'page': 187}, page_content='set while the others are used for the training of the model. This is clearly explained in the paper Bates, S., Hastie, T., and Tibshirani, R.; Cross-val -\\nidation: what does it estimate and how well does it do it? arXiv preprint arX iv:2104.00673, \\n2021 (https://arxiv.org/pdf/2104.00673.pdf ).'),\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector db from 1 batch docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [01:54<03:48, 114.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector db from 1 batch docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [03:50<01:55, 115.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging vector db, batch 1\n",
      "Building vector db from 1 batch docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:44<00:00, 114.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging vector db, batch 1\n",
      "Saving vector db to D:\\RAG NVIDIA\\emded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000  # 按照10000个tokens一批进行处理，可以根据自己的服务器内存配置进行调整\n",
    "batch_idx = 1\n",
    "\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    batch_docs = docs[i:i + batch_size]\n",
    "    print(f\"Building vector db from {batch_idx} batch docs\")\n",
    "    if i == 0:\n",
    "        vector_store = FAISS.from_documents(batch_docs,embedder)  # docs 为Document列表\n",
    "        \n",
    "        # torch_gc()\n",
    "    else:\n",
    "        vector_store_append = FAISS.from_documents(batch_docs,embedder)  # # docs 为Document列表\n",
    "        print(f\"Merging vector db, batch {batch_idx}\")\n",
    "        vector_store.merge_from(vector_store_append)  # 合并向量库\n",
    "        # torch_gc()\n",
    "        \n",
    "batch_idx += 1\n",
    "vs_path = r\"D:\\RAG NVIDIA\\emded\"\n",
    "print(f\"Saving vector db to {vs_path}\")\n",
    "vector_store.save_local(vs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "  \n",
    "def torch_gc():  \n",
    "    \"\"\"  \n",
    "    释放PyTorch GPU缓存。  \n",
    "    \"\"\"  \n",
    "    if torch.cuda.is_available():  \n",
    "        torch.cuda.empty_cache()  \n",
    "        print(\"GPU memory cache cleared.\")  \n",
    "    else:  \n",
    "        print(\"CUDA is not available. Skipping GPU memory cache clearing.\")  \n",
    "def batch_save_FAISS(docs,embedder,vs_path=\"embed/\",batch_size = 1024):\n",
    "    # 在Langchain API下 批量保存FAISS：\n",
    "    for i in tqdm(range(0, len(docs), batch_size)):\n",
    "        batch_docs = docs[i:i + batch_size]\n",
    "        print(f\"Building vector db from {batch_idx} batch docs\")\n",
    "        if i == 0:\n",
    "            vector_store = FAISS.from_documents(batch_docs,embedder)  # docs 为Document列表\n",
    "            \n",
    "            torch_gc()\n",
    "        else:\n",
    "            vector_store_append = FAISS.from_documents(batch_docs,embedder)  # # docs 为Document列表\n",
    "            print(f\"Merging vector db, batch {batch_idx}\")\n",
    "            vector_store.merge_from(vector_store_append)  # 合并向量库\n",
    "            torch_gc()\n",
    "    batch_idx += 1\n",
    "    print(f\"Saving vector db to {vs_path}\")\n",
    "    vector_store.save_local(vs_path)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
